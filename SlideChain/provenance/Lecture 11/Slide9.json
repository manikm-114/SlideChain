{
  "lecture": "Lecture 11",
  "models": {
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "category": "mathematics",
        "term": "statistical dependence"
      },
      "triples": {
        "triples": [
          {
            "confidence": 1.0,
            "evidence": "Mathematically, mutual information can actually be expressed in terms of the KL divergence.",
            "modalities": [
              "text"
            ],
            "o": "KL divergence",
            "p": "represents",
            "s": "mutual information"
          },
          {
            "confidence": 1.0,
            "evidence": "Mutual information is a way to measure how much knowing one random variable tells us about another.",
            "modalities": [
              "text"
            ],
            "o": "how much knowing one random variable tells us about another",
            "p": "measures",
            "s": "mutual information"
          }
        ]
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "concepts": [
          {
            "category": "mathematics",
            "term": "Mutual information"
          },
          {
            "category": "mathematics",
            "term": "KL divergence"
          },
          {
            "category": "mathematics",
            "term": "Joint distribution"
          },
          {
            "category": "mathematics",
            "term": "Marginal distribution"
          },
          {
            "category": "algorithm",
            "term": "Image registration"
          }
        ],
        "evidence": [
          "Mutual information is a way to measure how much knowing one random variable tells us about another.",
          "Mutual information can actually be expressed in terms of the KL divergence.",
          "Mutual information captures how much information one variable provides about the other.",
          "Mutual information is essentially a generalization of correlation, but in the language of information theory.",
          "Mutual information goes beyond simple linear relationships and captures any kind of statistical dependence."
        ]
      },
      "triples": {
        "triples": []
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "category": "ai_ml",
        "term": "correlation"
      },
      "triples": {
        "triples": []
      }
    },
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "concepts": [
          {
            "category": "communication",
            "term": "mutual information"
          },
          {
            "category": "mathematics",
            "term": "KL divergence"
          },
          {
            "category": "reconstruction",
            "term": "image registration"
          },
          {
            "category": "mathematics",
            "term": "uncertainty"
          },
          {
            "category": "signal_processing",
            "term": "correlation"
          }
        ],
        "evidence": [
          "Mutual information is a way to measure how much knowing one random variable tells us about another.",
          "Mathematically, mutual information can actually be expressed in terms of the KL divergence.",
          "In practice, this idea is very useful in areas like image registration, where we align two images.",
          "Instead of just matching pixel intensities, we can maximize the mutual information between the two images."
        ]
      },
      "triples": {
        "triples": [
          {
            "confidence": 0.9,
            "evidence": "Mutual information is a way to measure how much knowing one random variable tells us about another.",
            "modalities": [
              "text"
            ],
            "o": "information theory",
            "p": "measures",
            "s": "mutual information"
          }
        ]
      }
    }
  },
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide9.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide9.txt"
  },
  "slide_id": "Slide9"
}
