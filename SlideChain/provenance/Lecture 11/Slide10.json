{
  "lecture": "Lecture 11",
  "models": {
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "concepts": [
          {
            "category": "mathematics",
            "term": "entropy"
          },
          {
            "category": "mathematics",
            "term": "mutual information"
          },
          {
            "category": "mathematics",
            "term": "KL divergence"
          },
          {
            "category": "quality_metric",
            "term": "mean squared error"
          }
        ],
        "evidence": [
          "entropy tells us how much information, or how much unpredictability, is contained in a random variable.",
          "mutual information can be written as the difference between two entropies: the entropy of Y by itself, minus the entropy of Y given X.",
          "these information-theoretic measures - KL divergence, mutual information, and entropy - are very powerful",
          "simpler measures like mean squared error"
        ]
      },
      "triples": {
        "triples": [
          {
            "confidence": 1.0,
            "evidence": "entropy tells us how much information, or how much unpredictability, is contained in a random variable.",
            "modalities": [
              "text"
            ],
            "o": "uncertainty",
            "p": "measures",
            "s": "entropy"
          },
          {
            "confidence": 1.0,
            "evidence": "mutual information can be written as the difference between two entropies: the entropy of Y by itself, minus the entropy of Y given X.",
            "modalities": [
              "text"
            ],
            "o": "how much uncertainty about Y is reduced when you know X",
            "p": "measures",
            "s": "mutual information"
          }
        ]
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "concepts": [
          {
            "category": "mathematics",
            "term": "entropy"
          },
          {
            "category": "mathematics",
            "term": "mutual information"
          },
          {
            "category": "mathematics",
            "term": "uncertainty"
          },
          {
            "category": "mathematics",
            "term": "information theory"
          },
          {
            "category": "mathematics",
            "term": "KL divergence"
          },
          {
            "category": "mathematics",
            "term": "mean squared error"
          },
          {
            "category": "quality_metric",
            "term": "image quality assessment"
          }
        ],
        "evidence": [
          "entropy is a measure of uncertainty",
          "mutual information measures how two variables share or reduce that uncertainty",
          "entropy captures uncertainty",
          "mutual information is the difference between two entropies",
          "these information-theoretic measures are powerful tools",
          "we will soon return to the more practical measures"
        ]
      },
      "triples": {
        "triples": [
          {
            "confidence": 1.0,
            "evidence": "Entropy tells us how much information, or how much unpredictability, is contained in a random variable.",
            "modalities": [
              "text"
            ],
            "o": "Uncertainty",
            "p": "measures",
            "s": "Entropy"
          }
        ]
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "category": "mathematics",
        "term": "KL divergence, mutual information, and entropy are very powerful, but they are more advanced than what we need right now"
      },
      "triples": {
        "confidence": 0.95,
        "evidence": [
          "mutual information measures how much uncertainty about Y is reduced when you know X."
        ],
        "modalities": [
          "text"
        ],
        "o": "how much uncertainty about Y is reduced when you know X",
        "p": "measures",
        "s": "mutual information"
      }
    },
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "category": "software",
        "term": "tools"
      },
      "triples": {
        "triples": [
          {
            "confidence": 0.9,
            "evidence": "Entropy tells us how much information, or how much unpredictability, is contained in a random variable. In information theory, this is a central concept because information itself is really about reducing uncertainty.",
            "modalities": [
              "text"
            ],
            "o": "uncertainty",
            "p": "measures",
            "s": "entropy"
          }
        ]
      }
    }
  },
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide10.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide10.txt"
  },
  "slide_id": "Slide10"
}
