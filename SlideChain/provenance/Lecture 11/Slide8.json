{
  "lecture": "Lecture 11",
  "models": {
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "category": "signal_processing",
        "term": "statistical signal processing"
      },
      "triples": {
        "confidence": 1.0,
        "evidence": [
          "The KL divergence is always greater than or equal to zero"
        ],
        "modalities": [
          "text"
        ],
        "o": "always greater than or equal to zero",
        "p": "is",
        "s": "KL divergence"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "category": "signal_processing",
        "term": "statistical signal processing"
      },
      "triples": {
        "triples": []
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "category": "signal_processing",
        "term": "statistical signal processing"
      },
      "triples": {
        "confidence": 0.95,
        "evidence": [
          "Now, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in."
        ],
        "modalities": [
          "text"
        ],
        "o": "information divergence",
        "p": "measures",
        "s": "Kullback-Leibler Distance"
      }
    },
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "concepts": [
          {
            "category": "mathematics",
            "term": "KL divergence"
          },
          {
            "category": "mathematics",
            "term": "probability distributions"
          },
          {
            "category": "mathematics",
            "term": "Euclidean distance"
          },
          {
            "category": "mathematics",
            "term": "information theory"
          },
          {
            "category": "mathematics",
            "term": "KL divergence is not symmetric"
          },
          {
            "category": "mathematics",
            "term": "KL divergence measures directionality in information"
          },
          {
            "category": "ai_ml",
            "term": "machine learning"
          },
          {
            "category": "signal_processing",
            "term": "statistical signal processing"
          }
        ],
        "evidence": [
          "This is where probability theory comes in.",
          "The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.",
          "The KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P.",
          "KL divergence measures directionality in information."
        ]
      },
      "triples": {
        "triples": [
          {
            "confidence": 0.0,
            "evidence": "KL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.",
            "modalities": [
              "text"
            ],
            "o": "probability distributions",
            "p": "measures",
            "s": "KL divergence"
          }
        ]
      }
    }
  },
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide8.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide8.txt"
  },
  "slide_id": "Slide8"
}
