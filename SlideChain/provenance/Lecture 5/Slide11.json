{
  "lecture": "Lecture 5",
  "models": {
    "OpenGVLab__InternVL3-14B": {
      "concepts": null,
      "triples": null
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "category": "mathematics",
        "term": "orthogonal representation"
      },
      "triples": {
        "triples": []
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "category": "mathematics",
        "term": "any vector - no matter how high the dimension - as a sum of projections onto orthonormal basis vectors"
      },
      "triples": {
        "confidence": 0.95,
        "evidence": [
          "We have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1. Any 3D vector - let's say vector v - with components x, y, and z, can be written as a weighted sum of these basis vectors. So, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z."
        ],
        "modalities": [
          "text",
          "image"
        ],
        "o": "v = (v \u00b7 e_x) e_x + (v \u00b7 e_y) e_y + (v \u00b7 e_z) e_z",
        "p": "represents",
        "s": "vector v"
      }
    },
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "category": "ai_ml",
        "term": "machine learning"
      },
      "triples": {
        "triples": []
      }
    }
  },
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Images\\Slide11.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Texts\\Slide11.txt"
  },
  "slide_id": "Slide11"
}
