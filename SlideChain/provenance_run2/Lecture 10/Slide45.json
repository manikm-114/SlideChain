{
  "lecture": "Lecture 10",
  "models": {
    "OpenGVLab__InternVL3-14B": {
      "concepts": null,
      "triples": {
        "triples": []
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "concepts": [
          {
            "category": "algorithm",
            "term": "Gradient Descent"
          },
          {
            "category": "mathematics",
            "term": "Loss Function"
          },
          {
            "category": "software",
            "term": "Hilly Landscape"
          },
          {
            "category": "mathematics",
            "term": "Steepest Downward Direction"
          }
        ],
        "evidence": [
          "Once we define a loss function - the measure of difference between the network's output and the ground truth - our goal is to reduce that difference step by step.",
          "We do this by adjusting the network's parameters in a way that moves us downhill on the loss surface.",
          "This is the essence of gradient descent.",
          "At each step, we look around, find the steepest downward direction, and take a small move in that direction.",
          "Over time, with each update, the loss gets smaller and smaller until the network converges to a good solution - ideally, a point where further changes no longer improve the performance."
        ]
      },
      "triples": {
        "triples": []
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "category": "ai_ml",
        "term": "Over time, with each update, the loss gets smaller and smaller until the network converges to a good solution - ideally, a point where further changes no longer improve the performance."
      },
      "triples": {
        "triples": []
      }
    },
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "concepts": [
          {
            "category": "mathematics",
            "term": "loss function"
          },
          {
            "category": "algorithm",
            "term": "gradient descent"
          },
          {
            "category": "mathematics",
            "term": "loss surface"
          },
          {
            "category": "physics",
            "term": "hilly landscape"
          },
          {
            "category": "mathematics",
            "term": "steepest downward direction"
          },
          {
            "category": "mathematics",
            "term": "converge"
          },
          {
            "category": "software",
            "term": "good solution"
          }
        ],
        "evidence": [
          "Once we define a loss function - the measure of difference between the network\u2019s output and the ground truth - our goal is to reduce that difference step by step. We do this by adjusting the network\u2019s parameters in a way that moves us downhill on the loss surface.",
          "Over time, with each update, the loss gets smaller and smaller until the network converges to a good solution - ideally, a point where further changes no longer improve the performance."
        ]
      },
      "triples": {
        "triples": []
      }
    }
  },
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Images\\Slide45.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Texts\\Slide45.txt"
  },
  "slide_id": "Slide45"
}
