nohup: ignoring input
/scratch/manikm/myenv/lib/python3.10/site-packages/transformers/utils/hub.py:119: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
âœ… Device: cuda
ðŸ”¹ Loading: Qwen/Qwen3-VL-4B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.09it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.14it/s]
âœ… Loaded: Qwen/Qwen3-VL-4B-Instruct

=== Running model=Qwen/Qwen3-VL-4B-Instruct prompt=concepts on 29 slides ===
Qwen__Qwen3-VL-4B-Instruct | concepts:   0%|          | 0/29 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Qwen__Qwen3-VL-4B-Instruct | concepts:   3%|â–Ž         | 1/29 [00:08<04:06,  8.81s/it]Qwen__Qwen3-VL-4B-Instruct | concepts:   7%|â–‹         | 2/29 [00:18<04:14,  9.44s/it]Qwen__Qwen3-VL-4B-Instruct | concepts:  10%|â–ˆ         | 3/29 [00:28<04:13,  9.75s/it]Qwen__Qwen3-VL-4B-Instruct | concepts:  14%|â–ˆâ–        | 4/29 [00:38<04:03,  9.76s/it]Qwen__Qwen3-VL-4B-Instruct | concepts:  17%|â–ˆâ–‹        | 5/29 [00:48<03:54,  9.76s/it]Qwen__Qwen3-VL-4B-Instruct | concepts:  21%|â–ˆâ–ˆ        | 6/29 [00:58<03:44,  9.74s/it]Qwen__Qwen3-VL-4B-Instruct | concepts:  24%|â–ˆâ–ˆâ–       | 7/29 [01:07<03:34,  9.73s/it]Qwen__Qwen3-VL-4B-Instruct | concepts:  28%|â–ˆâ–ˆâ–Š       | 8/29 [01:17<03:24,  9.73s/it]