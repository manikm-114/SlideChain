So now that we’ve brought our signal into the computer, let’s take a closer look at what’s really happening during analog-to-digital conversion.

As you can see in the top plot, we start with a smooth, continuous signal — x of t — defined for all time and with continuous amplitude.
But the computer can’t store that continuous curve — it needs discrete data.
So we begin with the first major step: sampling, also called discretization in time.
This means we only record the value of the signal at selected time points — evenly spaced along the horizontal axis.At each of these points, we “catch” the value of the signal, just like placing a pin at that instant.
‘
Now, the second step is quantization — converting the amplitude of each sample into a finite-precision value.
We can’t store irrational numbers like pi or e with infinite precision.Instead, we round them to a reasonable approximation — say, 3.14 for pi.And for our purposes, we assume this rounding is accurate enough to not affect the result significantly.
So in our analysis, we mostly ignore the quantization error and focus on sampling — the time discretization.

The bottom diagram shows how the sampled signal looks:It’s now a series of impulses — each one located at a sample time and scaled to the value of the original signal at that moment.
This sequence of impulses is what we work with in digital signal processing.

But here’s something important to remember:
The real signal — the one that matters in the physical world — is still continuous.
What we do with computers is a second-best approximation, based on discrete samples.

So the key question becomes:
Can we process these sampled values in a way that still lets us understand or recover the original continuous signal?
This is the challenge at the heart of signal processing — bridging the gap between discrete computation and continuous reality.
And that’s what we’ll be exploring in the next steps of our journey.
