Here, we have the whole derivation of the sampling theorem summarized on one page.

You see a few steps underlined in red. These are the important turning points in the math — the places where something key happens. In a detailed lecture, we would walk through each of them slowly, but here, I want to focus on the big picture.
We start in the frequency domain, where the spectrum of our signal is multiplied by a periodic train of impulses. When we bring this back to the time domain, that multiplication turns into a convolution. The result of that convolution involves a special function called “sinc,” which naturally appears when you take the Fourier transform of a rectangular shape in frequency.

Next, when we write out the convolution, we see it as an infinite sum — a series of shifted sinc functions. Each one is scaled by the value of our signal at a sample point. The distance between these samples is delta-t, and that’s simply one over P, the spacing in the frequency domain.

The final line is the famous Shannon interpolation formula. In words, it says: take each sample of your signal, put a sinc function centered at that sample, scale it by the sample’s value, and then add them all together. Do this for every sample — stretching infinitely in both directions — and you get your original continuous signal back exactly, as long as the sampling conditions are satisfied.

This is a beautiful result. It tells us that perfect reconstruction is not just possible — it’s a direct consequence of the sampling theorem. And this formula is one of the core foundations of all digital signal processing.
