{
  "lecture": "Lecture 10",
  "slide_id": "Slide41",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Images\\Slide41.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Texts\\Slide41.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide41.json",
        "parsed": {
          "term": "activation functions",
          "category": "ai_ml"
        },
        "raw": "{\"slide_id\": \"Slide41\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nOnce you have the basic idea of how a neural network works, you can imagine that there’s no single way to connect the neurons. In fact, researchers have developed many different network topologies, each suited for specific tasks.\\n\\nHere in this chart, you can see a variety of examples.\\nAt the simplest level, we have the perceptron and feed-forward networks, where information flows in one direction from input to output.\\nThen there are radial basis networks and more specialized architectures like recurrent neural networks or RNNs, which loop information back so the network can remember past inputs.\\nLong Short-Term Memory networks — LSTMs — and Gated Recurrent Units, or GRUs, are powerful variants for processing sequences, such as speech or time-series data.\\n\\nOther designs, like autoencoders and variational autoencoders, are used for compressing data and then reconstructing it, often to find hidden patterns or generate new examples.\\nYou also see Boltzmann machines, Hopfield networks, and deep belief networks, which are useful for certain types of learning and pattern recognition.\\n\\nThe key point is that these different architectures are like tools in a toolbox. Some are better for images, some for language, some for prediction over time. Once you understand the fundamentals — inputs, weights, activation functions, and training — all these variations are just different ways of wiring those same building blocks together.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"perceptron\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"feed-forward networks\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"radial basis networks\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"recurrent neural networks (RNNs)\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"Long Short-Term Memory (LSTM) networks\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"Gated Recurrent Units (GRUs)\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"autoencoders\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"variational autoencoders\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"Boltzmann machines\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"Hopfield networks\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"deep belief networks\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"inputs\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"weights\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"activation functions\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"\", \"parsed\": {\"term\": \"activation functions\", \"category\": \"ai_ml\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide41.json",
        "parsed": {
          "triples": [
            {
              "s": "perceptron",
              "p": "uses",
              "o": "feed-forward networks",
              "modalities": [
                "text"
              ],
              "confidence": 0.0,
              "evidence": "Once you have the basic idea of how a neural network works, you can imagine that there’s no single way to connect the neurons. In fact, researchers have developed many different network topologies, each suited for specific tasks."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide41\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nOnce you have the basic idea of how a neural network works, you can imagine that there’s no single way to connect the neurons. In fact, researchers have developed many different network topologies, each suited for specific tasks.\\n\\nHere in this chart, you can see a variety of examples.\\nAt the simplest level, we have the perceptron and feed-forward networks, where information flows in one direction from input to output.\\nThen there are radial basis networks and more specialized architectures like recurrent neural networks or RNNs, which loop information back so the network can remember past inputs.\\nLong Short-Term Memory networks — LSTMs — and Gated Recurrent Units, or GRUs, are powerful variants for processing sequences, such as speech or time-series data.\\n\\nOther designs, like autoencoders and variational autoencoders, are used for compressing data and then reconstructing it, often to find hidden patterns or generate new examples.\\nYou also see Boltzmann machines, Hopfield networks, and deep belief networks, which are useful for certain types of learning and pattern recognition.\\n\\nThe key point is that these different architectures are like tools in a toolbox. Some are better for images, some for language, some for prediction over time. Once you understand the fundamentals — inputs, weights, activation functions, and training — all these variations are just different ways of wiring those same building blocks together.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"perceptron\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"feed-forward networks\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"Once you have the basic idea of how a neural network works, you can imagine that there’s no single way to connect the neurons. In fact, researchers have developed many different network topologies, each suited for specific tasks.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"perceptron\", \"p\": \"uses\", \"o\": \"feed-forward networks\", \"modalities\": [\"text\"], \"confidence\": 0.0, \"evidence\": \"Once you have the basic idea of how a neural network works, you can imagine that there’s no single way to connect the neurons. In fact, researchers have developed many different network topologies, each suited for specific tasks.\"}]}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide41.json",
        "parsed": null,
        "raw": "{\"slide_id\": \"Slide41\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"\", \"parsed\": null}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide41.json",
        "parsed": null,
        "raw": "{\"slide_id\": \"Slide41\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"\", \"parsed\": null}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide41.json",
        "parsed": {
          "term": "training",
          "category": "algorithm"
        },
        "raw": "{\"slide_id\": \"Slide41\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T04:11:34.535476+00:00\", \"text_length\": 1437, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nOnce you have the basic idea of how a neural network works, you can imagine that there’s no single way to connect the neurons. In fact, researchers have developed many different network topologies, each suited for specific tasks.\\n\\nHere in this chart, you can see a variety of examples.\\nAt the simplest level, we have the perceptron and feed-forward networks, where information flows in one direction from input to output.\\nThen there are radial basis networks and more specialized architectures like recurrent neural networks or RNNs, which loop information back so the network can remember past inputs.\\nLong Short-Term Memory networks — LSTMs — and Gated Recurrent Units, or GRUs, are powerful variants for processing sequences, such as speech or time-series data.\\n\\nOther designs, like autoencoders and variational autoencoders, are used for compressing data and then reconstructing it, often to find hidden patterns or generate new examples.\\nYou also see Boltzmann machines, Hopfield networks, and deep belief networks, which are useful for certain types of learning and pattern recognition.\\n\\nThe key point is that these different architectures are like tools in a toolbox. Some are better for images, some for language, some for prediction over time. Once you understand the fundamentals — inputs, weights, activation functions, and training — all these variations are just different ways of wiring those same building blocks together.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"perceptron\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"feed-forward networks\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"radial basis networks\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"recurrent neural networks\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"LSTMs\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Gated Recurrent Units\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"autoencoders\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"variational autoencoders\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Boltzmann machines\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Hopfield networks\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"deep belief networks\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"inputs\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"weights\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"activation functions\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"training\\\", \\\"category\\\": \\\"algorithm\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Researchers have developed many different network topologies, each suited for specific\", \"parsed\": {\"term\": \"training\", \"category\": \"algorithm\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide41.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide41\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T04:53:44.750898+00:00\", \"text_length\": 1437, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nOnce you have the basic idea of how a neural network works, you can imagine that there’s no single way to connect the neurons. In fact, researchers have developed many different network topologies, each suited for specific tasks.\\n\\nHere in this chart, you can see a variety of examples.\\nAt the simplest level, we have the perceptron and feed-forward networks, where information flows in one direction from input to output.\\nThen there are radial basis networks and more specialized architectures like recurrent neural networks or RNNs, which loop information back so the network can remember past inputs.\\nLong Short-Term Memory networks — LSTMs — and Gated Recurrent Units, or GRUs, are powerful variants for processing sequences, such as speech or time-series data.\\n\\nOther designs, like autoencoders and variational autoencoders, are used for compressing data and then reconstructing it, often to find hidden patterns or generate new examples.\\nYou also see Boltzmann machines, Hopfield networks, and deep belief networks, which are useful for certain types of learning and pattern recognition.\\n\\nThe key point is that these different architectures are like tools in a toolbox. Some are better for images, some for language, some for prediction over time. Once you understand the fundamentals — inputs, weights, activation functions, and training — all these variations are just different ways of wiring those same building blocks together.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide41.json",
        "parsed": {
          "term": "sparse ae",
          "category": "ai_ml"
        },
        "raw": "{\"slide_id\": \"Slide41\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-08T02:32:59.678154+00:00\", \"text_length\": 1437, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nOnce you have the basic idea of how a neural network works, you can imagine that there’s no single way to connect the neurons. In fact, researchers have developed many different network topologies, each suited for specific tasks.\\n\\nHere in this chart, you can see a variety of examples.\\nAt the simplest level, we have the perceptron and feed-forward networks, where information flows in one direction from input to output.\\nThen there are radial basis networks and more specialized architectures like recurrent neural networks or RNNs, which loop information back so the network can remember past inputs.\\nLong Short-Term Memory networks — LSTMs — and Gated Recurrent Units, or GRUs, are powerful variants for processing sequences, such as speech or time-series data.\\n\\nOther designs, like autoencoders and variational autoencoders, are used for compressing data and then reconstructing it, often to find hidden patterns or generate new examples.\\nYou also see Boltzmann machines, Hopfield networks, and deep belief networks, which are useful for certain types of learning and pattern recognition.\\n\\nThe key point is that these different architectures are like tools in a toolbox. Some are better for images, some for language, some for prediction over time. Once you understand the fundamentals — inputs, weights, activation functions, and training — all these variations are just different ways of wiring those same building blocks together.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"neural network\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"perceptron\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"feed-forward networks\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"radial basis networks\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"recurrent neural network\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"long / short term memory\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"gated recurrent unit\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"auto encoder\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"variational autoencoder\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"denoising ae\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"sparse ae\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"markov chain\\\",\\n      \\\"\", \"parsed\": {\"term\": \"sparse ae\", \"category\": \"ai_ml\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 10\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide41.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide41\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-08T02:36:19.665474+00:00\", \"text_length\": 1437, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nOnce you have the basic idea of how a neural network works, you can imagine that there’s no single way to connect the neurons. In fact, researchers have developed many different network topologies, each suited for specific tasks.\\n\\nHere in this chart, you can see a variety of examples.\\nAt the simplest level, we have the perceptron and feed-forward networks, where information flows in one direction from input to output.\\nThen there are radial basis networks and more specialized architectures like recurrent neural networks or RNNs, which loop information back so the network can remember past inputs.\\nLong Short-Term Memory networks — LSTMs — and Gated Recurrent Units, or GRUs, are powerful variants for processing sequences, such as speech or time-series data.\\n\\nOther designs, like autoencoders and variational autoencoders, are used for compressing data and then reconstructing it, often to find hidden patterns or generate new examples.\\nYou also see Boltzmann machines, Hopfield networks, and deep belief networks, which are useful for certain types of learning and pattern recognition.\\n\\nThe key point is that these different architectures are like tools in a toolbox. Some are better for images, some for language, some for prediction over time. Once you understand the fundamentals — inputs, weights, activation functions, and training — all these variations are just different ways of wiring those same building blocks together.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": []\\n}\", \"parsed\": {\"triples\": []}}"
      }
    }
  }
}