{
  "lecture": "Lecture 3",
  "slide_id": "Slide38",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 3\\Images\\Slide38.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 3\\Texts\\Slide38.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 3\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide38.json",
        "parsed": {
          "concepts": [
            {
              "term": "neural network",
              "category": "ai_ml"
            },
            {
              "term": "deep neural network",
              "category": "ai_ml"
            },
            {
              "term": "linear and nonlinear elements",
              "category": "ai_ml"
            },
            {
              "term": "simple features",
              "category": "ai_ml"
            },
            {
              "term": "abstract patterns",
              "category": "ai_ml"
            },
            {
              "term": "facial recognition",
              "category": "ai_ml"
            },
            {
              "term": "self-driving cars",
              "category": "ai_ml"
            }
          ],
          "evidence": [
            "When we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network-the foundation of modern deep learning.",
            "In these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs.",
            "Deep neural networks now drive advances in fields from facial recognition to self-driving cars. This is a powerful example of combining linear and nonlinear elements to solve complex problems."
          ]
        },
        "raw": "{\"slide_id\": \"Slide38\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nWhen we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network—the foundation of modern deep learning.\\nIn these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs. Deep neural networks now drive advances in fields from facial recognition to self-driving cars. This is a powerful example of combining linear and nonlinear elements to solve complex problems.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"neural network\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"deep neural network\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"linear and nonlinear elements\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"simple features\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"abstract patterns\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"facial recognition\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"self-driving cars\\\", \\\"category\\\": \\\"ai_ml\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"When we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network—the foundation of modern deep learning.\\\", \\\"In these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs.\\\", \\\"Deep neural networks now drive advances in fields from facial recognition to self-driving cars. This is a powerful example of combining linear and nonlinear elements to solve complex problems.\\\"]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"neural network\", \"category\": \"ai_ml\"}, {\"term\": \"deep neural network\", \"category\": \"ai_ml\"}, {\"term\": \"linear and nonlinear elements\", \"category\": \"ai_ml\"}, {\"term\": \"simple features\", \"category\": \"ai_ml\"}, {\"term\": \"abstract patterns\", \"category\": \"ai_ml\"}, {\"term\": \"facial recognition\", \"category\": \"ai_ml\"}, {\"term\": \"self-driving cars\", \"category\": \"ai_ml\"}], \"evidence\": [\"When we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network-the foundation of modern deep learning.\", \"In these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs.\", \"Deep neural networks now drive advances in fields from facial recognition to self-driving cars. This is a powerful example of combining linear and nonlinear elements to solve complex problems.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 3\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide38.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide38\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nWhen we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network—the foundation of modern deep learning.\\nIn these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs. Deep neural networks now drive advances in fields from facial recognition to self-driving cars. This is a powerful example of combining linear and nonlinear elements to solve complex problems.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"neural network\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"deep neural network\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"When we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network—the foundation of modern deep learning.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 3\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide38.json",
        "parsed": {
          "concepts": [
            {
              "term": "neural network",
              "category": "ai_ml"
            },
            {
              "term": "deep neural network",
              "category": "ai_ml"
            },
            {
              "term": "layers",
              "category": "ai_ml"
            },
            {
              "term": "features",
              "category": "ai_ml"
            },
            {
              "term": "abstract patterns",
              "category": "ai_ml"
            },
            {
              "term": "facial recognition",
              "category": "ai_ml"
            },
            {
              "term": "self-driving cars",
              "category": "ai_ml"
            },
            {
              "term": "linear and nonlinear elements",
              "category": "ai_ml"
            }
          ],
          "evidence": [
            "create a neural network",
            "deep neural network",
            "many layers",
            "simple features",
            "abstract patterns",
            "facial recognition",
            "self-driving cars",
            "combining linear and nonlinear elements"
          ]
        },
        "raw": "{\"slide_id\": \"Slide38\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"neural network\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"deep neural network\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"layers\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"features\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"abstract patterns\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"facial recognition\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"self-driving cars\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"linear and nonlinear elements\\\", \\\"category\\\": \\\"ai_ml\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"create a neural network\\\",\\n    \\\"deep neural network\\\",\\n    \\\"many layers\\\",\\n    \\\"simple features\\\",\\n    \\\"abstract patterns\\\",\\n    \\\"facial recognition\\\",\\n    \\\"self-driving cars\\\",\\n    \\\"combining linear and nonlinear elements\\\"\\n  ]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"neural network\", \"category\": \"ai_ml\"}, {\"term\": \"deep neural network\", \"category\": \"ai_ml\"}, {\"term\": \"layers\", \"category\": \"ai_ml\"}, {\"term\": \"features\", \"category\": \"ai_ml\"}, {\"term\": \"abstract patterns\", \"category\": \"ai_ml\"}, {\"term\": \"facial recognition\", \"category\": \"ai_ml\"}, {\"term\": \"self-driving cars\", \"category\": \"ai_ml\"}, {\"term\": \"linear and nonlinear elements\", \"category\": \"ai_ml\"}], \"evidence\": [\"create a neural network\", \"deep neural network\", \"many layers\", \"simple features\", \"abstract patterns\", \"facial recognition\", \"self-driving cars\", \"combining linear and nonlinear elements\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 3\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide38.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide38\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"Deep neural networks\\\",\\n      \\\"p\\\": \\\"uses\\\",\\n      \\\"o\\\": \\\"hierarchical feature representations\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"Deep neural networks learn hierarchical feature representations\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"lower layers\\\",\\n      \\\"p\\\": \\\"detects\\\",\\n      \\\"o\\\": \\\"simple features\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"the lower layers detect simple features, like edges or corners\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"higher layers\\\",\\n      \\\"p\\\": \\\"learn\\\",\\n      \\\"o\\\": \\\"abstract patterns\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"higher layers learn more abstract patterns, like faces or traffic signs\\\"\\n    }\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 3\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide38.json",
        "parsed": {
          "term": "solve complex problems",
          "category": "ai_ml"
        },
        "raw": "{\"slide_id\": \"Slide38\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-08T20:40:41.594089+00:00\", \"text_length\": 536, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nWhen we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network—the foundation of modern deep learning.\\nIn these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs. Deep neural networks now drive advances in fields from facial recognition to self-driving cars. This is a powerful example of combining linear and nonlinear elements to solve complex problems.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"deep neural network\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"neural network\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"lower layers\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"higher layers\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"simple features\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"abstract patterns\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"facial recognition\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"self-driving cars\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"linear and nonlinear elements\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"solve complex problems\\\", \\\"category\\\": \\\"ai_ml\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"When we connect these artificial neurons in layers, we create a neural network.\\\",\\n    \\\"With many layers, we have what’s called a deep neural network—the foundation of modern deep learning.\\\",\\n    \\\"In these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs.\\\",\\n    \\\"Deep neural networks now drive advances in fields from facial recognition\", \"parsed\": {\"term\": \"solve complex problems\", \"category\": \"ai_ml\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 3\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide38.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide38\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-08T21:10:18.877312+00:00\", \"text_length\": 536, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nWhen we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network—the foundation of modern deep learning.\\nIn these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs. Deep neural networks now drive advances in fields from facial recognition to self-driving cars. This is a powerful example of combining linear and nonlinear elements to solve complex problems.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Deep Neural Network (DNN)\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"Deep neural networks learn hierarchical feature representations\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"Deep neural networks learn hierarchical feature representations\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 3\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide38.json",
        "parsed": {
          "term": "solve complex problems",
          "category": "ai_ml"
        },
        "raw": "{\"slide_id\": \"Slide38\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-07T22:15:22.850973+00:00\", \"text_length\": 536, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nWhen we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network—the foundation of modern deep learning.\\nIn these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs. Deep neural networks now drive advances in fields from facial recognition to self-driving cars. This is a powerful example of combining linear and nonlinear elements to solve complex problems.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"deep neural network\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"artificial neurons\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"many layers\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"lower layers detect simple features\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"higher layers learn more abstract patterns\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"linear and nonlinear elements\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"solve complex problems\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    }\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"When we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network—the foundation of modern deep learning.\\\",\\n    \\\"In these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs.\\\",\\n    \\\"This is a powerful example of combining linear and nonlinear elements to solve complex problems.\\\"\\n  ]\", \"parsed\": {\"term\": \"solve complex problems\", \"category\": \"ai_ml\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 3\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide38.json",
        "parsed": {
          "s": "lower layers",
          "p": "detects",
          "o": "simple features",
          "modalities": [
            "text",
            "image"
          ],
          "confidence": 0.8,
          "evidence": [
            "the lower layers detect simple features, like edges or corners"
          ]
        },
        "raw": "{\"slide_id\": \"Slide38\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-07T22:17:14.981109+00:00\", \"text_length\": 536, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nWhen we connect these artificial neurons in layers, we create a neural network. With many layers, we have what’s called a deep neural network—the foundation of modern deep learning.\\nIn these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs. Deep neural networks now drive advances in fields from facial recognition to self-driving cars. This is a powerful example of combining linear and nonlinear elements to solve complex problems.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"Deep neural networks\\\",\\n      \\\"p\\\": \\\"represents\\\",\\n      \\\"o\\\": \\\"hierarchical feature representations\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"Deep neural networks learn hierarchical feature representations\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"Deep neural networks\\\",\\n      \\\"p\\\": \\\"uses\\\",\\n      \\\"o\\\": \\\"many layers\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.8,\\n      \\\"evidence\\\": \\\"With many layers, we have what’s called a deep neural network\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"lower layers\\\",\\n      \\\"p\\\": \\\"detects\\\",\\n      \\\"o\\\": \\\"simple features\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.8,\\n      \\\"evidence\\\": \\\"the lower layers detect simple features, like edges or corners\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"higher layers\\\",\\n      \\\"p\\\": \\\"learn\\\",\\n      \\\"o\\\": \\\"more abstract patterns\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\", \"parsed\": {\"s\": \"lower layers\", \"p\": \"detects\", \"o\": \"simple features\", \"modalities\": [\"text\", \"image\"], \"confidence\": 0.8, \"evidence\": [\"the lower layers detect simple features, like edges or corners\"]}}"
      }
    }
  }
}