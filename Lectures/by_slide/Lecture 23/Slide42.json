{
  "lecture": "Lecture 23",
  "slide_id": "Slide42",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 23\\Images\\Slide42.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 23\\Texts\\Slide42.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 23\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide42.json",
        "parsed": {
          "term": "stable reconstructions",
          "category": "reconstruction"
        },
        "raw": "{\"slide_id\": \"Slide42\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nAt RPI, we’ve been taking this concept even further. We’re working on developing a tightly integrated system for in vivo optical and X-ray imaging — that means imaging live animals, not just tissue samples.\\n\\nIn the earlier work, what we call ex vivo imaging, the studies were done on isolated or preserved samples, which is great for testing physics, but not for observing biological dynamics. So, our goal is to move toward in vivo imaging, where we can study living processes in real time.\\nOur approach is to build an orthogonal imaging chain — basically, an X-ray phase-contrast system aligned at a right angle to an optical imaging system. One part of the setup handles the phase-contrast X-ray imaging, while the other captures fluorescence or bioluminescence signals. By merging these two datasets, we can combine the strengths of both modalities — structural accuracy from X-rays and molecular sensitivity from optics.\\n\\nThe system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection. The optical part uses mirrors, spectrometers, and digital micromirror devices for detecting and filtering the light.\\nSo, what we’re working toward at RPI is a fully integrated hybrid imaging system — one that can acquire X-ray phase-contrast data and optical molecular data simultaneously.\\n\\nNow, this is still an active research topic. Achieving truly precise, stable 3D tomography in such a hybrid setup is not easy. There are still open questions — how to synchronize the modalities, how to register the datasets, how to compensate for motion, and how to achieve stable reconstructions.\\nBut it’s an exciting direction. When you can successfully merge X-ray and optical data — one showing the anatomy, the other revealing the function — that’s when you get a truly powerful multimodal imaging system.\\n\\nSo, this is what we’re actively working on at RPI — a tighter, more integrated system that pushes the boundary of X-ray–optical fusion imaging. It’s challenging, but also very rewarding work.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"in vivo imaging\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"ex vivo imaging\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"X-ray phase-contrast imaging\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"optical imaging\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"hybrid imaging system\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"3D tomography\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"synchronization\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"registration\\\", \\\"category\\\": \\\"data_processing\\\"},\\n    {\\\"term\\\": \\\"motion compensation\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"stable reconstructions\\\", \\\"category\\\": \\\"reconstruction\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"At RPI, we’ve been taking this concept even further. We’re working on developing a tightly integrated system for in vivo optical and X-ray imaging — that means imaging live animals, not just tissue samples.\\\", \\\"In the earlier work, what we call ex vivo imaging, the studies were done on isolated or preserved samples, which is great for testing physics, but not for observing biological dynamics\", \"parsed\": {\"term\": \"stable reconstructions\", \"category\": \"reconstruction\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 23\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide42.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide42\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nAt RPI, we’ve been taking this concept even further. We’re working on developing a tightly integrated system for in vivo optical and X-ray imaging — that means imaging live animals, not just tissue samples.\\n\\nIn the earlier work, what we call ex vivo imaging, the studies were done on isolated or preserved samples, which is great for testing physics, but not for observing biological dynamics. So, our goal is to move toward in vivo imaging, where we can study living processes in real time.\\nOur approach is to build an orthogonal imaging chain — basically, an X-ray phase-contrast system aligned at a right angle to an optical imaging system. One part of the setup handles the phase-contrast X-ray imaging, while the other captures fluorescence or bioluminescence signals. By merging these two datasets, we can combine the strengths of both modalities — structural accuracy from X-rays and molecular sensitivity from optics.\\n\\nThe system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection. The optical part uses mirrors, spectrometers, and digital micromirror devices for detecting and filtering the light.\\nSo, what we’re working toward at RPI is a fully integrated hybrid imaging system — one that can acquire X-ray phase-contrast data and optical molecular data simultaneously.\\n\\nNow, this is still an active research topic. Achieving truly precise, stable 3D tomography in such a hybrid setup is not easy. There are still open questions — how to synchronize the modalities, how to register the datasets, how to compensate for motion, and how to achieve stable reconstructions.\\nBut it’s an exciting direction. When you can successfully merge X-ray and optical data — one showing the anatomy, the other revealing the function — that’s when you get a truly powerful multimodal imaging system.\\n\\nSo, this is what we’re actively working on at RPI — a tighter, more integrated system that pushes the boundary of X-ray–optical fusion imaging. It’s challenging, but also very rewarding work.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"RPI\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"hybrid imaging system\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"At RPI, we’ve been taking this concept even further. We’re working on developing a tightly integrated system for in vivo optical and X-ray imaging — that means imaging live animals, not just tissue samples.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 23\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide42.json",
        "parsed": {
          "term": "synchronize the modalities",
          "category": "workflow"
        },
        "raw": "{\"slide_id\": \"Slide42\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"in vivo optical and X-ray imaging\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"ex vivo imaging\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"phase-contrast X-ray imaging\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"fluorescence or bioluminescence signals\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"CCD camera\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"filter wheel\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"laser stage\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"isoflurane anesthesia line\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"rotating gantry\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"mirrors\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"spectrometers\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"digital micromirror devices\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"3D tomography\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"synchronize the modalities\\\", \\\"category\\\": \\\"workflow\\\"},\\n    {\\\"\", \"parsed\": {\"term\": \"synchronize the modalities\", \"category\": \"workflow\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 23\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide42.json",
        "parsed": {
          "s": "system",
          "p": "uses",
          "o": "laser stage",
          "modalities": [
            "text"
          ],
          "confidence": 1.0,
          "evidence": [
            "The system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection."
          ]
        },
        "raw": "{\"slide_id\": \"Slide42\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"system\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"CCD camera\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"The system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection.\\\"},\\n    {\\\"s\\\":\\\"system\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"filter wheel\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"The system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection.\\\"},\\n    {\\\"s\\\":\\\"system\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"laser stage\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"The system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection.\\\"},\\n    {\\\"s\\\":\\\"system\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"isoflurane anesthesia line\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":\", \"parsed\": {\"s\": \"system\", \"p\": \"uses\", \"o\": \"laser stage\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": [\"The system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection.\"]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 23\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide42.json",
        "parsed": {
          "term": "register datasets",
          "category": "data_processing"
        },
        "raw": "{\"slide_id\": \"Slide42\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T18:21:22.003473+00:00\", \"text_length\": 2105, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nAt RPI, we’ve been taking this concept even further. We’re working on developing a tightly integrated system for in vivo optical and X-ray imaging — that means imaging live animals, not just tissue samples.\\n\\nIn the earlier work, what we call ex vivo imaging, the studies were done on isolated or preserved samples, which is great for testing physics, but not for observing biological dynamics. So, our goal is to move toward in vivo imaging, where we can study living processes in real time.\\nOur approach is to build an orthogonal imaging chain — basically, an X-ray phase-contrast system aligned at a right angle to an optical imaging system. One part of the setup handles the phase-contrast X-ray imaging, while the other captures fluorescence or bioluminescence signals. By merging these two datasets, we can combine the strengths of both modalities — structural accuracy from X-rays and molecular sensitivity from optics.\\n\\nThe system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection. The optical part uses mirrors, spectrometers, and digital micromirror devices for detecting and filtering the light.\\nSo, what we’re working toward at RPI is a fully integrated hybrid imaging system — one that can acquire X-ray phase-contrast data and optical molecular data simultaneously.\\n\\nNow, this is still an active research topic. Achieving truly precise, stable 3D tomography in such a hybrid setup is not easy. There are still open questions — how to synchronize the modalities, how to register the datasets, how to compensate for motion, and how to achieve stable reconstructions.\\nBut it’s an exciting direction. When you can successfully merge X-ray and optical data — one showing the anatomy, the other revealing the function — that’s when you get a truly powerful multimodal imaging system.\\n\\nSo, this is what we’re actively working on at RPI — a tighter, more integrated system that pushes the boundary of X-ray–optical fusion imaging. It’s challenging, but also very rewarding work.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"CCD camera\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"filter wheel\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"laser stage\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"isoflurane anesthesia line\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"rotating gantry\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"orthogonal imaging chain\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"X-ray phase-contrast system\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"optical imaging system\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"fluorescence\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"bioluminescence\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"hybrid imaging system\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"X-ray–optical fusion imaging\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"synchronize modalities\\\", \\\"category\\\": \\\"data_processing\\\"},\\n    {\\\"term\\\": \\\"register datasets\\\", \\\"category\\\": \\\"data_processing\\\"},\\n    {\\\"term\\\": \\\"compensate for motion\\\",\", \"parsed\": {\"term\": \"register datasets\", \"category\": \"data_processing\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 23\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide42.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide42\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T18:56:12.416747+00:00\", \"text_length\": 2105, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nAt RPI, we’ve been taking this concept even further. We’re working on developing a tightly integrated system for in vivo optical and X-ray imaging — that means imaging live animals, not just tissue samples.\\n\\nIn the earlier work, what we call ex vivo imaging, the studies were done on isolated or preserved samples, which is great for testing physics, but not for observing biological dynamics. So, our goal is to move toward in vivo imaging, where we can study living processes in real time.\\nOur approach is to build an orthogonal imaging chain — basically, an X-ray phase-contrast system aligned at a right angle to an optical imaging system. One part of the setup handles the phase-contrast X-ray imaging, while the other captures fluorescence or bioluminescence signals. By merging these two datasets, we can combine the strengths of both modalities — structural accuracy from X-rays and molecular sensitivity from optics.\\n\\nThe system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection. The optical part uses mirrors, spectrometers, and digital micromirror devices for detecting and filtering the light.\\nSo, what we’re working toward at RPI is a fully integrated hybrid imaging system — one that can acquire X-ray phase-contrast data and optical molecular data simultaneously.\\n\\nNow, this is still an active research topic. Achieving truly precise, stable 3D tomography in such a hybrid setup is not easy. There are still open questions — how to synchronize the modalities, how to register the datasets, how to compensate for motion, and how to achieve stable reconstructions.\\nBut it’s an exciting direction. When you can successfully merge X-ray and optical data — one showing the anatomy, the other revealing the function — that’s when you get a truly powerful multimodal imaging system.\\n\\nSo, this is what we’re actively working on at RPI — a tighter, more integrated system that pushes the boundary of X-ray–optical fusion imaging. It’s challenging, but also very rewarding work.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Filter Wheel\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"Detection DMD\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Filter Wheel\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 23\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide42.json",
        "parsed": {
          "term": "rotating gantry",
          "category": "instrumentation"
        },
        "raw": "{\"slide_id\": \"Slide42\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-08T16:00:16.904469+00:00\", \"text_length\": 2105, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nAt RPI, we’ve been taking this concept even further. We’re working on developing a tightly integrated system for in vivo optical and X-ray imaging — that means imaging live animals, not just tissue samples.\\n\\nIn the earlier work, what we call ex vivo imaging, the studies were done on isolated or preserved samples, which is great for testing physics, but not for observing biological dynamics. So, our goal is to move toward in vivo imaging, where we can study living processes in real time.\\nOur approach is to build an orthogonal imaging chain — basically, an X-ray phase-contrast system aligned at a right angle to an optical imaging system. One part of the setup handles the phase-contrast X-ray imaging, while the other captures fluorescence or bioluminescence signals. By merging these two datasets, we can combine the strengths of both modalities — structural accuracy from X-rays and molecular sensitivity from optics.\\n\\nThe system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection. The optical part uses mirrors, spectrometers, and digital micromirror devices for detecting and filtering the light.\\nSo, what we’re working toward at RPI is a fully integrated hybrid imaging system — one that can acquire X-ray phase-contrast data and optical molecular data simultaneously.\\n\\nNow, this is still an active research topic. Achieving truly precise, stable 3D tomography in such a hybrid setup is not easy. There are still open questions — how to synchronize the modalities, how to register the datasets, how to compensate for motion, and how to achieve stable reconstructions.\\nBut it’s an exciting direction. When you can successfully merge X-ray and optical data — one showing the anatomy, the other revealing the function — that’s when you get a truly powerful multimodal imaging system.\\n\\nSo, this is what we’re actively working on at RPI — a tighter, more integrated system that pushes the boundary of X-ray–optical fusion imaging. It’s challenging, but also very rewarding work.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"tighter integration\\\",\\n      \\\"category\\\": \\\"instrumentation\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"in vivo optical and X-ray imaging\\\",\\n      \\\"category\\\": \\\"modality\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"ex vivo imaging\\\",\\n      \\\"category\\\": \\\"modality\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"orthogonal imaging chain\\\",\\n      \\\"category\\\": \\\"instrumentation\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"X-ray phase-contrast system\\\",\\n      \\\"category\\\": \\\"physics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"optical imaging system\\\",\\n      \\\"category\\\": \\\"modality\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"CCD camera\\\",\\n      \\\"category\\\": \\\"instrumentation\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"filter wheel\\\",\\n      \\\"category\\\": \\\"instrumentation\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"laser stage\\\",\\n      \\\"category\\\": \\\"instrumentation\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"isoflurane anesthesia line\\\",\\n      \\\"category\\\": \\\"instrumentation\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"rotating gantry\\\",\\n      \\\"category\\\": \\\"instrumentation\\\"\\n    },\\n    {\\n      \\\"term\\\":\", \"parsed\": {\"term\": \"rotating gantry\", \"category\": \"instrumentation\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 23\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide42.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide42\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-08T16:07:13.394462+00:00\", \"text_length\": 2105, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nAt RPI, we’ve been taking this concept even further. We’re working on developing a tightly integrated system for in vivo optical and X-ray imaging — that means imaging live animals, not just tissue samples.\\n\\nIn the earlier work, what we call ex vivo imaging, the studies were done on isolated or preserved samples, which is great for testing physics, but not for observing biological dynamics. So, our goal is to move toward in vivo imaging, where we can study living processes in real time.\\nOur approach is to build an orthogonal imaging chain — basically, an X-ray phase-contrast system aligned at a right angle to an optical imaging system. One part of the setup handles the phase-contrast X-ray imaging, while the other captures fluorescence or bioluminescence signals. By merging these two datasets, we can combine the strengths of both modalities — structural accuracy from X-rays and molecular sensitivity from optics.\\n\\nThe system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection. The optical part uses mirrors, spectrometers, and digital micromirror devices for detecting and filtering the light.\\nSo, what we’re working toward at RPI is a fully integrated hybrid imaging system — one that can acquire X-ray phase-contrast data and optical molecular data simultaneously.\\n\\nNow, this is still an active research topic. Achieving truly precise, stable 3D tomography in such a hybrid setup is not easy. There are still open questions — how to synchronize the modalities, how to register the datasets, how to compensate for motion, and how to achieve stable reconstructions.\\nBut it’s an exciting direction. When you can successfully merge X-ray and optical data — one showing the anatomy, the other revealing the function — that’s when you get a truly powerful multimodal imaging system.\\n\\nSo, this is what we’re actively working on at RPI — a tighter, more integrated system that pushes the boundary of X-ray–optical fusion imaging. It’s challenging, but also very rewarding work.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": []\\n}\", \"parsed\": {\"triples\": []}}"
      }
    }
  }
}