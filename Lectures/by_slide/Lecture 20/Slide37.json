{
  "lecture": "Lecture 20",
  "slide_id": "Slide37",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 20\\Images\\Slide37.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 20\\Texts\\Slide37.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 20\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide37.json",
        "parsed": {
          "term": "understanding how the brain encodes visual information",
          "category": "algorithm"
        },
        "raw": "{\"slide_id\": \"Slide37\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at this fascinating experiment. Here, subjects were shown two different types of images — human faces and buildings. The MRI scans you see here show how the brain reacts differently in each case. When a person looks at a face, certain brain regions light up — you can see those areas in red and orange. When the same person looks at a building, other regions become more active. This happens because the brain’s oxygen-rich blood flow changes depending on what you are thinking or perceiving.\\n\\nWhen neurons in a region start firing, they consume more oxygen. The body responds by sending in oxygenated blood, which creates a stronger MRI signal in that area. This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast. By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing — for example, a face versus a house. This was one of the early experiments demonstrating how brain activity reflects thought and perception.\\n\\nIn more recent studies, researchers have gone even further. Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing — essentially allowing us to peek into human perception. It’s an incredible step toward understanding how the brain encodes visual information. If you have time, I highly recommend watching a short video on this topic — it’s truly amazing to see how far this technology has come.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"BOLD effect\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"Blood Oxygen Level Dependent contrast\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"brain activity\\\", \\\"category\\\": \\\"anatomy\\\"},\\n    {\\\"term\\\": \\\"neurons firing\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"oxygenated blood\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"MRI scans\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"brain's oxygen-rich blood flow\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"face versus a house\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"reconstruct what a person is actually seeing\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"with rough resolution\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"into human perception\\\", \\\"category\\\": \\\"communication\\\"},\\n    {\\\"term\\\": \\\"understanding how the brain encodes visual information\\\", \\\"category\\\": \\\"algorithm\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"This happens because the brain’s oxygen-rich blood flow changes depending on what you are thinking or perceiving.\\\", \\\"This was one of the early experiments demonstrating how brain activity reflects\", \"parsed\": {\"term\": \"understanding how the brain encodes visual information\", \"category\": \"algorithm\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 20\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide37.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide37\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at this fascinating experiment. Here, subjects were shown two different types of images — human faces and buildings. The MRI scans you see here show how the brain reacts differently in each case. When a person looks at a face, certain brain regions light up — you can see those areas in red and orange. When the same person looks at a building, other regions become more active. This happens because the brain’s oxygen-rich blood flow changes depending on what you are thinking or perceiving.\\n\\nWhen neurons in a region start firing, they consume more oxygen. The body responds by sending in oxygenated blood, which creates a stronger MRI signal in that area. This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast. By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing — for example, a face versus a house. This was one of the early experiments demonstrating how brain activity reflects thought and perception.\\n\\nIn more recent studies, researchers have gone even further. Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing — essentially allowing us to peek into human perception. It’s an incredible step toward understanding how the brain encodes visual information. If you have time, I highly recommend watching a short video on this topic — it’s truly amazing to see how far this technology has come.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"brain\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"oxygenated blood flow\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"When neurons in a region start firing, they consume more oxygen. The body responds by sending in oxygenated blood, which creates a stronger MRI signal in that area. This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 20\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide37.json",
        "parsed": {
          "concepts": [
            {
              "term": "MRI scans",
              "category": "modality"
            },
            {
              "term": "BOLD effect",
              "category": "physics"
            },
            {
              "term": "Blood Oxygen Level Dependent contrast",
              "category": "physics"
            },
            {
              "term": "signal patterns",
              "category": "signal_processing"
            },
            {
              "term": "machine learning",
              "category": "ai_ml"
            },
            {
              "term": "reconstruct",
              "category": "reconstruction"
            }
          ],
          "evidence": [
            "The MRI scans you see here show how the brain reacts differently in each case.",
            "This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast.",
            "By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing.",
            "Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing.",
            "understanding how the brain encodes visual information."
          ]
        },
        "raw": "{\"slide_id\": \"Slide37\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"MRI scans\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"BOLD effect\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"Blood Oxygen Level Dependent contrast\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"signal patterns\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"machine learning\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"reconstruct\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"visual information\\\", \\\"category\\\": \\\"anatomy\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"The MRI scans you see here show how the brain reacts differently in each case.\\\",\\n    \\\"This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast.\\\",\\n    \\\"By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing.\\\",\\n    \\\"Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing.\\\",\\n    \\\"understanding how the brain encodes visual information.\\\"\\n  ]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"MRI scans\", \"category\": \"modality\"}, {\"term\": \"BOLD effect\", \"category\": \"physics\"}, {\"term\": \"Blood Oxygen Level Dependent contrast\", \"category\": \"physics\"}, {\"term\": \"signal patterns\", \"category\": \"signal_processing\"}, {\"term\": \"machine learning\", \"category\": \"ai_ml\"}, {\"term\": \"reconstruct\", \"category\": \"reconstruction\"}], \"evidence\": [\"The MRI scans you see here show how the brain reacts differently in each case.\", \"This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast.\", \"By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing.\", \"Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing.\", \"understanding how the brain encodes visual information.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 20\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide37.json",
        "parsed": {
          "triples": [
            {
              "s": "MRI scans",
              "p": "measures",
              "o": "brain activity",
              "modalities": [
                "text",
                "image"
              ],
              "confidence": 1.0,
              "evidence": "The MRI scans you see here show how the brain reacts differently in each case."
            },
            {
              "s": "BOLD effect",
              "p": "depends_on",
              "o": "oxygen-rich blood flow",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast."
            },
            {
              "s": "advanced image analysis and machine learning",
              "p": "reconstructs_with",
              "o": "visual information",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing"
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide37\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"MRI scans\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"brain activity\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"The MRI scans you see here show how the brain reacts differently in each case.\\\"},\\n    {\\\"s\\\":\\\"BOLD effect\\\", \\\"p\\\":\\\"depends_on\\\", \\\"o\\\":\\\"oxygen-rich blood flow\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast.\\\"},\\n    {\\\"s\\\":\\\"advanced image analysis and machine learning\\\", \\\"p\\\":\\\"reconstructs_with\\\", \\\"o\\\":\\\"visual information\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"MRI scans\", \"p\": \"measures\", \"o\": \"brain activity\", \"modalities\": [\"text\", \"image\"], \"confidence\": 1.0, \"evidence\": \"The MRI scans you see here show how the brain reacts differently in each case.\"}, {\"s\": \"BOLD effect\", \"p\": \"depends_on\", \"o\": \"oxygen-rich blood flow\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast.\"}, {\"s\": \"advanced image analysis and machine learning\", \"p\": \"reconstructs_with\", \"o\": \"visual information\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing\"}]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 20\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide37.json",
        "parsed": {
          "term": "video",
          "category": "communication"
        },
        "raw": "{\"slide_id\": \"Slide37\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T15:47:00.838180+00:00\", \"text_length\": 1485, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at this fascinating experiment. Here, subjects were shown two different types of images — human faces and buildings. The MRI scans you see here show how the brain reacts differently in each case. When a person looks at a face, certain brain regions light up — you can see those areas in red and orange. When the same person looks at a building, other regions become more active. This happens because the brain’s oxygen-rich blood flow changes depending on what you are thinking or perceiving.\\n\\nWhen neurons in a region start firing, they consume more oxygen. The body responds by sending in oxygenated blood, which creates a stronger MRI signal in that area. This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast. By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing — for example, a face versus a house. This was one of the early experiments demonstrating how brain activity reflects thought and perception.\\n\\nIn more recent studies, researchers have gone even further. Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing — essentially allowing us to peek into human perception. It’s an incredible step toward understanding how the brain encodes visual information. If you have time, I highly recommend watching a short video on this topic — it’s truly amazing to see how far this technology has come.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"MRI scans\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"Blood Oxygen Level Dependent contrast\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"BOLD effect\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"brain activity\\\", \\\"category\\\": \\\"anatomy\\\"},\\n    {\\\"term\\\": \\\"neurons\\\", \\\"category\\\": \\\"anatomy\\\"},\\n    {\\\"term\\\": \\\"oxygenated blood\\\", \\\"category\\\": \\\"physiology\\\"},\\n    {\\\"term\\\": \\\"statistical analysis\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"machine learning\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"reconstruction\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"human perception\\\", \\\"category\\\": \\\"anatomy\\\"},\\n    {\\\"term\\\": \\\"visual information\\\", \\\"category\\\": \\\"anatomy\\\"},\\n    {\\\"term\\\": \\\"advanced image analysis\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"rough resolution\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"video\\\", \\\"category\\\": \\\"communication\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"This was one of the early experiments demonstrating how brain activity reflects thought and perception.\\\", \\\"Using advanced image analysis and machine learning\", \"parsed\": {\"term\": \"video\", \"category\": \"communication\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 20\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide37.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide37\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T16:45:00.892930+00:00\", \"text_length\": 1485, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at this fascinating experiment. Here, subjects were shown two different types of images — human faces and buildings. The MRI scans you see here show how the brain reacts differently in each case. When a person looks at a face, certain brain regions light up — you can see those areas in red and orange. When the same person looks at a building, other regions become more active. This happens because the brain’s oxygen-rich blood flow changes depending on what you are thinking or perceiving.\\n\\nWhen neurons in a region start firing, they consume more oxygen. The body responds by sending in oxygenated blood, which creates a stronger MRI signal in that area. This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast. By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing — for example, a face versus a house. This was one of the early experiments demonstrating how brain activity reflects thought and perception.\\n\\nIn more recent studies, researchers have gone even further. Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing — essentially allowing us to peek into human perception. It’s an incredible step toward understanding how the brain encodes visual information. If you have time, I highly recommend watching a short video on this topic — it’s truly amazing to see how far this technology has come.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 20\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide37.json",
        "parsed": {
          "term": "using advanced image analysis and machine learning",
          "category": "ai_ml"
        },
        "raw": "{\"slide_id\": \"Slide37\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T14:22:52.921753+00:00\", \"text_length\": 1485, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at this fascinating experiment. Here, subjects were shown two different types of images — human faces and buildings. The MRI scans you see here show how the brain reacts differently in each case. When a person looks at a face, certain brain regions light up — you can see those areas in red and orange. When the same person looks at a building, other regions become more active. This happens because the brain’s oxygen-rich blood flow changes depending on what you are thinking or perceiving.\\n\\nWhen neurons in a region start firing, they consume more oxygen. The body responds by sending in oxygenated blood, which creates a stronger MRI signal in that area. This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast. By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing — for example, a face versus a house. This was one of the early experiments demonstrating how brain activity reflects thought and perception.\\n\\nIn more recent studies, researchers have gone even further. Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing — essentially allowing us to peek into human perception. It’s an incredible step toward understanding how the brain encodes visual information. If you have time, I highly recommend watching a short video on this topic — it’s truly amazing to see how far this technology has come.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"BOLD effect\\\",\\n      \\\"category\\\": \\\"physics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"MRI scans\\\",\\n      \\\"category\\\": \\\"modality\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"oxygen-rich blood flow\\\",\\n      \\\"category\\\": \\\"physics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"brain activity reflects thought and perception\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"reconstruct, with rough resolution, what a person is actually seeing\\\",\\n      \\\"category\\\": \\\"reconstruction\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"using advanced image analysis and machine learning\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    }\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"When neurons in a region start firing, they consume more oxygen. The body responds by sending in oxygenated blood, which creates a stronger MRI signal in that area. This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast.\\\",\\n    \\\"By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing — for example, a face versus a house.\\\",\\n    \\\"In more recent studies, researchers have gone even further. Using advanced image\", \"parsed\": {\"term\": \"using advanced image analysis and machine learning\", \"category\": \"ai_ml\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 20\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide37.json",
        "parsed": {
          "s": "the MRI signal",
          "p": "produces",
          "o": "BOLD effect",
          "modalities": [
            "text",
            "image"
          ],
          "confidence": 0.9,
          "evidence": [
            "This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast."
          ]
        },
        "raw": "{\"slide_id\": \"Slide37\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T14:31:03.267994+00:00\", \"text_length\": 1485, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at this fascinating experiment. Here, subjects were shown two different types of images — human faces and buildings. The MRI scans you see here show how the brain reacts differently in each case. When a person looks at a face, certain brain regions light up — you can see those areas in red and orange. When the same person looks at a building, other regions become more active. This happens because the brain’s oxygen-rich blood flow changes depending on what you are thinking or perceiving.\\n\\nWhen neurons in a region start firing, they consume more oxygen. The body responds by sending in oxygenated blood, which creates a stronger MRI signal in that area. This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast. By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing — for example, a face versus a house. This was one of the early experiments demonstrating how brain activity reflects thought and perception.\\n\\nIn more recent studies, researchers have gone even further. Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing — essentially allowing us to peek into human perception. It’s an incredible step toward understanding how the brain encodes visual information. If you have time, I highly recommend watching a short video on this topic — it’s truly amazing to see how far this technology has come.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"the brain\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"oxygen-rich blood flow changes\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"When neurons in a region start firing, they consume more oxygen. The body responds by sending in oxygenated blood, which creates a stronger MRI signal in that area.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"the MRI signal\\\",\\n      \\\"p\\\": \\\"produces\\\",\\n      \\\"o\\\": \\\"BOLD effect\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"advanced image analysis and machine learning\\\",\\n      \\\"p\\\": \\\"reconstructs_with\\\",\\n      \\\"o\\\": \\\"what a person is actually seeing\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.8,\\n      \\\"evidence\\\": \\\"Using advanced image analysis and machine learning\", \"parsed\": {\"s\": \"the MRI signal\", \"p\": \"produces\", \"o\": \"BOLD effect\", \"modalities\": [\"text\", \"image\"], \"confidence\": 0.9, \"evidence\": [\"This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast.\"]}}"
      }
    }
  }
}