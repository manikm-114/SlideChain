{
  "lecture": "Lecture 16",
  "slide_id": "Slide54",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 16\\Images\\Slide54.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 16\\Texts\\Slide54.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 16\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide54.json",
        "parsed": {
          "term": "time-of-flight technology",
          "category": "instrumentation"
        },
        "raw": "{\"slide_id\": \"Slide54\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nUp to now, we’ve said that coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors. But that’s still quite a lot of uncertainty—it could be anywhere on that line.\\n\\nNow, with time-of-flight detection, we can do better. The idea is straightforward: when the two gamma photons are produced, they leave the annihilation site at the same instant, traveling in opposite directions at the speed of light. If the annihilation happens exactly in the middle, the photons will reach both detectors at the same time. But if the event happens closer to one detector, the photon on that side will arrive slightly earlier.\\n\\nSo by measuring this tiny arrival time difference, we can estimate where along the line the event occurred. Of course, because our timing measurements are not infinitely precise, there is still some uncertainty. That’s why we don’t pinpoint a single location, but instead assign a probability distribution—centered closer to one detector or the other, depending on the measured delay. The effect is illustrated here. Without time-of-flight information, the whole line is equally likely, so the back-projected signal is spread out. With time-of-flight, the probability is concentrated in a smaller region along that line. When you combine millions of such measurements, the reconstructed PET image is sharper, with better contrast and less noise.\\n\\nSo the concept is simple: coincidence detection gives us the line, and time-of-flight narrows it down to a region on that line where the paired emission happened. This is why modern PET scanners increasingly use time-of-flight technology—it significantly improves image quality without requiring extra radiation dose.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"coincidence detection\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"time-of-flight detection\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"probability distribution\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"image quality\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"PET scanner\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"time-of-flight technology\\\", \\\"category\\\": \\\"instrumentation\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"Up to now, we’ve said that coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors.\\\", \\n               \\\"But that’s still quite a lot of uncertainty—it could be anywhere on that line.\\\", \\n               \\\"Now, with time-of-flight detection, we can do better.\\\", \\n               \\\"The idea is straightforward: when the two gamma photons are produced, they leave the annihilation site at the same instant, traveling in opposite directions at the speed of light.\\\", \\n               \\\"If the annihilation happens exactly in the middle, the photons will reach both detectors at the same time.\\\", \\n               \\\"But if the event happens closer to one detector, the photon on that side will\", \"parsed\": {\"term\": \"time-of-flight technology\", \"category\": \"instrumentation\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 16\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide54.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide54\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nUp to now, we’ve said that coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors. But that’s still quite a lot of uncertainty—it could be anywhere on that line.\\n\\nNow, with time-of-flight detection, we can do better. The idea is straightforward: when the two gamma photons are produced, they leave the annihilation site at the same instant, traveling in opposite directions at the speed of light. If the annihilation happens exactly in the middle, the photons will reach both detectors at the same time. But if the event happens closer to one detector, the photon on that side will arrive slightly earlier.\\n\\nSo by measuring this tiny arrival time difference, we can estimate where along the line the event occurred. Of course, because our timing measurements are not infinitely precise, there is still some uncertainty. That’s why we don’t pinpoint a single location, but instead assign a probability distribution—centered closer to one detector or the other, depending on the measured delay. The effect is illustrated here. Without time-of-flight information, the whole line is equally likely, so the back-projected signal is spread out. With time-of-flight, the probability is concentrated in a smaller region along that line. When you combine millions of such measurements, the reconstructed PET image is sharper, with better contrast and less noise.\\n\\nSo the concept is simple: coincidence detection gives us the line, and time-of-flight narrows it down to a region on that line where the paired emission happened. This is why modern PET scanners increasingly use time-of-flight technology—it significantly improves image quality without requiring extra radiation dose.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"coincidence detection\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"annihilation event\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"Up to now, we’ve said that coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 16\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide54.json",
        "parsed": {
          "term": "radiation dose",
          "category": "physics"
        },
        "raw": "{\"slide_id\": \"Slide54\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"coincidence detection\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"annihilation event\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"time-of-flight detection\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"gamma photons\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"arrival time difference\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"probability distribution\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"back-projected signal\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"reconstructed PET image\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"image quality\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"time-of-flight technology\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"radiation dose\\\", \\\"category\\\": \\\"physics\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors\\\",\\n    \\\"when the two gamma photons are produced, they leave the annihilation site at the same instant\\\",\\n    \\\"by measuring this tiny arrival time difference, we can estimate where along the line the event occurred\\\",\", \"parsed\": {\"term\": \"radiation dose\", \"category\": \"physics\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 16\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide54.json",
        "parsed": {
          "s": "time-of-flight detection",
          "p": "assigns",
          "o": "probability distribution",
          "modalities": [
            "text"
          ],
          "confidence": 0.9,
          "evidence": [
            "That’s why we don’t pinpoint a single location, but instead assign a probability distribution-centered closer to one detector or the other, depending on the measured delay."
          ]
        },
        "raw": "{\"slide_id\": \"Slide54\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"time-of-flight detection\\\",\\n      \\\"p\\\": \\\"improves\\\",\\n      \\\"o\\\": \\\"image quality\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"This is why modern PET scanners increasingly use time-of-flight technology—it significantly improves image quality without requiring extra radiation dose.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"time-of-flight detection\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"arrival time difference\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"So by measuring this tiny arrival time difference, we can estimate where along the line the event occurred.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"time-of-flight detection\\\",\\n      \\\"p\\\": \\\"assigns\\\",\\n      \\\"o\\\": \\\"probability distribution\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"That’s why we don’t pinpoint a single location, but instead assign a probability distribution—centered closer to one detector or the other, depending on the measured delay.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"time-of-flight detection\", \"parsed\": {\"s\": \"time-of-flight detection\", \"p\": \"assigns\", \"o\": \"probability distribution\", \"modalities\": [\"text\"], \"confidence\": 0.9, \"evidence\": [\"That’s why we don’t pinpoint a single location, but instead assign a probability distribution-centered closer to one detector or the other, depending on the measured delay.\"]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 16\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide54.json",
        "parsed": {
          "concepts": [
            {
              "term": "coincidence detection",
              "category": "algorithm"
            },
            {
              "term": "time-of-flight detection",
              "category": "algorithm"
            },
            {
              "term": "gamma photons",
              "category": "physics"
            },
            {
              "term": "speed of light",
              "category": "physics"
            },
            {
              "term": "arrival time difference",
              "category": "signal_processing"
            },
            {
              "term": "probability distribution",
              "category": "mathematics"
            },
            {
              "term": "back-projected signal",
              "category": "signal_processing"
            },
            {
              "term": "reconstructed PET image",
              "category": "algorithm"
            },
            {
              "term": "image quality",
              "category": "quality_metric"
            },
            {
              "term": "time-of-flight technology",
              "category": "algorithm"
            },
            {
              "term": "modern PET scanners",
              "category": "modality"
            }
          ],
          "evidence": [
            "coincidence detection gives us the line",
            "time-of-flight narrows it down to a region on that line where the paired emission happened",
            "modern PET scanners increasingly use time-of-flight technology-it significantly improves image quality without requiring extra radiation dose"
          ]
        },
        "raw": "{\"slide_id\": \"Slide54\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T11:16:34.554187+00:00\", \"text_length\": 1744, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nUp to now, we’ve said that coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors. But that’s still quite a lot of uncertainty—it could be anywhere on that line.\\n\\nNow, with time-of-flight detection, we can do better. The idea is straightforward: when the two gamma photons are produced, they leave the annihilation site at the same instant, traveling in opposite directions at the speed of light. If the annihilation happens exactly in the middle, the photons will reach both detectors at the same time. But if the event happens closer to one detector, the photon on that side will arrive slightly earlier.\\n\\nSo by measuring this tiny arrival time difference, we can estimate where along the line the event occurred. Of course, because our timing measurements are not infinitely precise, there is still some uncertainty. That’s why we don’t pinpoint a single location, but instead assign a probability distribution—centered closer to one detector or the other, depending on the measured delay. The effect is illustrated here. Without time-of-flight information, the whole line is equally likely, so the back-projected signal is spread out. With time-of-flight, the probability is concentrated in a smaller region along that line. When you combine millions of such measurements, the reconstructed PET image is sharper, with better contrast and less noise.\\n\\nSo the concept is simple: coincidence detection gives us the line, and time-of-flight narrows it down to a region on that line where the paired emission happened. This is why modern PET scanners increasingly use time-of-flight technology—it significantly improves image quality without requiring extra radiation dose.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"coincidence detection\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"time-of-flight detection\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"gamma photons\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"speed of light\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"arrival time difference\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"probability distribution\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"back-projected signal\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"reconstructed PET image\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"image quality\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"time-of-flight technology\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"modern PET scanners\\\", \\\"category\\\": \\\"modality\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"coincidence detection gives us the line\\\",\\n    \\\"time-of-flight narrows it down to a region on that line where the paired emission happened\\\",\\n    \\\"modern PET scanners increasingly use time-of-flight technology—it significantly improves image quality without requiring extra radiation dose\\\"\\n  ]\\n}\", \"parsed\": {\"concepts\": [{\"term\": \"coincidence detection\", \"category\": \"algorithm\"}, {\"term\": \"time-of-flight detection\", \"category\": \"algorithm\"}, {\"term\": \"gamma photons\", \"category\": \"physics\"}, {\"term\": \"speed of light\", \"category\": \"physics\"}, {\"term\": \"arrival time difference\", \"category\": \"signal_processing\"}, {\"term\": \"probability distribution\", \"category\": \"mathematics\"}, {\"term\": \"back-projected signal\", \"category\": \"signal_processing\"}, {\"term\": \"reconstructed PET image\", \"category\": \"algorithm\"}, {\"term\": \"image quality\", \"category\": \"quality_metric\"}, {\"term\": \"time-of-flight technology\", \"category\": \"algorithm\"}, {\"term\": \"modern PET scanners\", \"category\": \"modality\"}], \"evidence\": [\"coincidence detection gives us the line\", \"time-of-flight narrows it down to a region on that line where the paired emission happened\", \"modern PET scanners increasingly use time-of-flight technology-it significantly improves image quality without requiring extra radiation dose\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 16\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide54.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide54\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T11:58:51.435113+00:00\", \"text_length\": 1744, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nUp to now, we’ve said that coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors. But that’s still quite a lot of uncertainty—it could be anywhere on that line.\\n\\nNow, with time-of-flight detection, we can do better. The idea is straightforward: when the two gamma photons are produced, they leave the annihilation site at the same instant, traveling in opposite directions at the speed of light. If the annihilation happens exactly in the middle, the photons will reach both detectors at the same time. But if the event happens closer to one detector, the photon on that side will arrive slightly earlier.\\n\\nSo by measuring this tiny arrival time difference, we can estimate where along the line the event occurred. Of course, because our timing measurements are not infinitely precise, there is still some uncertainty. That’s why we don’t pinpoint a single location, but instead assign a probability distribution—centered closer to one detector or the other, depending on the measured delay. The effect is illustrated here. Without time-of-flight information, the whole line is equally likely, so the back-projected signal is spread out. With time-of-flight, the probability is concentrated in a smaller region along that line. When you combine millions of such measurements, the reconstructed PET image is sharper, with better contrast and less noise.\\n\\nSo the concept is simple: coincidence detection gives us the line, and time-of-flight narrows it down to a region on that line where the paired emission happened. This is why modern PET scanners increasingly use time-of-flight technology—it significantly improves image quality without requiring extra radiation dose.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"time-of-flight detection\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"gamma photons\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"gamma photons leave the annihilation site at the same instant, traveling in opposite directions at the speed of light.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 16\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide54.json",
        "parsed": {
          "term": "by measuring this tiny arrival time difference, we can estimate where along the line the event occurred",
          "category": "reconstruction"
        },
        "raw": "{\"slide_id\": \"Slide54\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-12T19:15:23.739640+00:00\", \"text_length\": 1744, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nUp to now, we’ve said that coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors. But that’s still quite a lot of uncertainty—it could be anywhere on that line.\\n\\nNow, with time-of-flight detection, we can do better. The idea is straightforward: when the two gamma photons are produced, they leave the annihilation site at the same instant, traveling in opposite directions at the speed of light. If the annihilation happens exactly in the middle, the photons will reach both detectors at the same time. But if the event happens closer to one detector, the photon on that side will arrive slightly earlier.\\n\\nSo by measuring this tiny arrival time difference, we can estimate where along the line the event occurred. Of course, because our timing measurements are not infinitely precise, there is still some uncertainty. That’s why we don’t pinpoint a single location, but instead assign a probability distribution—centered closer to one detector or the other, depending on the measured delay. The effect is illustrated here. Without time-of-flight information, the whole line is equally likely, so the back-projected signal is spread out. With time-of-flight, the probability is concentrated in a smaller region along that line. When you combine millions of such measurements, the reconstructed PET image is sharper, with better contrast and less noise.\\n\\nSo the concept is simple: coincidence detection gives us the line, and time-of-flight narrows it down to a region on that line where the paired emission happened. This is why modern PET scanners increasingly use time-of-flight technology—it significantly improves image quality without requiring extra radiation dose.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors\\\",\\n      \\\"category\\\": \\\"reconstruction\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"time-of-flight detection, we can do better\\\",\\n      \\\"category\\\": \\\"reconstruction\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"when the two gamma photons are produced, they leave the annihilation site at the same instant, traveling in opposite directions at the speed of light\\\",\\n      \\\"category\\\": \\\"physics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"the photons will reach both detectors at the same time\\\",\\n      \\\"category\\\": \\\"physics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"if the event happens closer to one detector, the photon on that side will arrive slightly earlier\\\",\\n      \\\"category\\\": \\\"physics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"by measuring this tiny arrival time difference, we can estimate where along the line the event occurred\\\",\\n      \\\"category\\\": \\\"reconstruction\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"there is still some uncertainty. That’s why we don’t pinpoint a single location, but instead assign a probability distribution—centered closer to one detector or the other\", \"parsed\": {\"term\": \"by measuring this tiny arrival time difference, we can estimate where along the line the event occurred\", \"category\": \"reconstruction\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 16\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide54.json",
        "parsed": {
          "s": "time-of-flight detection",
          "p": "produces",
          "o": "probability distribution",
          "modalities": [
            "text",
            "image"
          ],
          "confidence": 0.9,
          "evidence": [
            "there is still some uncertainty. That's why we don't pinpoint a single location, but instead assign a probability distribution-centered closer to one detector or the other, depending on the measured delay."
          ]
        },
        "raw": "{\"slide_id\": \"Slide54\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-12T19:21:33.350115+00:00\", \"text_length\": 1744, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nUp to now, we’ve said that coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors. But that’s still quite a lot of uncertainty—it could be anywhere on that line.\\n\\nNow, with time-of-flight detection, we can do better. The idea is straightforward: when the two gamma photons are produced, they leave the annihilation site at the same instant, traveling in opposite directions at the speed of light. If the annihilation happens exactly in the middle, the photons will reach both detectors at the same time. But if the event happens closer to one detector, the photon on that side will arrive slightly earlier.\\n\\nSo by measuring this tiny arrival time difference, we can estimate where along the line the event occurred. Of course, because our timing measurements are not infinitely precise, there is still some uncertainty. That’s why we don’t pinpoint a single location, but instead assign a probability distribution—centered closer to one detector or the other, depending on the measured delay. The effect is illustrated here. Without time-of-flight information, the whole line is equally likely, so the back-projected signal is spread out. With time-of-flight, the probability is concentrated in a smaller region along that line. When you combine millions of such measurements, the reconstructed PET image is sharper, with better contrast and less noise.\\n\\nSo the concept is simple: coincidence detection gives us the line, and time-of-flight narrows it down to a region on that line where the paired emission happened. This is why modern PET scanners increasingly use time-of-flight technology—it significantly improves image quality without requiring extra radiation dose.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"time-of-flight detection\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"arrival time difference\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"by measuring this tiny arrival time difference, we can estimate where along the line the event occurred.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"time-of-flight detection\\\",\\n      \\\"p\\\": \\\"produces\\\",\\n      \\\"o\\\": \\\"probability distribution\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"there is still some uncertainty. That’s why we don’t pinpoint a single location, but instead assign a probability distribution—centered closer to one detector or the other, depending on the measured delay.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"time-of-flight detection\\\",\\n      \\\"p\\\": \\\"reconstructs_with\\\",\\n      \\\"o\\\": \\\"PET image\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"When you combine millions of\", \"parsed\": {\"s\": \"time-of-flight detection\", \"p\": \"produces\", \"o\": \"probability distribution\", \"modalities\": [\"text\", \"image\"], \"confidence\": 0.9, \"evidence\": [\"there is still some uncertainty. That's why we don't pinpoint a single location, but instead assign a probability distribution-centered closer to one detector or the other, depending on the measured delay.\"]}}"
      }
    }
  }
}