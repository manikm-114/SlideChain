{
  "lecture": "Lecture 11",
  "slide_id": "Slide14",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide14.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide14.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide14.json",
        "parsed": {
          "term": "image quality",
          "category": "communication"
        },
        "raw": "{\"slide_id\": \"Slide14\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at an example to see how the Structural Similarity Index, or SSIM, actually works in practice.\\nOn the top left, we have the reference image — the original, undistorted version. If we compare this image with itself, the SSIM score is exactly 1. That makes sense, because they are identical.\\n\\nNow, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989. These numbers are very close to one, reflecting the fact that the images are almost identical to our eyes.\\nOn the bottom row, however, we see very different results. One image has been heavily pixelated, another blurred, and another corrupted with noise. When compared to the original, their SSIM values drop significantly — around 0.67, 0.69, and about 0.72. That means they retain only about two-thirds of the structural similarity.\\n\\nThe important thing here is that SSIM values line up with what we visually perceive. Images that look good to us score close to one. Images that look distorted or degraded score much lower. That is the power of SSIM — it bridges the gap between mathematical measurement and human vision.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Structural Similarity Index\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"SSIM\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"reference image\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"distorted version\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"pixelated\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"blurred\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"corrupted with noise\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"SSIM values\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"mathematical measurement\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"human vision\\\", \\\"category\\\": \\\"anatomy\\\"},\\n    {\\\"term\\\": \\\"SSIM\\\", \\\"category\\\": \\\"software\\\"},\\n    {\\\"term\\\": \\\"image quality\\\", \\\"category\\\": \\\"communication\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"On the top left, we have the reference image — the original, undistorted version. If we compare this image with itself, the SSIM score is exactly 1. That makes sense, because they are identical.\\\", \\n             \\\"Now, compare\", \"parsed\": {\"term\": \"image quality\", \"category\": \"communication\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide14.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide14\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at an example to see how the Structural Similarity Index, or SSIM, actually works in practice.\\nOn the top left, we have the reference image — the original, undistorted version. If we compare this image with itself, the SSIM score is exactly 1. That makes sense, because they are identical.\\n\\nNow, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989. These numbers are very close to one, reflecting the fact that the images are almost identical to our eyes.\\nOn the bottom row, however, we see very different results. One image has been heavily pixelated, another blurred, and another corrupted with noise. When compared to the original, their SSIM values drop significantly — around 0.67, 0.69, and about 0.72. That means they retain only about two-thirds of the structural similarity.\\n\\nThe important thing here is that SSIM values line up with what we visually perceive. Images that look good to us score close to one. Images that look distorted or degraded score much lower. That is the power of SSIM — it bridges the gap between mathematical measurement and human vision.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Structural Similarity Index (SSIM)\\\", \\\"p\\\":\\\"measures\\\",\\n     \\\"o\\\":\\\"Structural Similarity Index (SSIM)\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"The SSIM score is exactly 1 when comparing the reference image with itself, because they are identical.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide14.json",
        "parsed": {
          "concepts": [
            {
              "term": "Structural Similarity Index",
              "category": "quality_metric"
            },
            {
              "term": "SSIM score",
              "category": "quality_metric"
            },
            {
              "term": "identical",
              "category": "quality_metric"
            },
            {
              "term": "pixelated",
              "category": "quality_metric"
            },
            {
              "term": "blurred",
              "category": "quality_metric"
            },
            {
              "term": "noise",
              "category": "quality_metric"
            },
            {
              "term": "SSIM values",
              "category": "quality_metric"
            },
            {
              "term": "structural similarity",
              "category": "quality_metric"
            }
          ],
          "evidence": [
            "Structural Similarity Index, or SSIM",
            "SSIM score is exactly 1",
            "identical",
            "heavily pixelated",
            "another blurred",
            "corrupted with noise",
            "SSIM values drop significantly",
            "structural similarity"
          ]
        },
        "raw": "{\"slide_id\": \"Slide14\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Structural Similarity Index\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"SSIM score\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"identical\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"pixelated\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"blurred\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"noise\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"SSIM values\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"structural similarity\\\", \\\"category\\\": \\\"quality_metric\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Structural Similarity Index, or SSIM\\\",\\n    \\\"SSIM score is exactly 1\\\",\\n    \\\"identical\\\",\\n    \\\"heavily pixelated\\\",\\n    \\\"another blurred\\\",\\n    \\\"corrupted with noise\\\",\\n    \\\"SSIM values drop significantly\\\",\\n    \\\"structural similarity\\\"\\n  ]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"Structural Similarity Index\", \"category\": \"quality_metric\"}, {\"term\": \"SSIM score\", \"category\": \"quality_metric\"}, {\"term\": \"identical\", \"category\": \"quality_metric\"}, {\"term\": \"pixelated\", \"category\": \"quality_metric\"}, {\"term\": \"blurred\", \"category\": \"quality_metric\"}, {\"term\": \"noise\", \"category\": \"quality_metric\"}, {\"term\": \"SSIM values\", \"category\": \"quality_metric\"}, {\"term\": \"structural similarity\", \"category\": \"quality_metric\"}], \"evidence\": [\"Structural Similarity Index, or SSIM\", \"SSIM score is exactly 1\", \"identical\", \"heavily pixelated\", \"another blurred\", \"corrupted with noise\", \"SSIM values drop significantly\", \"structural similarity\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide14.json",
        "parsed": {
          "triples": [
            {
              "s": "SSIM",
              "p": "measures",
              "o": "structural similarity",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "The important thing here is that SSIM values line up with what we visually perceive."
            },
            {
              "s": "SSIM",
              "p": "produces",
              "o": "SSIM score",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "If we compare this image with itself, the SSIM score is exactly 1."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide14\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"SSIM\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"structural similarity\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"The important thing here is that SSIM values line up with what we visually perceive.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"SSIM\\\",\\n      \\\"p\\\": \\\"produces\\\",\\n      \\\"o\\\": \\\"SSIM score\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"If we compare this image with itself, the SSIM score is exactly 1.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"SSIM\\\",\\n      \\\"p\\\": \\\"represents\\\",\\n      \\\"o\\\": \\\"visual perception\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"SSIM values line up with what we visually perceive.\\\"\\n    }\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"SSIM\", \"p\": \"measures\", \"o\": \"structural similarity\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"The important thing here is that SSIM values line up with what we visually perceive.\"}, {\"s\": \"SSIM\", \"p\": \"produces\", \"o\": \"SSIM score\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"If we compare this image with itself, the SSIM score is exactly 1.\"}]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide14.json",
        "parsed": {
          "concepts": [
            {
              "term": "Structural Similarity Index",
              "category": "quality_metric"
            },
            {
              "term": "SSIM",
              "category": "quality_metric"
            },
            {
              "term": "reference image",
              "category": "algorithm"
            },
            {
              "term": "undistorted version",
              "category": "algorithm"
            },
            {
              "term": "pixelated",
              "category": "signal_processing"
            },
            {
              "term": "blurred",
              "category": "signal_processing"
            },
            {
              "term": "corrupted with noise",
              "category": "signal_processing"
            },
            {
              "term": "SSIM score",
              "category": "quality_metric"
            },
            {
              "term": "human vision",
              "category": "quality_metric"
            },
            {
              "term": "mathematical measurement",
              "category": "quality_metric"
            }
          ],
          "evidence": [
            "SSIM score is exactly 1",
            "SSIM values are very close to one",
            "SSIM values drop significantly",
            "SSIM values line up with what we visually perceive",
            "SSIM values bridge the gap between mathematical measurement and human vision"
          ]
        },
        "raw": "{\"slide_id\": \"Slide14\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:43:28.062012+00:00\", \"text_length\": 1198, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at an example to see how the Structural Similarity Index, or SSIM, actually works in practice.\\nOn the top left, we have the reference image — the original, undistorted version. If we compare this image with itself, the SSIM score is exactly 1. That makes sense, because they are identical.\\n\\nNow, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989. These numbers are very close to one, reflecting the fact that the images are almost identical to our eyes.\\nOn the bottom row, however, we see very different results. One image has been heavily pixelated, another blurred, and another corrupted with noise. When compared to the original, their SSIM values drop significantly — around 0.67, 0.69, and about 0.72. That means they retain only about two-thirds of the structural similarity.\\n\\nThe important thing here is that SSIM values line up with what we visually perceive. Images that look good to us score close to one. Images that look distorted or degraded score much lower. That is the power of SSIM — it bridges the gap between mathematical measurement and human vision.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Structural Similarity Index\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"SSIM\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"reference image\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"undistorted version\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"pixelated\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"blurred\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"corrupted with noise\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"SSIM score\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"human vision\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"mathematical measurement\\\", \\\"category\\\": \\\"quality_metric\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"SSIM score is exactly 1\\\",\\n    \\\"SSIM values are very close to one\\\",\\n    \\\"SSIM values drop significantly\\\",\\n    \\\"SSIM values line up with what we visually perceive\\\",\\n    \\\"SSIM values bridge the gap between mathematical measurement and human vision\\\"\\n  ]\\n}\", \"parsed\": {\"concepts\": [{\"term\": \"Structural Similarity Index\", \"category\": \"quality_metric\"}, {\"term\": \"SSIM\", \"category\": \"quality_metric\"}, {\"term\": \"reference image\", \"category\": \"algorithm\"}, {\"term\": \"undistorted version\", \"category\": \"algorithm\"}, {\"term\": \"pixelated\", \"category\": \"signal_processing\"}, {\"term\": \"blurred\", \"category\": \"signal_processing\"}, {\"term\": \"corrupted with noise\", \"category\": \"signal_processing\"}, {\"term\": \"SSIM score\", \"category\": \"quality_metric\"}, {\"term\": \"human vision\", \"category\": \"quality_metric\"}, {\"term\": \"mathematical measurement\", \"category\": \"quality_metric\"}], \"evidence\": [\"SSIM score is exactly 1\", \"SSIM values are very close to one\", \"SSIM values drop significantly\", \"SSIM values line up with what we visually perceive\", \"SSIM values bridge the gap between mathematical measurement and human vision\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide14.json",
        "parsed": {
          "s": "Structural Similarity Index (SSIM)",
          "p": "measures",
          "o": "image quality",
          "modalities": [
            "text",
            "image"
          ],
          "confidence": 1.0,
          "evidence": [
            "Now, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989. These numbers are very close to one, reflecting the fact that the images are almost identical to our eyes. On the bottom row, however, we see very different results. One image has been heavily pixelated, another blurred, and another corrupted with noise. When compared to the original, their SSIM values drop significantly - around 0.67, 0.69, and about 0.72. That means they retain only about two-thirds of the structural similarity. The important thing here is that SSIM values line up with what we visually perceive. Images that look good to us score close to one. Images that look distorted or degraded score much lower. That is the power of SSIM - it bridges the gap between mathematical measurement and human vision."
          ]
        },
        "raw": "{\"slide_id\": \"Slide14\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T05:27:45.126691+00:00\", \"text_length\": 1198, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at an example to see how the Structural Similarity Index, or SSIM, actually works in practice.\\nOn the top left, we have the reference image — the original, undistorted version. If we compare this image with itself, the SSIM score is exactly 1. That makes sense, because they are identical.\\n\\nNow, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989. These numbers are very close to one, reflecting the fact that the images are almost identical to our eyes.\\nOn the bottom row, however, we see very different results. One image has been heavily pixelated, another blurred, and another corrupted with noise. When compared to the original, their SSIM values drop significantly — around 0.67, 0.69, and about 0.72. That means they retain only about two-thirds of the structural similarity.\\n\\nThe important thing here is that SSIM values line up with what we visually perceive. Images that look good to us score close to one. Images that look distorted or degraded score much lower. That is the power of SSIM — it bridges the gap between mathematical measurement and human vision.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Structural Similarity Index (SSIM)\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"image quality\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Now, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989. These numbers are very close to one, reflecting the fact that the images are almost identical to our eyes. On the bottom row, however, we see very different results. One image has been heavily pixelated, another blurred, and another corrupted with noise. When compared to the original, their SSIM values drop significantly — around 0.67, 0.69, and about 0.72. That means they retain only about two-thirds of the structural similarity. The important thing here is that SSIM values line up with what we visually perceive. Images that look good to us score close to one. Images that look distorted or degraded score much lower. That is the power of SSIM — it bridges the gap between mathematical measurement and human vision.\\\"}\", \"parsed\": {\"s\": \"Structural Similarity Index (SSIM)\", \"p\": \"measures\", \"o\": \"image quality\", \"modalities\": [\"text\", \"image\"], \"confidence\": 1.0, \"evidence\": [\"Now, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989. These numbers are very close to one, reflecting the fact that the images are almost identical to our eyes. On the bottom row, however, we see very different results. One image has been heavily pixelated, another blurred, and another corrupted with noise. When compared to the original, their SSIM values drop significantly - around 0.67, 0.69, and about 0.72. That means they retain only about two-thirds of the structural similarity. The important thing here is that SSIM values line up with what we visually perceive. Images that look good to us score close to one. Images that look distorted or degraded score much lower. That is the power of SSIM - it bridges the gap between mathematical measurement and human vision.\"]}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide14.json",
        "parsed": {
          "term": "images that look good to us score close to one",
          "category": "quality_metric"
        },
        "raw": "{\"slide_id\": \"Slide14\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:02:12.551539+00:00\", \"text_length\": 1198, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at an example to see how the Structural Similarity Index, or SSIM, actually works in practice.\\nOn the top left, we have the reference image — the original, undistorted version. If we compare this image with itself, the SSIM score is exactly 1. That makes sense, because they are identical.\\n\\nNow, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989. These numbers are very close to one, reflecting the fact that the images are almost identical to our eyes.\\nOn the bottom row, however, we see very different results. One image has been heavily pixelated, another blurred, and another corrupted with noise. When compared to the original, their SSIM values drop significantly — around 0.67, 0.69, and about 0.72. That means they retain only about two-thirds of the structural similarity.\\n\\nThe important thing here is that SSIM values line up with what we visually perceive. Images that look good to us score close to one. Images that look distorted or degraded score much lower. That is the power of SSIM — it bridges the gap between mathematical measurement and human vision.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"structural similarity index, or ssim\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"reference image\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"ssim=1\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"ssim=0.949\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"ssim=0.989\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"ssim=0.671\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"ssim=0.688\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"mssim=0.723\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"images that look good to us score close to one\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"images that look distorted or degraded score much lower\\\",\\n      \\\"category\\\": \\\"quality\", \"parsed\": {\"term\": \"images that look good to us score close to one\", \"category\": \"quality_metric\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide14.json",
        "parsed": {
          "s": "SSIM=0.949",
          "p": "measures",
          "o": "the image next to the reference image",
          "modalities": [
            "text",
            "image"
          ],
          "confidence": 0.95,
          "evidence": [
            "Now, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989."
          ]
        },
        "raw": "{\"slide_id\": \"Slide14\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:11:58.145369+00:00\", \"text_length\": 1198, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s look at an example to see how the Structural Similarity Index, or SSIM, actually works in practice.\\nOn the top left, we have the reference image — the original, undistorted version. If we compare this image with itself, the SSIM score is exactly 1. That makes sense, because they are identical.\\n\\nNow, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989. These numbers are very close to one, reflecting the fact that the images are almost identical to our eyes.\\nOn the bottom row, however, we see very different results. One image has been heavily pixelated, another blurred, and another corrupted with noise. When compared to the original, their SSIM values drop significantly — around 0.67, 0.69, and about 0.72. That means they retain only about two-thirds of the structural similarity.\\n\\nThe important thing here is that SSIM values line up with what we visually perceive. Images that look good to us score close to one. Images that look distorted or degraded score much lower. That is the power of SSIM — it bridges the gap between mathematical measurement and human vision.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"SSIM=1\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"the reference image\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"On the top left, we have the reference image — the original, undistorted version. If we compare this image with itself, the SSIM score is exactly 1.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"SSIM=0.949\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"the image next to the reference image\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"Now, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"SSIM=0.989\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"the\", \"parsed\": {\"s\": \"SSIM=0.949\", \"p\": \"measures\", \"o\": \"the image next to the reference image\", \"modalities\": [\"text\", \"image\"], \"confidence\": 0.95, \"evidence\": [\"Now, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989.\"]}}"
      }
    }
  }
}