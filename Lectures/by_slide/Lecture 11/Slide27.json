{
  "lecture": "Lecture 11",
  "slide_id": "Slide27",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide27.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide27.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide27.json",
        "parsed": {
          "term": "student performance",
          "category": "education"
        },
        "raw": "{\"slide_id\": \"Slide27\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nBecause SSIM works so well, researchers have developed many extensions of the method.\\n\\nThe version we’ve been discussing applies to grayscale images, where each pixel has only one intensity value. But in reality, most images are in color. So naturally, the first extension was to adapt SSIM for color image quality assessment. This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\\n\\nAnother extension is to video quality assessment. Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency. This has become very important in areas like video compression and streaming.\\nThere is also multi-scale SSIM, which evaluates images at different levels of resolution. This is especially useful because human vision itself operates on a multi-scale level — we notice both fine details and large structures, depending on how we view an image.\\nAnd finally, there is complex wavelet SSIM, which can handle images that have complex values, such as those produced in MRI. This is a powerful extension that broadens SSIM to applications in medical imaging and beyond.\\n\\nThe core idea of SSIM — comparing luminance, contrast, and structure — has proven to be so flexible that it has been adapted to various domains.\\n\\nNow, let me pause here and use an analogy that connects to your own experience. When we design an exam, we want the scores to spread out enough to show meaningful differences among students. If everyone scores 90 or higher, we can’t distinguish performance very well. Ideally, the mean should be somewhere in the middle, say around 50, with variation above and below. That way, the test reveals the true distribution of understanding.\\nIt’s the same idea with SSIM: we normalize by the mean and by the variation, so we can focus on the meaningful structural differences between signals.\\n\\nSo, whether we’re evaluating images or student performance, the principle is the same: measure differences relative to expectations and variability, not in absolute terms.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"SSIM\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"color image quality assessment\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"video quality assessment\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"multi-scale SSIM\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"complex wavelet SSIM\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"luminance, contrast, and structure\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"mean and variation\\\", \\\"category\\\": \\\"statistics\\\"},\\n    {\\\"term\\\": \\\"student performance\\\", \\\"category\\\": \\\"education\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"The version we’ve been discussing applies to grayscale images, where each pixel has only one intensity value. But in reality, most images are in color. So naturally, the first extension was to adapt SSIM for color image quality assessment. This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\\\",\\n    \\\"Another extension is to video quality assessment. Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency.\", \"parsed\": {\"term\": \"student performance\", \"category\": \"education\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide27.json",
        "parsed": {
          "s": "SSIM",
          "p": "uses|via|represents|depends_on|measures|produces|reconstructs_with",
          "o": "video quality assessment",
          "modalities": [
            "text",
            "image"
          ],
          "confidence": 0.0,
          "evidence": [
            "Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency."
          ]
        },
        "raw": "{\"slide_id\": \"Slide27\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nBecause SSIM works so well, researchers have developed many extensions of the method.\\n\\nThe version we’ve been discussing applies to grayscale images, where each pixel has only one intensity value. But in reality, most images are in color. So naturally, the first extension was to adapt SSIM for color image quality assessment. This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\\n\\nAnother extension is to video quality assessment. Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency. This has become very important in areas like video compression and streaming.\\nThere is also multi-scale SSIM, which evaluates images at different levels of resolution. This is especially useful because human vision itself operates on a multi-scale level — we notice both fine details and large structures, depending on how we view an image.\\nAnd finally, there is complex wavelet SSIM, which can handle images that have complex values, such as those produced in MRI. This is a powerful extension that broadens SSIM to applications in medical imaging and beyond.\\n\\nThe core idea of SSIM — comparing luminance, contrast, and structure — has proven to be so flexible that it has been adapted to various domains.\\n\\nNow, let me pause here and use an analogy that connects to your own experience. When we design an exam, we want the scores to spread out enough to show meaningful differences among students. If everyone scores 90 or higher, we can’t distinguish performance very well. Ideally, the mean should be somewhere in the middle, say around 50, with variation above and below. That way, the test reveals the true distribution of understanding.\\nIt’s the same idea with SSIM: we normalize by the mean and by the variation, so we can focus on the meaningful structural differences between signals.\\n\\nSo, whether we’re evaluating images or student performance, the principle is the same: measure differences relative to expectations and variability, not in absolute terms.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n```json\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"SSIM\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"color image quality assessment\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\\\"},\\n    {\\\"s\\\":\\\"SSIM\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"video quality assessment\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency.\\\"},\\n    {\\\"s\\\":\\\"SSIM\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"multi-scale SSIM\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"This is especially useful because human vision itself operates on a multi-scale level — we notice both fine details and large structures\", \"parsed\": {\"s\": \"SSIM\", \"p\": \"uses|via|represents|depends_on|measures|produces|reconstructs_with\", \"o\": \"video quality assessment\", \"modalities\": [\"text\", \"image\"], \"confidence\": 0.0, \"evidence\": [\"Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency.\"]}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide27.json",
        "parsed": {
          "concepts": [
            {
              "term": "color image quality assessment",
              "category": "quality_metric"
            },
            {
              "term": "video quality assessment",
              "category": "quality_metric"
            },
            {
              "term": "multi-scale SSIM",
              "category": "signal_processing"
            },
            {
              "term": "complex wavelet SSIM",
              "category": "signal_processing"
            },
            {
              "term": "luminance",
              "category": "signal_processing"
            },
            {
              "term": "contrast",
              "category": "signal_processing"
            },
            {
              "term": "structure",
              "category": "signal_processing"
            },
            {
              "term": "mean",
              "category": "mathematics"
            },
            {
              "term": "variation",
              "category": "mathematics"
            }
          ],
          "evidence": [
            "adapt SSIM for color image quality assessment",
            "SSIM is applied not just frame by frame, but also across time",
            "evaluates images at different levels of resolution",
            "can handle images that have complex values",
            "comparing luminance, contrast, and structure",
            "normalize by the mean and by the variation"
          ]
        },
        "raw": "{\"slide_id\": \"Slide27\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"color image quality assessment\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"video quality assessment\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"multi-scale SSIM\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"complex wavelet SSIM\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"luminance\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"contrast\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"structure\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"mean\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"variation\\\", \\\"category\\\": \\\"mathematics\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"adapt SSIM for color image quality assessment\\\",\\n    \\\"SSIM is applied not just frame by frame, but also across time\\\",\\n    \\\"evaluates images at different levels of resolution\\\",\\n    \\\"can handle images that have complex values\\\",\\n    \\\"comparing luminance, contrast, and structure\\\",\\n    \\\"normalize by the mean and by the variation\\\"\\n  ]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"color image quality assessment\", \"category\": \"quality_metric\"}, {\"term\": \"video quality assessment\", \"category\": \"quality_metric\"}, {\"term\": \"multi-scale SSIM\", \"category\": \"signal_processing\"}, {\"term\": \"complex wavelet SSIM\", \"category\": \"signal_processing\"}, {\"term\": \"luminance\", \"category\": \"signal_processing\"}, {\"term\": \"contrast\", \"category\": \"signal_processing\"}, {\"term\": \"structure\", \"category\": \"signal_processing\"}, {\"term\": \"mean\", \"category\": \"mathematics\"}, {\"term\": \"variation\", \"category\": \"mathematics\"}], \"evidence\": [\"adapt SSIM for color image quality assessment\", \"SSIM is applied not just frame by frame, but also across time\", \"evaluates images at different levels of resolution\", \"can handle images that have complex values\", \"comparing luminance, contrast, and structure\", \"normalize by the mean and by the variation\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide27.json",
        "parsed": {
          "triples": [
            {
              "s": "Color Image Quality Assessment",
              "p": "uses",
              "o": "SSIM",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "Color Image Quality Assessment Toet & Lucassen, Displays, '03"
            },
            {
              "s": "Video Quality Assessment",
              "p": "uses",
              "o": "SSIM",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "Video Quality Assessment Wang, et al., Signal Processing: Image Communication, '04"
            },
            {
              "s": "Multi-scale SSIM",
              "p": "uses",
              "o": "SSIM",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "Multi-scale SSIM Wang, et al., Invited Paper, IEEE Asilomar Conf. '03"
            },
            {
              "s": "Complex Wavelet SSIM",
              "p": "uses",
              "o": "SSIM",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "Complex Wavelet SSIM Wang & Simoncelli, ICASSP '05"
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide27\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Color Image Quality Assessment\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"SSIM\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Color Image Quality Assessment Toet & Lucassen, Displays, '03\\\"},\\n    {\\\"s\\\":\\\"Video Quality Assessment\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"SSIM\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Video Quality Assessment Wang, et al., Signal Processing: Image Communication, '04\\\"},\\n    {\\\"s\\\":\\\"Multi-scale SSIM\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"SSIM\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Multi-scale SSIM Wang, et al., Invited Paper, IEEE Asilomar Conf. '03\\\"},\\n    {\\\"s\\\":\\\"Complex Wavelet SSIM\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"SSIM\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Complex Wavelet SSIM Wang & Simoncelli, ICASSP '05\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"Color Image Quality Assessment\", \"p\": \"uses\", \"o\": \"SSIM\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"Color Image Quality Assessment Toet & Lucassen, Displays, '03\"}, {\"s\": \"Video Quality Assessment\", \"p\": \"uses\", \"o\": \"SSIM\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"Video Quality Assessment Wang, et al., Signal Processing: Image Communication, '04\"}, {\"s\": \"Multi-scale SSIM\", \"p\": \"uses\", \"o\": \"SSIM\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"Multi-scale SSIM Wang, et al., Invited Paper, IEEE Asilomar Conf. '03\"}, {\"s\": \"Complex Wavelet SSIM\", \"p\": \"uses\", \"o\": \"SSIM\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"Complex Wavelet SSIM Wang & Simoncelli, ICASSP '05\"}]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide27.json",
        "parsed": {
          "concepts": [
            {
              "term": "Color Image Quality Assessment",
              "category": "quality_metric"
            },
            {
              "term": "Video Quality Assessment",
              "category": "quality_metric"
            },
            {
              "term": "Multi-scale SSIM",
              "category": "signal_processing"
            },
            {
              "term": "Complex Wavelet SSIM",
              "category": "signal_processing"
            }
          ],
          "evidence": [
            "This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.",
            "Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency.",
            "This is especially useful because human vision itself operates on a multi-scale level - we notice both fine details and large structures, depending on how we view an image.",
            "This is a powerful extension that broadens SSIM to applications in medical imaging and beyond."
          ]
        },
        "raw": "{\"slide_id\": \"Slide27\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T04:05:48.553799+00:00\", \"text_length\": 2095, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nBecause SSIM works so well, researchers have developed many extensions of the method.\\n\\nThe version we’ve been discussing applies to grayscale images, where each pixel has only one intensity value. But in reality, most images are in color. So naturally, the first extension was to adapt SSIM for color image quality assessment. This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\\n\\nAnother extension is to video quality assessment. Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency. This has become very important in areas like video compression and streaming.\\nThere is also multi-scale SSIM, which evaluates images at different levels of resolution. This is especially useful because human vision itself operates on a multi-scale level — we notice both fine details and large structures, depending on how we view an image.\\nAnd finally, there is complex wavelet SSIM, which can handle images that have complex values, such as those produced in MRI. This is a powerful extension that broadens SSIM to applications in medical imaging and beyond.\\n\\nThe core idea of SSIM — comparing luminance, contrast, and structure — has proven to be so flexible that it has been adapted to various domains.\\n\\nNow, let me pause here and use an analogy that connects to your own experience. When we design an exam, we want the scores to spread out enough to show meaningful differences among students. If everyone scores 90 or higher, we can’t distinguish performance very well. Ideally, the mean should be somewhere in the middle, say around 50, with variation above and below. That way, the test reveals the true distribution of understanding.\\nIt’s the same idea with SSIM: we normalize by the mean and by the variation, so we can focus on the meaningful structural differences between signals.\\n\\nSo, whether we’re evaluating images or student performance, the principle is the same: measure differences relative to expectations and variability, not in absolute terms.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Color Image Quality Assessment\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"Video Quality Assessment\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"Multi-scale SSIM\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"Complex Wavelet SSIM\\\", \\\"category\\\": \\\"signal_processing\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\\\",\\n    \\\"Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency.\\\",\\n    \\\"This is especially useful because human vision itself operates on a multi-scale level — we notice both fine details and large structures, depending on how we view an image.\\\",\\n    \\\"This is a powerful extension that broadens SSIM to applications in medical imaging and beyond.\\\"\\n  ]\\n}\", \"parsed\": {\"concepts\": [{\"term\": \"Color Image Quality Assessment\", \"category\": \"quality_metric\"}, {\"term\": \"Video Quality Assessment\", \"category\": \"quality_metric\"}, {\"term\": \"Multi-scale SSIM\", \"category\": \"signal_processing\"}, {\"term\": \"Complex Wavelet SSIM\", \"category\": \"signal_processing\"}], \"evidence\": [\"This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\", \"Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency.\", \"This is especially useful because human vision itself operates on a multi-scale level - we notice both fine details and large structures, depending on how we view an image.\", \"This is a powerful extension that broadens SSIM to applications in medical imaging and beyond.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide27.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide27\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T05:36:07.382594+00:00\", \"text_length\": 2095, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nBecause SSIM works so well, researchers have developed many extensions of the method.\\n\\nThe version we’ve been discussing applies to grayscale images, where each pixel has only one intensity value. But in reality, most images are in color. So naturally, the first extension was to adapt SSIM for color image quality assessment. This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\\n\\nAnother extension is to video quality assessment. Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency. This has become very important in areas like video compression and streaming.\\nThere is also multi-scale SSIM, which evaluates images at different levels of resolution. This is especially useful because human vision itself operates on a multi-scale level — we notice both fine details and large structures, depending on how we view an image.\\nAnd finally, there is complex wavelet SSIM, which can handle images that have complex values, such as those produced in MRI. This is a powerful extension that broadens SSIM to applications in medical imaging and beyond.\\n\\nThe core idea of SSIM — comparing luminance, contrast, and structure — has proven to be so flexible that it has been adapted to various domains.\\n\\nNow, let me pause here and use an analogy that connects to your own experience. When we design an exam, we want the scores to spread out enough to show meaningful differences among students. If everyone scores 90 or higher, we can’t distinguish performance very well. Ideally, the mean should be somewhere in the middle, say around 50, with variation above and below. That way, the test reveals the true distribution of understanding.\\nIt’s the same idea with SSIM: we normalize by the mean and by the variation, so we can focus on the meaningful structural differences between signals.\\n\\nSo, whether we’re evaluating images or student performance, the principle is the same: measure differences relative to expectations and variability, not in absolute terms.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide27.json",
        "parsed": {
          "term": "measure differences relative to expectations and variability, not in absolute terms",
          "category": "quality_metric"
        },
        "raw": "{\"slide_id\": \"Slide27\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:04:19.160532+00:00\", \"text_length\": 2095, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nBecause SSIM works so well, researchers have developed many extensions of the method.\\n\\nThe version we’ve been discussing applies to grayscale images, where each pixel has only one intensity value. But in reality, most images are in color. So naturally, the first extension was to adapt SSIM for color image quality assessment. This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\\n\\nAnother extension is to video quality assessment. Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency. This has become very important in areas like video compression and streaming.\\nThere is also multi-scale SSIM, which evaluates images at different levels of resolution. This is especially useful because human vision itself operates on a multi-scale level — we notice both fine details and large structures, depending on how we view an image.\\nAnd finally, there is complex wavelet SSIM, which can handle images that have complex values, such as those produced in MRI. This is a powerful extension that broadens SSIM to applications in medical imaging and beyond.\\n\\nThe core idea of SSIM — comparing luminance, contrast, and structure — has proven to be so flexible that it has been adapted to various domains.\\n\\nNow, let me pause here and use an analogy that connects to your own experience. When we design an exam, we want the scores to spread out enough to show meaningful differences among students. If everyone scores 90 or higher, we can’t distinguish performance very well. Ideally, the mean should be somewhere in the middle, say around 50, with variation above and below. That way, the test reveals the true distribution of understanding.\\nIt’s the same idea with SSIM: we normalize by the mean and by the variation, so we can focus on the meaningful structural differences between signals.\\n\\nSo, whether we’re evaluating images or student performance, the principle is the same: measure differences relative to expectations and variability, not in absolute terms.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"Color Image Quality Assessment\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"Video Quality Assessment\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"Multi-scale SSIM\\\",\\n      \\\"category\\\": \\\"algorithm\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"Complex Wavelet SSIM\\\",\\n      \\\"category\\\": \\\"algorithm\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"luminance, contrast, and structure\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"measure differences relative to expectations and variability, not in absolute terms\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    }\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Because SSIM works so well, researchers have developed many extensions of the method.\\\",\\n    \\\"The version we’ve been discussing applies to grayscale images, where each pixel has only one intensity value. But in reality, most images are in color.\\\",\\n    \\\"Another extension is to video quality assessment. Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency.\\\",\\n    \\\"There is also multi-scale SSIM, which evaluates images\", \"parsed\": {\"term\": \"measure differences relative to expectations and variability, not in absolute terms\", \"category\": \"quality_metric\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide27.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide27\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:13:19.156211+00:00\", \"text_length\": 2095, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nBecause SSIM works so well, researchers have developed many extensions of the method.\\n\\nThe version we’ve been discussing applies to grayscale images, where each pixel has only one intensity value. But in reality, most images are in color. So naturally, the first extension was to adapt SSIM for color image quality assessment. This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\\n\\nAnother extension is to video quality assessment. Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency. This has become very important in areas like video compression and streaming.\\nThere is also multi-scale SSIM, which evaluates images at different levels of resolution. This is especially useful because human vision itself operates on a multi-scale level — we notice both fine details and large structures, depending on how we view an image.\\nAnd finally, there is complex wavelet SSIM, which can handle images that have complex values, such as those produced in MRI. This is a powerful extension that broadens SSIM to applications in medical imaging and beyond.\\n\\nThe core idea of SSIM — comparing luminance, contrast, and structure — has proven to be so flexible that it has been adapted to various domains.\\n\\nNow, let me pause here and use an analogy that connects to your own experience. When we design an exam, we want the scores to spread out enough to show meaningful differences among students. If everyone scores 90 or higher, we can’t distinguish performance very well. Ideally, the mean should be somewhere in the middle, say around 50, with variation above and below. That way, the test reveals the true distribution of understanding.\\nIt’s the same idea with SSIM: we normalize by the mean and by the variation, so we can focus on the meaningful structural differences between signals.\\n\\nSo, whether we’re evaluating images or student performance, the principle is the same: measure differences relative to expectations and variability, not in absolute terms.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": []\\n}\", \"parsed\": {\"triples\": []}}"
      }
    }
  }
}