{
  "lecture": "Lecture 11",
  "slide_id": "Slide5",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide5.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide5.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide5.json",
        "parsed": {
          "term": "algorithm",
          "category": "software"
        },
        "raw": "{\"slide_id\": \"Slide5\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nLet’s begin with one of the most basic and widely used quality measures: the Mean Squared Error, or MSE.\\nSuppose we have two images: the true image, which we’ll call y, and the reconstructed image, which we’ll call y-hat. Each image is made up of many pixels, indexed by i. If the image is 512 by 512, then the total number of pixels, n, is over 260,000.\\n\\nThe formula for MSE is simple:\\u000bMSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\nIn words, this means we compare the two images pixel by pixel. At each pixel, we take the difference, square it so that positive and negative errors don’t cancel out, and then average over all pixels. That gives us the mean squared error.\\nNow, let’s go a bit deeper. When we estimate a parameter — say theta — we often write the estimate as theta-hat. The error between theta-hat and the true value theta can be analyzed in expectation, meaning averaged over many trials. When you expand the algebra, you find that the mean squared error naturally splits into two parts.\\n\\nThe first part is the variance. This tells us how much our estimates fluctuate around their average value. You can think of variance as a measure of random scatter.\\n\\nThe second part is the bias squared. This measures the difference between the average of our estimates and the true value. If our method consistently overshoots or undershoots, that’s bias.\\n\\nSo, in summary: MSE equals variance plus bias squared. Variance captures random error, bias captures systematic error, and together they define the total error.\\n\\nThis decomposition is very useful. It reminds us that an algorithm might have low variance but high bias, or vice versa. Understanding both helps us judge the quality of an estimator or an image reconstruction method.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"MSE\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"mean squared error\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"pixel\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"image\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"reconstructed image\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"y-hat\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"y\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"difference\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"squared\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"average\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"variance\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"bias\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"mean squared error\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"random scatter\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"systematic error\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"algorithm\\\", \\\"category\\\": \\\"software\\\"},\\n    {\\\"term\\\": \\\"estimator\\\", \\\"\", \"parsed\": {\"term\": \"algorithm\", \"category\": \"software\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide5.json",
        "parsed": {
          "triples": [
            {
              "s": "MSE",
              "p": "measures",
              "o": "y-hat",
              "modalities": [
                "text"
              ],
              "confidence": 0.0,
              "evidence": "MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide5\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nLet’s begin with one of the most basic and widely used quality measures: the Mean Squared Error, or MSE.\\nSuppose we have two images: the true image, which we’ll call y, and the reconstructed image, which we’ll call y-hat. Each image is made up of many pixels, indexed by i. If the image is 512 by 512, then the total number of pixels, n, is over 260,000.\\n\\nThe formula for MSE is simple:\\u000bMSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\nIn words, this means we compare the two images pixel by pixel. At each pixel, we take the difference, square it so that positive and negative errors don’t cancel out, and then average over all pixels. That gives us the mean squared error.\\nNow, let’s go a bit deeper. When we estimate a parameter — say theta — we often write the estimate as theta-hat. The error between theta-hat and the true value theta can be analyzed in expectation, meaning averaged over many trials. When you expand the algebra, you find that the mean squared error naturally splits into two parts.\\n\\nThe first part is the variance. This tells us how much our estimates fluctuate around their average value. You can think of variance as a measure of random scatter.\\n\\nThe second part is the bias squared. This measures the difference between the average of our estimates and the true value. If our method consistently overshoots or undershoots, that’s bias.\\n\\nSo, in summary: MSE equals variance plus bias squared. Variance captures random error, bias captures systematic error, and together they define the total error.\\n\\nThis decomposition is very useful. It reminds us that an algorithm might have low variance but high bias, or vice versa. Understanding both helps us judge the quality of an estimator or an image reconstruction method.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"MSE\\\", \\\"p\\\":\\\"measures\\\",\\n     \\\"o\\\":\\\"y-hat\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"MSE\", \"p\": \"measures\", \"o\": \"y-hat\", \"modalities\": [\"text\"], \"confidence\": 0.0, \"evidence\": \"MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\"}]}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide5.json",
        "parsed": {
          "term": "decomposition",
          "category": "mathematics"
        },
        "raw": "{\"slide_id\": \"Slide5\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Mean Squared Error\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"variance\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"bias\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"parameter estimate\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"image reconstruction\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"pixel\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"difference\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"average\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"expectation\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"decomposition\\\", \\\"category\\\": \\\"mathematics\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"one of the most basic and widely used quality measures\\\",\\n    \\\"how much our estimates fluctuate around their average value\\\",\\n    \\\"difference between the average of our estimates and the true value\\\",\\n    \\\"an algorithm might have low variance but high bias\\\",\\n    \\\"compare the two images pixel by pixel\\\",\\n    \\\"difference between y-sub-i and y-hat-sub-i\\\",\\n    \\\"averaged over many trials\\\",\\n    \\\"the mean squared error naturally\", \"parsed\": {\"term\": \"decomposition\", \"category\": \"mathematics\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide5.json",
        "parsed": {
          "triples": [
            {
              "s": "Mean Squared Error",
              "p": "measures",
              "o": "difference between y-sub-i and y-hat-sub-i",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared."
            },
            {
              "s": "MSE",
              "p": "represents",
              "o": "variance plus bias squared",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "So, in summary: MSE equals variance plus bias squared."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide5\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Mean Squared Error\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"difference between y-sub-i and y-hat-sub-i\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\\"},\\n    {\\\"s\\\":\\\"MSE\\\", \\\"p\\\":\\\"represents\\\", \\\"o\\\":\\\"variance plus bias squared\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"So, in summary: MSE equals variance plus bias squared.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"Mean Squared Error\", \"p\": \"measures\", \"o\": \"difference between y-sub-i and y-hat-sub-i\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\"}, {\"s\": \"MSE\", \"p\": \"represents\", \"o\": \"variance plus bias squared\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"So, in summary: MSE equals variance plus bias squared.\"}]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide5.json",
        "parsed": {
          "term": "MSE(theta) = E[(theta - theta)^2]",
          "category": "mathematics"
        },
        "raw": "{\"slide_id\": \"Slide5\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:29:22.096976+00:00\", \"text_length\": 1816, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nLet’s begin with one of the most basic and widely used quality measures: the Mean Squared Error, or MSE.\\nSuppose we have two images: the true image, which we’ll call y, and the reconstructed image, which we’ll call y-hat. Each image is made up of many pixels, indexed by i. If the image is 512 by 512, then the total number of pixels, n, is over 260,000.\\n\\nThe formula for MSE is simple:\\u000bMSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\nIn words, this means we compare the two images pixel by pixel. At each pixel, we take the difference, square it so that positive and negative errors don’t cancel out, and then average over all pixels. That gives us the mean squared error.\\nNow, let’s go a bit deeper. When we estimate a parameter — say theta — we often write the estimate as theta-hat. The error between theta-hat and the true value theta can be analyzed in expectation, meaning averaged over many trials. When you expand the algebra, you find that the mean squared error naturally splits into two parts.\\n\\nThe first part is the variance. This tells us how much our estimates fluctuate around their average value. You can think of variance as a measure of random scatter.\\n\\nThe second part is the bias squared. This measures the difference between the average of our estimates and the true value. If our method consistently overshoots or undershoots, that’s bias.\\n\\nSo, in summary: MSE equals variance plus bias squared. Variance captures random error, bias captures systematic error, and together they define the total error.\\n\\nThis decomposition is very useful. It reminds us that an algorithm might have low variance but high bias, or vice versa. Understanding both helps us judge the quality of an estimator or an image reconstruction method.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Mean Squared Error\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"MSE\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"Mean Squared Error formula\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"theta\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"theta-hat\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"theta-hat - theta\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"E[theta] - theta\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"E[theta] - theta = const.\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"E[theta] = const.\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"MSE(theta) = E[(theta - theta)^2]\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"MSE(theta) = E[(theta - E[theta] + E[theta] - theta)^2]\\\",\", \"parsed\": {\"term\": \"MSE(theta) = E[(theta - theta)^2]\", \"category\": \"mathematics\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide5.json",
        "parsed": {
          "triples": [
            {
              "s": "Mean Squared Error",
              "p": "measures",
              "o": "MSE",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "The formula for MSE is simple: MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide5\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T05:19:53.820823+00:00\", \"text_length\": 1816, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nLet’s begin with one of the most basic and widely used quality measures: the Mean Squared Error, or MSE.\\nSuppose we have two images: the true image, which we’ll call y, and the reconstructed image, which we’ll call y-hat. Each image is made up of many pixels, indexed by i. If the image is 512 by 512, then the total number of pixels, n, is over 260,000.\\n\\nThe formula for MSE is simple:\\u000bMSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\nIn words, this means we compare the two images pixel by pixel. At each pixel, we take the difference, square it so that positive and negative errors don’t cancel out, and then average over all pixels. That gives us the mean squared error.\\nNow, let’s go a bit deeper. When we estimate a parameter — say theta — we often write the estimate as theta-hat. The error between theta-hat and the true value theta can be analyzed in expectation, meaning averaged over many trials. When you expand the algebra, you find that the mean squared error naturally splits into two parts.\\n\\nThe first part is the variance. This tells us how much our estimates fluctuate around their average value. You can think of variance as a measure of random scatter.\\n\\nThe second part is the bias squared. This measures the difference between the average of our estimates and the true value. If our method consistently overshoots or undershoots, that’s bias.\\n\\nSo, in summary: MSE equals variance plus bias squared. Variance captures random error, bias captures systematic error, and together they define the total error.\\n\\nThis decomposition is very useful. It reminds us that an algorithm might have low variance but high bias, or vice versa. Understanding both helps us judge the quality of an estimator or an image reconstruction method.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Mean Squared Error\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"MSE\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"The formula for MSE is simple: MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"Mean Squared Error\", \"p\": \"measures\", \"o\": \"MSE\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"The formula for MSE is simple: MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\"}]}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide5.json",
        "parsed": {
          "term": "theta-hat",
          "category": "algorithm"
        },
        "raw": "{\"slide_id\": \"Slide5\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:00:42.114495+00:00\", \"text_length\": 1816, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nLet’s begin with one of the most basic and widely used quality measures: the Mean Squared Error, or MSE.\\nSuppose we have two images: the true image, which we’ll call y, and the reconstructed image, which we’ll call y-hat. Each image is made up of many pixels, indexed by i. If the image is 512 by 512, then the total number of pixels, n, is over 260,000.\\n\\nThe formula for MSE is simple:\\u000bMSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\nIn words, this means we compare the two images pixel by pixel. At each pixel, we take the difference, square it so that positive and negative errors don’t cancel out, and then average over all pixels. That gives us the mean squared error.\\nNow, let’s go a bit deeper. When we estimate a parameter — say theta — we often write the estimate as theta-hat. The error between theta-hat and the true value theta can be analyzed in expectation, meaning averaged over many trials. When you expand the algebra, you find that the mean squared error naturally splits into two parts.\\n\\nThe first part is the variance. This tells us how much our estimates fluctuate around their average value. You can think of variance as a measure of random scatter.\\n\\nThe second part is the bias squared. This measures the difference between the average of our estimates and the true value. If our method consistently overshoots or undershoots, that’s bias.\\n\\nSo, in summary: MSE equals variance plus bias squared. Variance captures random error, bias captures systematic error, and together they define the total error.\\n\\nThis decomposition is very useful. It reminds us that an algorithm might have low variance but high bias, or vice versa. Understanding both helps us judge the quality of an estimator or an image reconstruction method.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"Mean Squared Error\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"MSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"MSE equals variance plus bias squared.\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"variance\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"bias squared\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"reconstructed image\\\",\\n      \\\"category\\\": \\\"reconstruction\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"true image\\\",\\n      \\\"category\\\": \\\"reconstruction\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"estimate\\\",\\n      \\\"category\\\": \\\"algorithm\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"parameter\\\",\\n      \\\"category\\\": \\\"algorithm\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"theta-hat\\\",\\n      \\\"category\\\": \\\"algorithm\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"theta\\\",\\n      \\\"category\\\": \\\"algorithm\", \"parsed\": {\"term\": \"theta-hat\", \"category\": \"algorithm\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide5.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide5\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:10:58.083361+00:00\", \"text_length\": 1816, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nLet’s begin with one of the most basic and widely used quality measures: the Mean Squared Error, or MSE.\\nSuppose we have two images: the true image, which we’ll call y, and the reconstructed image, which we’ll call y-hat. Each image is made up of many pixels, indexed by i. If the image is 512 by 512, then the total number of pixels, n, is over 260,000.\\n\\nThe formula for MSE is simple:\\u000bMSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\\nIn words, this means we compare the two images pixel by pixel. At each pixel, we take the difference, square it so that positive and negative errors don’t cancel out, and then average over all pixels. That gives us the mean squared error.\\nNow, let’s go a bit deeper. When we estimate a parameter — say theta — we often write the estimate as theta-hat. The error between theta-hat and the true value theta can be analyzed in expectation, meaning averaged over many trials. When you expand the algebra, you find that the mean squared error naturally splits into two parts.\\n\\nThe first part is the variance. This tells us how much our estimates fluctuate around their average value. You can think of variance as a measure of random scatter.\\n\\nThe second part is the bias squared. This measures the difference between the average of our estimates and the true value. If our method consistently overshoots or undershoots, that’s bias.\\n\\nSo, in summary: MSE equals variance plus bias squared. Variance captures random error, bias captures systematic error, and together they define the total error.\\n\\nThis decomposition is very useful. It reminds us that an algorithm might have low variance but high bias, or vice versa. Understanding both helps us judge the quality of an estimator or an image reconstruction method.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": []\\n}\", \"parsed\": {\"triples\": []}}"
      }
    }
  }
}