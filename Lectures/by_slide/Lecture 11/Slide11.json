{
  "lecture": "Lecture 11",
  "slide_id": "Slide11",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide11.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide11.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide11.json",
        "parsed": {
          "term": "image structure preservation",
          "category": "signal_processing"
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s see why mean squared error, or MSE, is not always good enough.\\nAt the top left, we have the original image — the best version, taken under ideal conditions. Below it, you see five different degraded versions of the same image. Some look noisy, some are blurry, and some have other distortions. Clearly, to the human eye, these images do not look equally good.\\n\\nBut here’s the problem: when we compute the mean squared error between the original image and each of these five degraded ones, the result is the same — two hundred and twenty-five. Mathematically, MSE tells us they are equally different from the original.\\nVisually, though, that’s obviously not true. Some versions look much closer to the original, while others look far worse. Our eyes immediately pick up those differences, but MSE cannot.\\n\\nAnd this is the key point: MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.\\n\\nThis leads us to the next important idea — we need a measure that better matches what humans actually see. That’s where structural similarity, or SSIM, comes in.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"mean squared error\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"structural similarity\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"image degradation\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"human perception\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"pixel-wise differences\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"image structure preservation\\\", \\\"category\\\": \\\"signal_processing\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"At the top left, we have the original image — the best version, taken under ideal conditions. Below it, you see five different degraded versions of the same image. Some look noisy, some are blurry, and some have other distortions. Clearly, to the human eye, these images do not look equally good.\\\", \\\"Mathematically, MSE tells us they are equally different from the original. Visually, though, that’s obviously not true. Some versions look much closer to the original, while others look far worse. Our eyes immediately pick up those differences, but MSE cannot.\\\", \\\"This leads us to the next important idea — we need a measure that better matches what humans actually see. That’s where structural similarity\", \"parsed\": {\"term\": \"image structure preservation\", \"category\": \"signal_processing\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide11.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s see why mean squared error, or MSE, is not always good enough.\\nAt the top left, we have the original image — the best version, taken under ideal conditions. Below it, you see five different degraded versions of the same image. Some look noisy, some are blurry, and some have other distortions. Clearly, to the human eye, these images do not look equally good.\\n\\nBut here’s the problem: when we compute the mean squared error between the original image and each of these five degraded ones, the result is the same — two hundred and twenty-five. Mathematically, MSE tells us they are equally different from the original.\\nVisually, though, that’s obviously not true. Some versions look much closer to the original, while others look far worse. Our eyes immediately pick up those differences, but MSE cannot.\\n\\nAnd this is the key point: MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.\\n\\nThis leads us to the next important idea — we need a measure that better matches what humans actually see. That’s where structural similarity, or SSIM, comes in.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"MSE\\\", \\\"p\\\":\\\"measures\\\",\\n     \\\"o\\\":\\\"mean squared error\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically, MSE tells us they are equally different from the original. Mathematically,\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide11.json",
        "parsed": {
          "concepts": [
            {
              "term": "mean squared error",
              "category": "quality_metric"
            },
            {
              "term": "structural similarity",
              "category": "quality_metric"
            },
            {
              "term": "SSIM",
              "category": "quality_metric"
            }
          ],
          "evidence": [
            "mean squared error between the original image and each of these five degraded ones, the result is the same - two hundred and twenty-five.",
            "we need a measure that better matches what humans actually see. That’s where structural similarity, or SSIM, comes in."
          ]
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"mean squared error\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"structural similarity\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"MSE\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"SSIM\\\", \\\"category\\\": \\\"quality_metric\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"mean squared error between the original image and each of these five degraded ones, the result is the same — two hundred and twenty-five.\\\",\\n    \\\"we need a measure that better matches what humans actually see. That’s where structural similarity, or SSIM, comes in.\\\"\\n  ]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"mean squared error\", \"category\": \"quality_metric\"}, {\"term\": \"structural similarity\", \"category\": \"quality_metric\"}, {\"term\": \"SSIM\", \"category\": \"quality_metric\"}], \"evidence\": [\"mean squared error between the original image and each of these five degraded ones, the result is the same - two hundred and twenty-five.\", \"we need a measure that better matches what humans actually see. That’s where structural similarity, or SSIM, comes in.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide11.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"mean squared error (MSE)\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"pixel-wise differences\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"MSE tells us they are equally different from the original.\\\"},\\n    {\\\"s\\\":\\\"structural similarity (SSIM)\\\", \\\"p\\\":\\\"produces\\\", \\\"o\\\":\\\"a measure that better matches what humans actually see\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"That’s where structural similarity, or SSIM, comes in.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide11.json",
        "parsed": {
          "concepts": [
            {
              "term": "human perception",
              "category": "quality_metric"
            }
          ],
          "evidence": [
            "MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.",
            "This leads us to the next important idea - we need a measure that better matches what humans actually see. That's where structural similarity, or SSIM, comes in."
          ]
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:38:42.383284+00:00\", \"text_length\": 1168, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s see why mean squared error, or MSE, is not always good enough.\\nAt the top left, we have the original image — the best version, taken under ideal conditions. Below it, you see five different degraded versions of the same image. Some look noisy, some are blurry, and some have other distortions. Clearly, to the human eye, these images do not look equally good.\\n\\nBut here’s the problem: when we compute the mean squared error between the original image and each of these five degraded ones, the result is the same — two hundred and twenty-five. Mathematically, MSE tells us they are equally different from the original.\\nVisually, though, that’s obviously not true. Some versions look much closer to the original, while others look far worse. Our eyes immediately pick up those differences, but MSE cannot.\\n\\nAnd this is the key point: MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.\\n\\nThis leads us to the next important idea — we need a measure that better matches what humans actually see. That’s where structural similarity, or SSIM, comes in.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"mean squared error (MSE)\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"human perception\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"structural similarity (SSIM)\\\", \\\"category\\\": \\\"quality_metric\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.\\\",\\n    \\\"This leads us to the next important idea — we need a measure that better matches what humans actually see. That’s where structural similarity, or SSIM, comes in.\\\"\\n  ]\\n}\", \"parsed\": {\"concepts\": [{\"term\": \"human perception\", \"category\": \"quality_metric\"}], \"evidence\": [\"MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.\", \"This leads us to the next important idea - we need a measure that better matches what humans actually see. That's where structural similarity, or SSIM, comes in.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide11.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T05:24:32.435175+00:00\", \"text_length\": 1168, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s see why mean squared error, or MSE, is not always good enough.\\nAt the top left, we have the original image — the best version, taken under ideal conditions. Below it, you see five different degraded versions of the same image. Some look noisy, some are blurry, and some have other distortions. Clearly, to the human eye, these images do not look equally good.\\n\\nBut here’s the problem: when we compute the mean squared error between the original image and each of these five degraded ones, the result is the same — two hundred and twenty-five. Mathematically, MSE tells us they are equally different from the original.\\nVisually, though, that’s obviously not true. Some versions look much closer to the original, while others look far worse. Our eyes immediately pick up those differences, but MSE cannot.\\n\\nAnd this is the key point: MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.\\n\\nThis leads us to the next important idea — we need a measure that better matches what humans actually see. That’s where structural similarity, or SSIM, comes in.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"mean squared error (MSE)\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"human perception\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide11.json",
        "parsed": {
          "concepts": [
            {
              "term": "mean squared error, or mse",
              "category": "quality_metric"
            },
            {
              "term": "mse does not reflect human perception very well",
              "category": "quality_metric"
            },
            {
              "term": "we need a measure that better matches what humans actually see",
              "category": "quality_metric"
            },
            {
              "term": "structural similarity, or ssim",
              "category": "quality_metric"
            }
          ],
          "evidence": [
            "when we compute the mean squared error between the original image and each of these five degraded ones, the result is the same - two hundred and twenty-five.",
            "mse does not reflect human perception very well.",
            "we need a measure that better matches what humans actually see.",
            "that's where structural similarity, or ssim, comes in."
          ]
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:01:41.426761+00:00\", \"text_length\": 1168, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s see why mean squared error, or MSE, is not always good enough.\\nAt the top left, we have the original image — the best version, taken under ideal conditions. Below it, you see five different degraded versions of the same image. Some look noisy, some are blurry, and some have other distortions. Clearly, to the human eye, these images do not look equally good.\\n\\nBut here’s the problem: when we compute the mean squared error between the original image and each of these five degraded ones, the result is the same — two hundred and twenty-five. Mathematically, MSE tells us they are equally different from the original.\\nVisually, though, that’s obviously not true. Some versions look much closer to the original, while others look far worse. Our eyes immediately pick up those differences, but MSE cannot.\\n\\nAnd this is the key point: MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.\\n\\nThis leads us to the next important idea — we need a measure that better matches what humans actually see. That’s where structural similarity, or SSIM, comes in.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"mean squared error, or mse\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"mse does not reflect human perception very well\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"we need a measure that better matches what humans actually see\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"structural similarity, or ssim\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    }\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"when we compute the mean squared error between the original image and each of these five degraded ones, the result is the same — two hundred and twenty-five.\\\",\\n    \\\"mse does not reflect human perception very well.\\\",\\n    \\\"we need a measure that better matches what humans actually see.\\\",\\n    \\\"that’s where structural similarity, or ssim, comes in.\\\"\\n  ]\\n}\", \"parsed\": {\"concepts\": [{\"term\": \"mean squared error, or mse\", \"category\": \"quality_metric\"}, {\"term\": \"mse does not reflect human perception very well\", \"category\": \"quality_metric\"}, {\"term\": \"we need a measure that better matches what humans actually see\", \"category\": \"quality_metric\"}, {\"term\": \"structural similarity, or ssim\", \"category\": \"quality_metric\"}], \"evidence\": [\"when we compute the mean squared error between the original image and each of these five degraded ones, the result is the same - two hundred and twenty-five.\", \"mse does not reflect human perception very well.\", \"we need a measure that better matches what humans actually see.\", \"that's where structural similarity, or ssim, comes in.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide11.json",
        "parsed": {
          "triples": [
            {
              "s": "MSE",
              "p": "measures",
              "o": "pixel-wise differences",
              "modalities": [
                "text",
                "image"
              ],
              "confidence": 0.95,
              "evidence": "MSE tells us they are equally different from the original."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:11:46.506219+00:00\", \"text_length\": 1168, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s see why mean squared error, or MSE, is not always good enough.\\nAt the top left, we have the original image — the best version, taken under ideal conditions. Below it, you see five different degraded versions of the same image. Some look noisy, some are blurry, and some have other distortions. Clearly, to the human eye, these images do not look equally good.\\n\\nBut here’s the problem: when we compute the mean squared error between the original image and each of these five degraded ones, the result is the same — two hundred and twenty-five. Mathematically, MSE tells us they are equally different from the original.\\nVisually, though, that’s obviously not true. Some versions look much closer to the original, while others look far worse. Our eyes immediately pick up those differences, but MSE cannot.\\n\\nAnd this is the key point: MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.\\n\\nThis leads us to the next important idea — we need a measure that better matches what humans actually see. That’s where structural similarity, or SSIM, comes in.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"MSE\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"pixel-wise differences\\\",\\n      \\\"modalities\\\": [\\\"text\\\", \\\"image\\\"],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"MSE tells us they are equally different from the original.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"MSE\\\",\\n      \\\"p\\\": \\\"does not reflect\\\",\\n      \\\"o\\\": \\\"human perception\\\",\\n      \\\"modalities\\\": [\\\"text\\\", \\\"image\\\"],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"MSE does not reflect human perception very well.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"MSE\\\",\\n      \\\"p\\\": \\\"cannot capture\\\",\\n      \\\"o\\\": \\\"whether the overall structure of the image is preserved\\\",\\n      \\\"modalities\\\": [\\\"text\\\", \\\"image\\\"],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"MSE cannot capture whether the overall structure of the image is preserved.\\\"\\n    }\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"MSE\", \"p\": \"measures\", \"o\": \"pixel-wise differences\", \"modalities\": [\"text\", \"image\"], \"confidence\": 0.95, \"evidence\": \"MSE tells us they are equally different from the original.\"}]}}"
      }
    }
  }
}