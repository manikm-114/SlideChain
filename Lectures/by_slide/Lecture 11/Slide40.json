{
  "lecture": "Lecture 11",
  "slide_id": "Slide40",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide40.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide40.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide40.json",
        "parsed": {
          "term": "medical imaging research",
          "category": "modality"
        },
        "raw": "{\"slide_id\": \"Slide40\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.\\n\\nHere’s the key idea: sensitivity and specificity are not fixed properties of a test. They depend on the threshold we use to decide between “disease” and “no disease.”\\n\\nFor example, if I set the threshold very low, I’ll call almost everything abnormal. That means I’ll catch nearly every true case, so sensitivity will be very high. But I’ll also generate many false alarms, so specificity will drop.\\nOn the other hand, if I set the threshold very high, I’ll call almost everything normal. That means I’ll have very few false alarms — so specificity will be excellent. But I’ll also miss many real cases, so sensitivity will be low.\\n\\nThe ROC curve captures this trade-off. On the x-axis, we plot 1 – specificity, which is the false positive rate. On the y-axis, we plot sensitivity, the true positive rate. By sweeping through all possible thresholds, we trace out the curve.\\nThe ROC curve shows the overall performance of the test across all possible decision boundaries. And importantly, the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy. An AUC of 1.0 means perfect classification. An AUC of 0.5 — a diagonal line — means the test is no better than random guessing.\\n\\nSo the ROC curve is widely used in medical imaging research, because it provides a clear, quantitative way to compare diagnostic systems and observer performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"receiver operating characteristic curve\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"sensitivity\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"specificity\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"false positive rate\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"true positive rate\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"ROC curve\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"area under the ROC curve\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"AUC\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"diagnostic accuracy\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"medical imaging research\\\", \\\"category\\\": \\\"modality\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"Now let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.\\\", \\\"Here’s the key idea: sensitivity and specificity are not fixed properties of a test. They depend on the threshold we use to decide between “disease” and “no disease.”\\\", \\\"For example, if I set the threshold very low, I’ll call almost\", \"parsed\": {\"term\": \"medical imaging research\", \"category\": \"modality\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide40.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide40\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.\\n\\nHere’s the key idea: sensitivity and specificity are not fixed properties of a test. They depend on the threshold we use to decide between “disease” and “no disease.”\\n\\nFor example, if I set the threshold very low, I’ll call almost everything abnormal. That means I’ll catch nearly every true case, so sensitivity will be very high. But I’ll also generate many false alarms, so specificity will drop.\\nOn the other hand, if I set the threshold very high, I’ll call almost everything normal. That means I’ll have very few false alarms — so specificity will be excellent. But I’ll also miss many real cases, so sensitivity will be low.\\n\\nThe ROC curve captures this trade-off. On the x-axis, we plot 1 – specificity, which is the false positive rate. On the y-axis, we plot sensitivity, the true positive rate. By sweeping through all possible thresholds, we trace out the curve.\\nThe ROC curve shows the overall performance of the test across all possible decision boundaries. And importantly, the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy. An AUC of 1.0 means perfect classification. An AUC of 0.5 — a diagonal line — means the test is no better than random guessing.\\n\\nSo the ROC curve is widely used in medical imaging research, because it provides a clear, quantitative way to compare diagnostic systems and observer performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"receiver operating characteristic curve\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"diagnostic performance evaluation\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"Now let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide40.json",
        "parsed": {
          "concepts": [
            {
              "term": "sensitivity",
              "category": "quality_metric"
            },
            {
              "term": "specificity",
              "category": "quality_metric"
            },
            {
              "term": "ROC curve",
              "category": "algorithm"
            },
            {
              "term": "threshold",
              "category": "algorithm"
            },
            {
              "term": "false positive rate",
              "category": "quality_metric"
            },
            {
              "term": "true positive rate",
              "category": "quality_metric"
            },
            {
              "term": "diagnostic accuracy",
              "category": "quality_metric"
            }
          ],
          "evidence": [
            "sensitivity and specificity are not fixed properties of a test",
            "sweeping through all possible thresholds",
            "On the x-axis, we plot 1 - specificity, which is the false positive rate",
            "On the y-axis, we plot sensitivity, the true positive rate",
            "the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy"
          ]
        },
        "raw": "{\"slide_id\": \"Slide40\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"sensitivity\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"specificity\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"ROC curve\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"threshold\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"false positive rate\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"true positive rate\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"AUC\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"diagnostic accuracy\\\", \\\"category\\\": \\\"quality_metric\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"sensitivity and specificity are not fixed properties of a test\\\",\\n    \\\"sweeping through all possible thresholds\\\",\\n    \\\"On the x-axis, we plot 1 – specificity, which is the false positive rate\\\",\\n    \\\"On the y-axis, we plot sensitivity, the true positive rate\\\",\\n    \\\"the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy\\\"\\n  ]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"sensitivity\", \"category\": \"quality_metric\"}, {\"term\": \"specificity\", \"category\": \"quality_metric\"}, {\"term\": \"ROC curve\", \"category\": \"algorithm\"}, {\"term\": \"threshold\", \"category\": \"algorithm\"}, {\"term\": \"false positive rate\", \"category\": \"quality_metric\"}, {\"term\": \"true positive rate\", \"category\": \"quality_metric\"}, {\"term\": \"diagnostic accuracy\", \"category\": \"quality_metric\"}], \"evidence\": [\"sensitivity and specificity are not fixed properties of a test\", \"sweeping through all possible thresholds\", \"On the x-axis, we plot 1 - specificity, which is the false positive rate\", \"On the y-axis, we plot sensitivity, the true positive rate\", \"the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide40.json",
        "parsed": {
          "triples": [
            {
              "s": "ROC curve",
              "p": "measures",
              "o": "sensitivity and specificity",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "Here’s the key idea: sensitivity and specificity are not fixed properties of a test."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide40\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"ROC curve\\\", \\\"p\\\":\\\"represents\\\", \\\"o\\\":\\\"trade-off between sensitivity and specificity\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"The ROC curve captures this trade-off.\\\"},\\n    {\\\"s\\\":\\\"ROC curve\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"sensitivity and specificity\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Here’s the key idea: sensitivity and specificity are not fixed properties of a test.\\\"},\\n    {\\\"s\\\":\\\"ROC curve\\\", \\\"p\\\":\\\"produces\\\", \\\"o\\\":\\\"area under the curve (AUC)\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"And importantly, the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"ROC curve\", \"p\": \"measures\", \"o\": \"sensitivity and specificity\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"Here’s the key idea: sensitivity and specificity are not fixed properties of a test.\"}]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide40.json",
        "parsed": {
          "term": "medical imaging research",
          "category": "algorithm"
        },
        "raw": "{\"slide_id\": \"Slide40\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T04:30:16.532019+00:00\", \"text_length\": 1527, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.\\n\\nHere’s the key idea: sensitivity and specificity are not fixed properties of a test. They depend on the threshold we use to decide between “disease” and “no disease.”\\n\\nFor example, if I set the threshold very low, I’ll call almost everything abnormal. That means I’ll catch nearly every true case, so sensitivity will be very high. But I’ll also generate many false alarms, so specificity will drop.\\nOn the other hand, if I set the threshold very high, I’ll call almost everything normal. That means I’ll have very few false alarms — so specificity will be excellent. But I’ll also miss many real cases, so sensitivity will be low.\\n\\nThe ROC curve captures this trade-off. On the x-axis, we plot 1 – specificity, which is the false positive rate. On the y-axis, we plot sensitivity, the true positive rate. By sweeping through all possible thresholds, we trace out the curve.\\nThe ROC curve shows the overall performance of the test across all possible decision boundaries. And importantly, the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy. An AUC of 1.0 means perfect classification. An AUC of 0.5 — a diagonal line — means the test is no better than random guessing.\\n\\nSo the ROC curve is widely used in medical imaging research, because it provides a clear, quantitative way to compare diagnostic systems and observer performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"receiver operating characteristic curve\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"ROC curve\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"sensitivity\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"specificity\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"threshold\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"false positive rate\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"true positive rate\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"diagnostic performance evaluation\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"area under the ROC curve\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"AUC\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"perfect classification\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"random guessing\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"medical imaging research\\\", \\\"category\\\": \\\"algorithm\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"The ROC curve captures this trade-off.\\\",\\n    \\\"The ROC curve shows the overall performance of the test across all possible decision boundaries.\\\",\\n    \\\"The area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy.\\\",\", \"parsed\": {\"term\": \"medical imaging research\", \"category\": \"algorithm\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide40.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide40\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T05:45:46.521096+00:00\", \"text_length\": 1527, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.\\n\\nHere’s the key idea: sensitivity and specificity are not fixed properties of a test. They depend on the threshold we use to decide between “disease” and “no disease.”\\n\\nFor example, if I set the threshold very low, I’ll call almost everything abnormal. That means I’ll catch nearly every true case, so sensitivity will be very high. But I’ll also generate many false alarms, so specificity will drop.\\nOn the other hand, if I set the threshold very high, I’ll call almost everything normal. That means I’ll have very few false alarms — so specificity will be excellent. But I’ll also miss many real cases, so sensitivity will be low.\\n\\nThe ROC curve captures this trade-off. On the x-axis, we plot 1 – specificity, which is the false positive rate. On the y-axis, we plot sensitivity, the true positive rate. By sweeping through all possible thresholds, we trace out the curve.\\nThe ROC curve shows the overall performance of the test across all possible decision boundaries. And importantly, the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy. An AUC of 1.0 means perfect classification. An AUC of 0.5 — a diagonal line — means the test is no better than random guessing.\\n\\nSo the ROC curve is widely used in medical imaging research, because it provides a clear, quantitative way to compare diagnostic systems and observer performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide40.json",
        "parsed": {
          "term": "An AUC of 0.5 - a diagonal line - means the test is no better than random guessing",
          "category": "quality_metric"
        },
        "raw": "{\"slide_id\": \"Slide40\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:06:33.047348+00:00\", \"text_length\": 1527, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.\\n\\nHere’s the key idea: sensitivity and specificity are not fixed properties of a test. They depend on the threshold we use to decide between “disease” and “no disease.”\\n\\nFor example, if I set the threshold very low, I’ll call almost everything abnormal. That means I’ll catch nearly every true case, so sensitivity will be very high. But I’ll also generate many false alarms, so specificity will drop.\\nOn the other hand, if I set the threshold very high, I’ll call almost everything normal. That means I’ll have very few false alarms — so specificity will be excellent. But I’ll also miss many real cases, so sensitivity will be low.\\n\\nThe ROC curve captures this trade-off. On the x-axis, we plot 1 – specificity, which is the false positive rate. On the y-axis, we plot sensitivity, the true positive rate. By sweeping through all possible thresholds, we trace out the curve.\\nThe ROC curve shows the overall performance of the test across all possible decision boundaries. And importantly, the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy. An AUC of 1.0 means perfect classification. An AUC of 0.5 — a diagonal line — means the test is no better than random guessing.\\n\\nSo the ROC curve is widely used in medical imaging research, because it provides a clear, quantitative way to compare diagnostic systems and observer performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"receiver operating characteristic curve, or ROC curve\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"sensitivity and specificity are not fixed properties of a test\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"They depend on the threshold we use to decide between “disease” and “no disease.”\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"sensitivity, the true positive rate\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"1 – specificity, which is the false positive rate\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"the area under the ROC curve, or AUC\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"An AUC of 1.0 means perfect classification\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"An AUC of 0.5 — a diagonal line — means the test is no better than random guessing\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"\", \"parsed\": {\"term\": \"An AUC of 0.5 - a diagonal line - means the test is no better than random guessing\", \"category\": \"quality_metric\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide40.json",
        "parsed": {
          "s": "ROC curve",
          "p": "produces",
          "o": "area under the ROC curve (AUC)",
          "modalities": [
            "text",
            "image"
          ],
          "confidence": 0.95,
          "evidence": [
            "The area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy."
          ]
        },
        "raw": "{\"slide_id\": \"Slide40\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:15:19.738727+00:00\", \"text_length\": 1527, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.\\n\\nHere’s the key idea: sensitivity and specificity are not fixed properties of a test. They depend on the threshold we use to decide between “disease” and “no disease.”\\n\\nFor example, if I set the threshold very low, I’ll call almost everything abnormal. That means I’ll catch nearly every true case, so sensitivity will be very high. But I’ll also generate many false alarms, so specificity will drop.\\nOn the other hand, if I set the threshold very high, I’ll call almost everything normal. That means I’ll have very few false alarms — so specificity will be excellent. But I’ll also miss many real cases, so sensitivity will be low.\\n\\nThe ROC curve captures this trade-off. On the x-axis, we plot 1 – specificity, which is the false positive rate. On the y-axis, we plot sensitivity, the true positive rate. By sweeping through all possible thresholds, we trace out the curve.\\nThe ROC curve shows the overall performance of the test across all possible decision boundaries. And importantly, the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy. An AUC of 1.0 means perfect classification. An AUC of 0.5 — a diagonal line — means the test is no better than random guessing.\\n\\nSo the ROC curve is widely used in medical imaging research, because it provides a clear, quantitative way to compare diagnostic systems and observer performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"ROC curve\\\",\\n      \\\"p\\\": \\\"represents\\\",\\n      \\\"o\\\": \\\"trade-off between sensitivity and specificity\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"The ROC curve captures this trade-off.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"ROC curve\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"overall performance of the test across all possible decision boundaries\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"The ROC curve shows the overall performance of the test across all possible decision boundaries.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"ROC curve\\\",\\n      \\\"p\\\": \\\"produces\\\",\\n      \\\"o\\\": \\\"area under the ROC curve (AUC)\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"The area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"\", \"parsed\": {\"s\": \"ROC curve\", \"p\": \"produces\", \"o\": \"area under the ROC curve (AUC)\", \"modalities\": [\"text\", \"image\"], \"confidence\": 0.95, \"evidence\": [\"The area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy.\"]}}"
      }
    }
  }
}