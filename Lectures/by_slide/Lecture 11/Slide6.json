{
  "lecture": "Lecture 11",
  "slide_id": "Slide6",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide6.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide6.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide6.json",
        "parsed": {
          "concepts": [
            {
              "term": "Mean Squared Error",
              "category": "quality_metric"
            },
            {
              "term": "Root Mean Squared Error",
              "category": "quality_metric"
            },
            {
              "term": "Mean Absolute Error",
              "category": "quality_metric"
            },
            {
              "term": "Mean Absolute Percentage Error",
              "category": "quality_metric"
            }
          ],
          "evidence": [
            "The first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.",
            "A closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error.",
            "Sometimes we want a measure that treats all errors more equally. That’s where the Mean Absolute Error, or MAE, comes in.",
            "Finally, we have the Mean Absolute Percentage Error, or MAPE. This is the mean absolute error expressed as a percentage of the true value."
          ]
        },
        "raw": "{\"slide_id\": \"Slide6\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, the mean squared error is not the only way to measure differences. There are several variants, each with slightly different properties.\\n\\nThe first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\\nA closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error. Why? Because this brings the units back to the same scale as the original measurement. For example, if we are measuring pixel intensities, RMSE will be expressed in the same units as those intensities, which makes it easier to interpret.\\n\\nOne important property of squaring is that it emphasizes larger errors much more strongly. If a difference is 100, squaring turns it into 10,000. That means MSE and RMSE heavily penalize large deviations.\\nSometimes we want a measure that treats all errors more equally. That’s where the Mean Absolute Error, or MAE, comes in. Instead of squaring, we take the absolute value of the difference at each pixel, then average. This is also called the L1 norm, while MSE is associated with the L2 norm. The L1 norm is less sensitive to outliers compared to the L2 norm.\\n\\nFinally, we have the Mean Absolute Percentage Error, or MAPE. This is the mean absolute error expressed as a percentage of the true value. In other words, it’s MAE divided by the ground truth at each point, multiplied by 100 percent. This can be useful when we want to understand an error in relative terms — for example, saying “the error is 5 percent” rather than giving a raw number.\\n\\nSo, these different distance measures — MSE, RMSE, MAE, and MAPE — give us different perspectives on error. The choice depends on the problem: do we want to penalize large errors more, or do we care more about relative error percentages?\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Mean Squared Error\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"Root Mean Squared Error\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"Mean Absolute Error\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"Mean Absolute Percentage Error\\\", \\\"category\\\": \\\"quality_metric\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"The first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\\\", \\n             \\\"A closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error.\\\", \\n             \\\"Sometimes we want a measure that treats all errors more equally. That’s where the Mean Absolute Error, or MAE, comes in.\\\", \\n             \\\"Finally, we have the Mean Absolute Percentage Error, or MAPE. This is the mean absolute error expressed as a percentage of the true value.\\\"]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"Mean Squared Error\", \"category\": \"quality_metric\"}, {\"term\": \"Root Mean Squared Error\", \"category\": \"quality_metric\"}, {\"term\": \"Mean Absolute Error\", \"category\": \"quality_metric\"}, {\"term\": \"Mean Absolute Percentage Error\", \"category\": \"quality_metric\"}], \"evidence\": [\"The first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\", \"A closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error.\", \"Sometimes we want a measure that treats all errors more equally. That’s where the Mean Absolute Error, or MAE, comes in.\", \"Finally, we have the Mean Absolute Percentage Error, or MAPE. This is the mean absolute error expressed as a percentage of the true value.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide6.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide6\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, the mean squared error is not the only way to measure differences. There are several variants, each with slightly different properties.\\n\\nThe first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\\nA closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error. Why? Because this brings the units back to the same scale as the original measurement. For example, if we are measuring pixel intensities, RMSE will be expressed in the same units as those intensities, which makes it easier to interpret.\\n\\nOne important property of squaring is that it emphasizes larger errors much more strongly. If a difference is 100, squaring turns it into 10,000. That means MSE and RMSE heavily penalize large deviations.\\nSometimes we want a measure that treats all errors more equally. That’s where the Mean Absolute Error, or MAE, comes in. Instead of squaring, we take the absolute value of the difference at each pixel, then average. This is also called the L1 norm, while MSE is associated with the L2 norm. The L1 norm is less sensitive to outliers compared to the L2 norm.\\n\\nFinally, we have the Mean Absolute Percentage Error, or MAPE. This is the mean absolute error expressed as a percentage of the true value. In other words, it’s MAE divided by the ground truth at each point, multiplied by 100 percent. This can be useful when we want to understand an error in relative terms — for example, saying “the error is 5 percent” rather than giving a raw number.\\n\\nSo, these different distance measures — MSE, RMSE, MAE, and MAPE — give us different perspectives on error. The choice depends on the problem: do we want to penalize large errors more, or do we care more about relative error percentages?\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n```json\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"MSE\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"RMSE\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"RMSE is simply the square root of the mean squared error.\\\"}\\n  ]\\n}\\n```\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide6.json",
        "parsed": {
          "concepts": [
            {
              "term": "Mean Squared Error",
              "category": "mathematics"
            },
            {
              "term": "Root Mean Squared Error",
              "category": "mathematics"
            },
            {
              "term": "Mean Absolute Error",
              "category": "mathematics"
            },
            {
              "term": "Mean Absolute Percentage Error",
              "category": "mathematics"
            }
          ],
          "evidence": [
            "average of the squared differences between prediction and truth",
            "square root of the mean squared error",
            "take the absolute value of the difference at each pixel, then average",
            "mean absolute error expressed as a percentage of the true value"
          ]
        },
        "raw": "{\"slide_id\": \"Slide6\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Mean Squared Error\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Root Mean Squared Error\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Mean Absolute Error\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Mean Absolute Percentage Error\\\", \\\"category\\\": \\\"mathematics\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"average of the squared differences between prediction and truth\\\",\\n    \\\"square root of the mean squared error\\\",\\n    \\\"take the absolute value of the difference at each pixel, then average\\\",\\n    \\\"mean absolute error expressed as a percentage of the true value\\\"\\n  ]\\n}\", \"parsed\": {\"concepts\": [{\"term\": \"Mean Squared Error\", \"category\": \"mathematics\"}, {\"term\": \"Root Mean Squared Error\", \"category\": \"mathematics\"}, {\"term\": \"Mean Absolute Error\", \"category\": \"mathematics\"}, {\"term\": \"Mean Absolute Percentage Error\", \"category\": \"mathematics\"}], \"evidence\": [\"average of the squared differences between prediction and truth\", \"square root of the mean squared error\", \"take the absolute value of the difference at each pixel, then average\", \"mean absolute error expressed as a percentage of the true value\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide6.json",
        "parsed": {
          "s": "Mean Absolute Error",
          "p": "measures",
          "o": "absolute value of the difference at each pixel",
          "modalities": [
            "text"
          ],
          "confidence": 1.0,
          "evidence": [
            "Instead of squaring, we take the absolute value of the difference at each pixel, then average."
          ]
        },
        "raw": "{\"slide_id\": \"Slide6\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Mean Squared Error\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"squared differences between prediction and truth\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"The first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\\\"},\\n    {\\\"s\\\":\\\"Root Mean Squared Error\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"square root of the mean squared error\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"A closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error.\\\"},\\n    {\\\"s\\\":\\\"Mean Absolute Error\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"absolute value of the difference at each pixel\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Instead of squaring, we take the absolute value of the difference at each pixel, then average.\\\"},\\n    {\\\"s\\\":\\\"Mean Absolute Percentage Error\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"mean absolute error expressed as a percentage of the true value\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\", \"parsed\": {\"s\": \"Mean Absolute Error\", \"p\": \"measures\", \"o\": \"absolute value of the difference at each pixel\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": [\"Instead of squaring, we take the absolute value of the difference at each pixel, then average.\"]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide6.json",
        "parsed": {
          "term": "Mean absolute percentage error",
          "category": "mathematics"
        },
        "raw": "{\"slide_id\": \"Slide6\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:31:26.952339+00:00\", \"text_length\": 1854, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, the mean squared error is not the only way to measure differences. There are several variants, each with slightly different properties.\\n\\nThe first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\\nA closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error. Why? Because this brings the units back to the same scale as the original measurement. For example, if we are measuring pixel intensities, RMSE will be expressed in the same units as those intensities, which makes it easier to interpret.\\n\\nOne important property of squaring is that it emphasizes larger errors much more strongly. If a difference is 100, squaring turns it into 10,000. That means MSE and RMSE heavily penalize large deviations.\\nSometimes we want a measure that treats all errors more equally. That’s where the Mean Absolute Error, or MAE, comes in. Instead of squaring, we take the absolute value of the difference at each pixel, then average. This is also called the L1 norm, while MSE is associated with the L2 norm. The L1 norm is less sensitive to outliers compared to the L2 norm.\\n\\nFinally, we have the Mean Absolute Percentage Error, or MAPE. This is the mean absolute error expressed as a percentage of the true value. In other words, it’s MAE divided by the ground truth at each point, multiplied by 100 percent. This can be useful when we want to understand an error in relative terms — for example, saying “the error is 5 percent” rather than giving a raw number.\\n\\nSo, these different distance measures — MSE, RMSE, MAE, and MAPE — give us different perspectives on error. The choice depends on the problem: do we want to penalize large errors more, or do we care more about relative error percentages?\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Mean squared error\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Root mean squared error\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Mean absolute error\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Mean absolute percentage error\\\", \\\"category\\\": \\\"mathematics\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"The first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\\\",\\n    \\\"A closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error.\\\",\\n    \\\"One important property of squaring is that it emphasizes larger errors much more strongly. If a difference is 100, squaring turns it into 10,000. That means MSE and RMSE heavily penalize large deviations.\\\",\\n    \\\"Sometimes we want a measure that treats all errors more equally. That’s where the Mean Absolute Error, or MAE, comes in. Instead of squaring, we take the absolute value of the difference at each pixel, then average.\\\",\\n    \\\"The L1 norm is less sensitive\", \"parsed\": {\"term\": \"Mean absolute percentage error\", \"category\": \"mathematics\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide6.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide6\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T05:20:47.617422+00:00\", \"text_length\": 1854, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, the mean squared error is not the only way to measure differences. There are several variants, each with slightly different properties.\\n\\nThe first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\\nA closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error. Why? Because this brings the units back to the same scale as the original measurement. For example, if we are measuring pixel intensities, RMSE will be expressed in the same units as those intensities, which makes it easier to interpret.\\n\\nOne important property of squaring is that it emphasizes larger errors much more strongly. If a difference is 100, squaring turns it into 10,000. That means MSE and RMSE heavily penalize large deviations.\\nSometimes we want a measure that treats all errors more equally. That’s where the Mean Absolute Error, or MAE, comes in. Instead of squaring, we take the absolute value of the difference at each pixel, then average. This is also called the L1 norm, while MSE is associated with the L2 norm. The L1 norm is less sensitive to outliers compared to the L2 norm.\\n\\nFinally, we have the Mean Absolute Percentage Error, or MAPE. This is the mean absolute error expressed as a percentage of the true value. In other words, it’s MAE divided by the ground truth at each point, multiplied by 100 percent. This can be useful when we want to understand an error in relative terms — for example, saying “the error is 5 percent” rather than giving a raw number.\\n\\nSo, these different distance measures — MSE, RMSE, MAE, and MAPE — give us different perspectives on error. The choice depends on the problem: do we want to penalize large errors more, or do we care more about relative error percentages?\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Mean squared error\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"MSE\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"Mean squared error is the average of the squared differences between prediction and truth.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide6.json",
        "parsed": {
          "concepts": [
            {
              "term": "mean squared error",
              "category": "quality_metric"
            },
            {
              "term": "root mean squared error",
              "category": "quality_metric"
            },
            {
              "term": "mean absolute error",
              "category": "quality_metric"
            },
            {
              "term": "mean absolute percentage error",
              "category": "quality_metric"
            },
            {
              "term": "l1 norm",
              "category": "mathematics"
            },
            {
              "term": "l2 norm",
              "category": "mathematics"
            }
          ],
          "evidence": [
            "mean squared error is the average of the squared differences between prediction and truth",
            "root mean squared error is the square root of the mean squared error",
            "mean absolute error is the average of the absolute value of the difference",
            "mean absolute percentage error is the mean absolute error expressed as a percentage of the true value",
            "l1 norm is the mean absolute error",
            "l2 norm is associated with the mean squared error"
          ]
        },
        "raw": "{\"slide_id\": \"Slide6\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:00:51.851582+00:00\", \"text_length\": 1854, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, the mean squared error is not the only way to measure differences. There are several variants, each with slightly different properties.\\n\\nThe first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\\nA closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error. Why? Because this brings the units back to the same scale as the original measurement. For example, if we are measuring pixel intensities, RMSE will be expressed in the same units as those intensities, which makes it easier to interpret.\\n\\nOne important property of squaring is that it emphasizes larger errors much more strongly. If a difference is 100, squaring turns it into 10,000. That means MSE and RMSE heavily penalize large deviations.\\nSometimes we want a measure that treats all errors more equally. That’s where the Mean Absolute Error, or MAE, comes in. Instead of squaring, we take the absolute value of the difference at each pixel, then average. This is also called the L1 norm, while MSE is associated with the L2 norm. The L1 norm is less sensitive to outliers compared to the L2 norm.\\n\\nFinally, we have the Mean Absolute Percentage Error, or MAPE. This is the mean absolute error expressed as a percentage of the true value. In other words, it’s MAE divided by the ground truth at each point, multiplied by 100 percent. This can be useful when we want to understand an error in relative terms — for example, saying “the error is 5 percent” rather than giving a raw number.\\n\\nSo, these different distance measures — MSE, RMSE, MAE, and MAPE — give us different perspectives on error. The choice depends on the problem: do we want to penalize large errors more, or do we care more about relative error percentages?\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"mean squared error\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"root mean squared error\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"mean absolute error\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"mean absolute percentage error\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"l1 norm\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"l2 norm\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    }\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"mean squared error is the average of the squared differences between prediction and truth\\\",\\n    \\\"root mean squared error is the square root of the mean squared error\\\",\\n    \\\"mean absolute error is the average of the absolute value of the difference\\\",\\n    \\\"mean absolute percentage error is the mean absolute error expressed as a percentage of the true value\\\",\\n    \\\"l1 norm is the mean absolute error\\\",\\n    \\\"l2 norm is associated with the mean squared error\\\"\\n  ]\\n}\", \"parsed\": {\"concepts\": [{\"term\": \"mean squared error\", \"category\": \"quality_metric\"}, {\"term\": \"root mean squared error\", \"category\": \"quality_metric\"}, {\"term\": \"mean absolute error\", \"category\": \"quality_metric\"}, {\"term\": \"mean absolute percentage error\", \"category\": \"quality_metric\"}, {\"term\": \"l1 norm\", \"category\": \"mathematics\"}, {\"term\": \"l2 norm\", \"category\": \"mathematics\"}], \"evidence\": [\"mean squared error is the average of the squared differences between prediction and truth\", \"root mean squared error is the square root of the mean squared error\", \"mean absolute error is the average of the absolute value of the difference\", \"mean absolute percentage error is the mean absolute error expressed as a percentage of the true value\", \"l1 norm is the mean absolute error\", \"l2 norm is associated with the mean squared error\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide6.json",
        "parsed": {
          "s": "Root mean squared error",
          "p": "produces",
          "o": "the square root of the mean squared error",
          "modalities": [
            "text"
          ],
          "confidence": 0.95,
          "evidence": [
            "A closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error."
          ]
        },
        "raw": "{\"slide_id\": \"Slide6\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:11:08.510148+00:00\", \"text_length\": 1854, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, the mean squared error is not the only way to measure differences. There are several variants, each with slightly different properties.\\n\\nThe first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\\nA closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error. Why? Because this brings the units back to the same scale as the original measurement. For example, if we are measuring pixel intensities, RMSE will be expressed in the same units as those intensities, which makes it easier to interpret.\\n\\nOne important property of squaring is that it emphasizes larger errors much more strongly. If a difference is 100, squaring turns it into 10,000. That means MSE and RMSE heavily penalize large deviations.\\nSometimes we want a measure that treats all errors more equally. That’s where the Mean Absolute Error, or MAE, comes in. Instead of squaring, we take the absolute value of the difference at each pixel, then average. This is also called the L1 norm, while MSE is associated with the L2 norm. The L1 norm is less sensitive to outliers compared to the L2 norm.\\n\\nFinally, we have the Mean Absolute Percentage Error, or MAPE. This is the mean absolute error expressed as a percentage of the true value. In other words, it’s MAE divided by the ground truth at each point, multiplied by 100 percent. This can be useful when we want to understand an error in relative terms — for example, saying “the error is 5 percent” rather than giving a raw number.\\n\\nSo, these different distance measures — MSE, RMSE, MAE, and MAPE — give us different perspectives on error. The choice depends on the problem: do we want to penalize large errors more, or do we care more about relative error percentages?\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"Mean squared error\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"the average of the squared differences between prediction and truth\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"The first, which we’ve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"Root mean squared error\\\",\\n      \\\"p\\\": \\\"produces\\\",\\n      \\\"o\\\": \\\"the square root of the mean squared error\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"A closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"Mean absolute error\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"the average of the absolute values of the differences between prediction and truth\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\"\\n      ],\\n      \\\"confidence\\\": 0.95\", \"parsed\": {\"s\": \"Root mean squared error\", \"p\": \"produces\", \"o\": \"the square root of the mean squared error\", \"modalities\": [\"text\"], \"confidence\": 0.95, \"evidence\": [\"A closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error.\"]}}"
      }
    }
  }
}