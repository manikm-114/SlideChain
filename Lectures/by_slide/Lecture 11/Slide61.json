{
  "lecture": "Lecture 11",
  "slide_id": "Slide61",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide61.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide61.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide61.json",
        "parsed": {
          "term": "state-of-the-art performance",
          "category": "ai_ml"
        },
        "raw": "{\"slide_id\": \"Slide61\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nSupervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\n\\nThe figure shows how the inputs are transformed step by step through hidden units and weights, leading to correct outputs for all training examples. This principle extends far beyond toy problems—it is the basis of modern deep learning, where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"supervised learning\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"hidden layers\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"nonlinear activation functions\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"neural network\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"XOR problem\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"deep learning\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"medical images\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"hierarchical features\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"state-of-the-art performance\\\", \\\"category\\\": \\\"ai_ml\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"Supervised learning builds on this idea by training models directly on labeled data.\\\", \\n               \\\"For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\\", \\n               \\\"This principle extends far beyond toy problems—it is the basis of modern deep learning, where very large networks can learn hierarchical features from medical images and achieve state-of\", \"parsed\": {\"term\": \"state-of-the-art performance\", \"category\": \"ai_ml\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide61.json",
        "parsed": {
          "s": "neural network",
          "p": "uses|via|represents|depends_on|measures|produces|reconstructs_with",
          "o": "hidden layers and nonlinear activation functions",
          "modalities": [
            "text"
          ],
          "confidence": 0.0,
          "evidence": [
            "can learn to separate the classes correctly"
          ]
        },
        "raw": "{\"slide_id\": \"Slide61\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nSupervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\n\\nThe figure shows how the inputs are transformed step by step through hidden units and weights, leading to correct outputs for all training examples. This principle extends far beyond toy problems—it is the basis of modern deep learning, where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"supervised learning\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"training models\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"builds on this idea by training models directly on labeled data\\\"}\\n  ],\\n    {\\\"s\\\":\\\"XOR problem\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"linear classifiers\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"cannot draw a single straight line to separate the classes\\\"}\\n  ],\\n    {\\\"s\\\":\\\"neural network\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"hidden layers and nonlinear activation functions\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"can learn to separate the classes correctly\\\"}\\n  ],\\n    {\\\"s\\\":\\\"modern deep learning\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"large networks\\\",\", \"parsed\": {\"s\": \"neural network\", \"p\": \"uses|via|represents|depends_on|measures|produces|reconstructs_with\", \"o\": \"hidden layers and nonlinear activation functions\", \"modalities\": [\"text\"], \"confidence\": 0.0, \"evidence\": [\"can learn to separate the classes correctly\"]}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide61.json",
        "parsed": {
          "concepts": [
            {
              "term": "supervised learning",
              "category": "ai_ml"
            },
            {
              "term": "linear classifiers",
              "category": "algorithm"
            },
            {
              "term": "hidden layers",
              "category": "ai_ml"
            },
            {
              "term": "nonlinear activation functions",
              "category": "ai_ml"
            },
            {
              "term": "neural network",
              "category": "ai_ml"
            },
            {
              "term": "deep learning",
              "category": "ai_ml"
            },
            {
              "term": "hierarchical features",
              "category": "ai_ml"
            },
            {
              "term": "state-of-the-art performance",
              "category": "ai_ml"
            }
          ],
          "evidence": [
            "Supervised learning builds on this idea by training models directly on labeled data.",
            "linear classifiers cannot draw a single straight line to separate the classes.",
            "by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.",
            "This principle extends far beyond toy problems-it is the basis of modern deep learning",
            "very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance."
          ]
        },
        "raw": "{\"slide_id\": \"Slide61\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"supervised learning\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"linear classifiers\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"hidden layers\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"nonlinear activation functions\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"neural network\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"deep learning\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"hierarchical features\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"state-of-the-art performance\\\", \\\"category\\\": \\\"ai_ml\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Supervised learning builds on this idea by training models directly on labeled data.\\\",\\n    \\\"linear classifiers cannot draw a single straight line to separate the classes.\\\",\\n    \\\"by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\\",\\n    \\\"This principle extends far beyond toy problems—it is the basis of modern deep learning\\\",\\n    \\\"very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance.\\\"\\n  ]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"supervised learning\", \"category\": \"ai_ml\"}, {\"term\": \"linear classifiers\", \"category\": \"algorithm\"}, {\"term\": \"hidden layers\", \"category\": \"ai_ml\"}, {\"term\": \"nonlinear activation functions\", \"category\": \"ai_ml\"}, {\"term\": \"neural network\", \"category\": \"ai_ml\"}, {\"term\": \"deep learning\", \"category\": \"ai_ml\"}, {\"term\": \"hierarchical features\", \"category\": \"ai_ml\"}, {\"term\": \"state-of-the-art performance\", \"category\": \"ai_ml\"}], \"evidence\": [\"Supervised learning builds on this idea by training models directly on labeled data.\", \"linear classifiers cannot draw a single straight line to separate the classes.\", \"by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\", \"This principle extends far beyond toy problems-it is the basis of modern deep learning\", \"very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide61.json",
        "parsed": {
          "s": "neural network",
          "p": "introduces",
          "o": "hidden layers and nonlinear activation functions",
          "modalities": [
            "text"
          ],
          "confidence": 1.0,
          "evidence": [
            "But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly."
          ]
        },
        "raw": "{\"slide_id\": \"Slide61\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"Supervised learning\\\",\\n      \\\"p\\\": \\\"uses\\\",\\n      \\\"o\\\": \\\"labeled data\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"Supervised learning builds on this idea by training models directly on labeled data.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"linear classifiers\\\",\\n      \\\"p\\\": \\\"cannot solve\\\",\\n      \\\"o\\\": \\\"XOR problem\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"neural network\\\",\\n      \\\"p\\\": \\\"introduces\\\",\\n      \\\"o\\\": \\\"hidden layers and nonlinear activation functions\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"neural network\\\",\\n      \\\"p\\\": \\\"produces\\\",\\n      \\\"o\\\": \\\"correct outputs\\\",\\n      \\\"modalities\", \"parsed\": {\"s\": \"neural network\", \"p\": \"introduces\", \"o\": \"hidden layers and nonlinear activation functions\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": [\"But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\"]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide61.json",
        "parsed": {
          "term": "state-of-the-art performance",
          "category": "algorithm"
        },
        "raw": "{\"slide_id\": \"Slide61\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T05:10:18.064484+00:00\", \"text_length\": 685, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nSupervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\n\\nThe figure shows how the inputs are transformed step by step through hidden units and weights, leading to correct outputs for all training examples. This principle extends far beyond toy problems—it is the basis of modern deep learning, where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Supervised Learning\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"XOR problem\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"linear classifiers\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"hidden layers\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"nonlinear activation functions\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"neural network\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"training models\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"labeled data\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"correct outputs\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"modern deep learning\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"medical images\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"state-of-the-art performance\\\", \\\"category\\\": \\\"algorithm\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Supervised learning builds on this idea by training models directly on labeled data.\\\",\\n    \\\"For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes.\\\",\\n    \\\"But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\\",\", \"parsed\": {\"term\": \"state-of-the-art performance\", \"category\": \"algorithm\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide61.json",
        "parsed": {
          "triples": [
            {
              "s": "Supervised Learning",
              "p": "uses",
              "o": "XOR problem",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "Supervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide61\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T06:02:08.494544+00:00\", \"text_length\": 685, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nSupervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\n\\nThe figure shows how the inputs are transformed step by step through hidden units and weights, leading to correct outputs for all training examples. This principle extends far beyond toy problems—it is the basis of modern deep learning, where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Supervised Learning\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"XOR problem\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Supervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"Supervised Learning\", \"p\": \"uses\", \"o\": \"XOR problem\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"Supervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\"}]}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide61.json",
        "parsed": {
          "term": "where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance",
          "category": "ai_ml"
        },
        "raw": "{\"slide_id\": \"Slide61\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:10:10.521736+00:00\", \"text_length\": 685, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nSupervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\n\\nThe figure shows how the inputs are transformed step by step through hidden units and weights, leading to correct outputs for all training examples. This principle extends far beyond toy problems—it is the basis of modern deep learning, where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"Supervised learning builds on this idea by training models directly on labeled data\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"linear classifiers cannot solve this\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"The figure shows how the inputs are transformed step by step through hidden units and weights, leading to correct outputs for all training examples\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"This principle extends far beyond toy problems—it is the basis of modern deep learning\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    }\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Supervised learning builds on this idea by training models directly on labeled data\\\",\\n    \\\"linear classifiers cannot solve this\\\",\\n    \\\"by introducing hidden layers and nonlinear activation functions, a\", \"parsed\": {\"term\": \"where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance\", \"category\": \"ai_ml\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide61.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide61\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:18:26.418065+00:00\", \"text_length\": 685, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nSupervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\\n\\nThe figure shows how the inputs are transformed step by step through hidden units and weights, leading to correct outputs for all training examples. This principle extends far beyond toy problems—it is the basis of modern deep learning, where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": []\\n}\", \"parsed\": {\"triples\": []}}"
      }
    }
  }
}