{
  "lecture": "Lecture 11",
  "slide_id": "Slide12",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide12.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide12.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide12.json",
        "parsed": {
          "term": "structural similarity",
          "category": "quality_metric"
        },
        "raw": "{\"slide_id\": \"Slide12\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s think about this from the perspective of the human visual system, or HVS.\\n\\nThe human eye does not look at images pixel by pixel, the way mean squared error does. MSE simply compares each pixel individually, treats every error the same, and then adds them up. That’s a bottom-up approach — starting from the smallest units and working upward.\\nBut our visual system works very differently. We are much more sensitive to structural information — the patterns, edges, and relationships that give an image its overall form. For example, even a small change in the background, or a shift in texture, is something we can notice right away. Our brains are highly adapted to detect these kinds of contextual changes.\\n\\nIn the classical view, the focus was on error visibility: if you see a discrepancy, count it as an error. In the newer view, the focus shifts to structural distortion: what matters is whether the structure of the image has been preserved.\\nThis also connects to the way our vision system interprets images. Rather than starting with tiny details, many researchers argue that we first process the global structure — sometimes described in terms of topological features. These are high-level properties, such as whether objects are connected, how many distinct regions there are, or whether certain shapes remain intact. Importantly, these properties don’t change if the image is stretched, rotated, or rescaled.\\n\\nSo here’s the philosophical shift: instead of measuring differences pixel by pixel, we want to measure how much of the structural information has been preserved. That is what structural similarity is all about, and it’s why it is far more aligned with how humans actually see images.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"pixel\\\", \\\"category\\\": \\\"software\\\"},\\n    {\\\"term\\\": \\\"mean squared error\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"bottom-up approach\\\", \\\"category\\\": \\\"software\\\"},\\n    {\\\"term\\\": \\\"structural information\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"structural distortion\\\", \\\"category\\\": \\\"software\\\"},\\n    {\\\"term\\\": \\\"global structure\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"topological features\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"structural similarity\\\", \\\"category\\\": \\\"quality_metric\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"The human eye does not look at images pixel by pixel, the way mean squared error does.\\\", \\n               \\\"That’s a bottom-up approach — starting from the smallest units and working upward.\\\", \\n               \\\"In the classical view, the focus was on error visibility: if you see a discrepancy, count it as an error.\\\", \\n               \\\"In the newer view, the focus shifts to structural distortion: what matters is whether the structure of the image has been preserved.\\\", \\n               \\\"Rather than starting with tiny details, many researchers argue that we first process the global structure — sometimes described in terms of\", \"parsed\": {\"term\": \"structural similarity\", \"category\": \"quality_metric\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide12.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide12\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s think about this from the perspective of the human visual system, or HVS.\\n\\nThe human eye does not look at images pixel by pixel, the way mean squared error does. MSE simply compares each pixel individually, treats every error the same, and then adds them up. That’s a bottom-up approach — starting from the smallest units and working upward.\\nBut our visual system works very differently. We are much more sensitive to structural information — the patterns, edges, and relationships that give an image its overall form. For example, even a small change in the background, or a shift in texture, is something we can notice right away. Our brains are highly adapted to detect these kinds of contextual changes.\\n\\nIn the classical view, the focus was on error visibility: if you see a discrepancy, count it as an error. In the newer view, the focus shifts to structural distortion: what matters is whether the structure of the image has been preserved.\\nThis also connects to the way our vision system interprets images. Rather than starting with tiny details, many researchers argue that we first process the global structure — sometimes described in terms of topological features. These are high-level properties, such as whether objects are connected, how many distinct regions there are, or whether certain shapes remain intact. Importantly, these properties don’t change if the image is stretched, rotated, or rescaled.\\n\\nSo here’s the philosophical shift: instead of measuring differences pixel by pixel, we want to measure how much of the structural information has been preserved. That is what structural similarity is all about, and it’s why it is far more aligned with how humans actually see images.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"human visual system (HVS)\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"mean squared error (MSE)\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"MSE simply compares each pixel individually, treats every error the same, and then adds them up. That’s a bottom-up approach — starting from the smallest units and working upward.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide12.json",
        "parsed": {
          "concepts": [
            {
              "term": "Bottom-up",
              "category": "algorithm"
            },
            {
              "term": "Error Visibility",
              "category": "quality_metric"
            },
            {
              "term": "Structural Distortion",
              "category": "quality_metric"
            },
            {
              "term": "Structural Information",
              "category": "algorithm"
            },
            {
              "term": "Structural Similarity",
              "category": "quality_metric"
            }
          ],
          "evidence": [
            "the way mean squared error does",
            "Our visual system works very differently",
            "the patterns, edges, and relationships",
            "we first process the global structure",
            "if you see a discrepancy, count it as an error",
            "whether the structure of the image has been preserved",
            "how much of the structural information has been preserved"
          ]
        },
        "raw": "{\"slide_id\": \"Slide12\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"HVS Extracts Structural Information\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"HVS Highly Adapted for Contextual Changes\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Bottom-up\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Top-down\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Error Visibility\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"Structural Distortion\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"Structural Information\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Structural Similarity\\\", \\\"category\\\": \\\"quality_metric\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"the way mean squared error does\\\",\\n    \\\"Our visual system works very differently\\\",\\n    \\\"the patterns, edges, and relationships\\\",\\n    \\\"we first process the global structure\\\",\\n    \\\"if you see a discrepancy, count it as an error\\\",\\n    \\\"whether the structure of the image has been preserved\\\",\\n    \\\"how much of the structural information has been preserved\\\"\\n  ]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"Bottom-up\", \"category\": \"algorithm\"}, {\"term\": \"Error Visibility\", \"category\": \"quality_metric\"}, {\"term\": \"Structural Distortion\", \"category\": \"quality_metric\"}, {\"term\": \"Structural Information\", \"category\": \"algorithm\"}, {\"term\": \"Structural Similarity\", \"category\": \"quality_metric\"}], \"evidence\": [\"the way mean squared error does\", \"Our visual system works very differently\", \"the patterns, edges, and relationships\", \"we first process the global structure\", \"if you see a discrepancy, count it as an error\", \"whether the structure of the image has been preserved\", \"how much of the structural information has been preserved\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide12.json",
        "parsed": {
          "s": "HVS",
          "p": "uses",
          "o": "Top-down",
          "modalities": [
            "text"
          ],
          "confidence": 1.0,
          "evidence": [
            "Rather than starting with tiny details, many researchers argue that we first process the global structure"
          ]
        },
        "raw": "{\"slide_id\": \"Slide12\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"HVS\\\", \\\"p\\\":\\\"extracts\\\", \\\"o\\\":\\\"Structural Information\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"HVS Extracts Structural Information\\\"},\\n    {\\\"s\\\":\\\"HVS\\\", \\\"p\\\":\\\"is highly adapted for\\\", \\\"o\\\":\\\"Contextual Changes\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"HVS Highly Adapted for Contextual Changes\\\"},\\n    {\\\"s\\\":\\\"Classical\\\", \\\"p\\\":\\\"focuses on\\\", \\\"o\\\":\\\"Error Visibility\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"In the classical view, the focus was on error visibility\\\"},\\n    {\\\"s\\\":\\\"New\\\", \\\"p\\\":\\\"focuses on\\\", \\\"o\\\":\\\"Structural Distortion\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"In the newer view, the focus shifts to structural distortion\\\"},\\n    {\\\"s\\\":\\\"HVS\\\", \\\"p\\\":\\\"uses\\\", \\\"o\\\":\\\"Top-down\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Rather than starting with tiny details, many researchers argue that we first process the global structure\\\"},\\n    {\\\"s\\\":\\\"\", \"parsed\": {\"s\": \"HVS\", \"p\": \"uses\", \"o\": \"Top-down\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": [\"Rather than starting with tiny details, many researchers argue that we first process the global structure\"]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide12.json",
        "parsed": {
          "term": "Structural Similarity",
          "category": "signal_processing"
        },
        "raw": "{\"slide_id\": \"Slide12\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:40:25.431965+00:00\", \"text_length\": 1713, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s think about this from the perspective of the human visual system, or HVS.\\n\\nThe human eye does not look at images pixel by pixel, the way mean squared error does. MSE simply compares each pixel individually, treats every error the same, and then adds them up. That’s a bottom-up approach — starting from the smallest units and working upward.\\nBut our visual system works very differently. We are much more sensitive to structural information — the patterns, edges, and relationships that give an image its overall form. For example, even a small change in the background, or a shift in texture, is something we can notice right away. Our brains are highly adapted to detect these kinds of contextual changes.\\n\\nIn the classical view, the focus was on error visibility: if you see a discrepancy, count it as an error. In the newer view, the focus shifts to structural distortion: what matters is whether the structure of the image has been preserved.\\nThis also connects to the way our vision system interprets images. Rather than starting with tiny details, many researchers argue that we first process the global structure — sometimes described in terms of topological features. These are high-level properties, such as whether objects are connected, how many distinct regions there are, or whether certain shapes remain intact. Importantly, these properties don’t change if the image is stretched, rotated, or rescaled.\\n\\nSo here’s the philosophical shift: instead of measuring differences pixel by pixel, we want to measure how much of the structural information has been preserved. That is what structural similarity is all about, and it’s why it is far more aligned with how humans actually see images.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"HVS\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Structural Information\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"Contextual Changes\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"Bottom-up\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"Top-down\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"Error Visibility\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"Structural Distortion\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"Classical\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"New\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Global Structure\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"Topological Features\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"Structural Similarity\\\", \\\"category\\\": \\\"signal_processing\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"The human eye does not look at images pixel by pixel, the way mean squared error does.\\\",\\n    \\\"Our brains are highly adapted to detect these kinds of contextual changes.\\\",\\n    \\\"In the classical view, the focus was on error visibility: if you see a discrepancy, count it\", \"parsed\": {\"term\": \"Structural Similarity\", \"category\": \"signal_processing\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide12.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide12\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T05:25:11.981507+00:00\", \"text_length\": 1713, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s think about this from the perspective of the human visual system, or HVS.\\n\\nThe human eye does not look at images pixel by pixel, the way mean squared error does. MSE simply compares each pixel individually, treats every error the same, and then adds them up. That’s a bottom-up approach — starting from the smallest units and working upward.\\nBut our visual system works very differently. We are much more sensitive to structural information — the patterns, edges, and relationships that give an image its overall form. For example, even a small change in the background, or a shift in texture, is something we can notice right away. Our brains are highly adapted to detect these kinds of contextual changes.\\n\\nIn the classical view, the focus was on error visibility: if you see a discrepancy, count it as an error. In the newer view, the focus shifts to structural distortion: what matters is whether the structure of the image has been preserved.\\nThis also connects to the way our vision system interprets images. Rather than starting with tiny details, many researchers argue that we first process the global structure — sometimes described in terms of topological features. These are high-level properties, such as whether objects are connected, how many distinct regions there are, or whether certain shapes remain intact. Importantly, these properties don’t change if the image is stretched, rotated, or rescaled.\\n\\nSo here’s the philosophical shift: instead of measuring differences pixel by pixel, we want to measure how much of the structural information has been preserved. That is what structural similarity is all about, and it’s why it is far more aligned with how humans actually see images.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide12.json",
        "parsed": {
          "term": "mean squared error",
          "category": "mathematics"
        },
        "raw": "{\"slide_id\": \"Slide12\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:01:51.878471+00:00\", \"text_length\": 1713, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s think about this from the perspective of the human visual system, or HVS.\\n\\nThe human eye does not look at images pixel by pixel, the way mean squared error does. MSE simply compares each pixel individually, treats every error the same, and then adds them up. That’s a bottom-up approach — starting from the smallest units and working upward.\\nBut our visual system works very differently. We are much more sensitive to structural information — the patterns, edges, and relationships that give an image its overall form. For example, even a small change in the background, or a shift in texture, is something we can notice right away. Our brains are highly adapted to detect these kinds of contextual changes.\\n\\nIn the classical view, the focus was on error visibility: if you see a discrepancy, count it as an error. In the newer view, the focus shifts to structural distortion: what matters is whether the structure of the image has been preserved.\\nThis also connects to the way our vision system interprets images. Rather than starting with tiny details, many researchers argue that we first process the global structure — sometimes described in terms of topological features. These are high-level properties, such as whether objects are connected, how many distinct regions there are, or whether certain shapes remain intact. Importantly, these properties don’t change if the image is stretched, rotated, or rescaled.\\n\\nSo here’s the philosophical shift: instead of measuring differences pixel by pixel, we want to measure how much of the structural information has been preserved. That is what structural similarity is all about, and it’s why it is far more aligned with how humans actually see images.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"human visual system\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"structural information\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"contextual changes\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"bottom-up\\\",\\n      \\\"category\\\": \\\"data_processing\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"top-down\\\",\\n      \\\"category\\\": \\\"data_processing\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"error visibility\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"structural distortion\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"structural similarity\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"topological features\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"high-level properties\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"mean squared error\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    }\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"The human eye does not look at images pixel by pixel, the way\", \"parsed\": {\"term\": \"mean squared error\", \"category\": \"mathematics\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide12.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide12\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:11:47.151342+00:00\", \"text_length\": 1713, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s think about this from the perspective of the human visual system, or HVS.\\n\\nThe human eye does not look at images pixel by pixel, the way mean squared error does. MSE simply compares each pixel individually, treats every error the same, and then adds them up. That’s a bottom-up approach — starting from the smallest units and working upward.\\nBut our visual system works very differently. We are much more sensitive to structural information — the patterns, edges, and relationships that give an image its overall form. For example, even a small change in the background, or a shift in texture, is something we can notice right away. Our brains are highly adapted to detect these kinds of contextual changes.\\n\\nIn the classical view, the focus was on error visibility: if you see a discrepancy, count it as an error. In the newer view, the focus shifts to structural distortion: what matters is whether the structure of the image has been preserved.\\nThis also connects to the way our vision system interprets images. Rather than starting with tiny details, many researchers argue that we first process the global structure — sometimes described in terms of topological features. These are high-level properties, such as whether objects are connected, how many distinct regions there are, or whether certain shapes remain intact. Importantly, these properties don’t change if the image is stretched, rotated, or rescaled.\\n\\nSo here’s the philosophical shift: instead of measuring differences pixel by pixel, we want to measure how much of the structural information has been preserved. That is what structural similarity is all about, and it’s why it is far more aligned with how humans actually see images.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": []\\n}\", \"parsed\": {\"triples\": []}}"
      }
    }
  }
}