{
  "lecture": "Lecture 11",
  "slide_id": "Slide49",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide49.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide49.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide49.json",
        "parsed": {
          "term": "between",
          "category": "signal_processing"
        },
        "raw": "{\"slide_id\": \"Slide49\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s quantify what we’ve been discussing. The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds. But how do we summarize the curve with a single number?\\nThat’s where the area under the ROC curve, or AUC, comes in.\\n\\nIf the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value — you’re essentially flipping a coin. A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1. In practice, no system achieves that ideal, because there’s always some chance of error.\\nSo most real diagnostic tests fall somewhere in between. The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases.\\n\\nThis is why AUC has become such a standard benchmark. It condenses all those trade-offs between sensitivity and specificity into a single number. And just like in teaching or training, performance varies — some students, or some doctors, do exceptionally well; others struggle. The ROC and its AUC make that difference visible in a very clear, quantitative way.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"receiver operating characteristic\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"ROC curve\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"area under the ROC curve\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"AUC\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"diagnostic performance\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"decision thresholds\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"sensitivity and specificity\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"trade-offs\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"quantitative\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"standard benchmark\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"diagnostic tests\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"diseased from non-diseased cases\\\", \\\"category\\\": \\\"modality\\\"},\\n    {\\\"term\\\": \\\"ideal\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"error\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"between\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"between\\\", \\\"category\\\": \\\"\", \"parsed\": {\"term\": \"between\", \"category\": \"signal_processing\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide49.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide49\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s quantify what we’ve been discussing. The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds. But how do we summarize the curve with a single number?\\nThat’s where the area under the ROC curve, or AUC, comes in.\\n\\nIf the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value — you’re essentially flipping a coin. A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1. In practice, no system achieves that ideal, because there’s always some chance of error.\\nSo most real diagnostic tests fall somewhere in between. The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases.\\n\\nThis is why AUC has become such a standard benchmark. It condenses all those trade-offs between sensitivity and specificity into a single number. And just like in teaching or training, performance varies — some students, or some doctors, do exceptionally well; others struggle. The ROC and its AUC make that difference visible in a very clear, quantitative way.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"receiver operating characteristic\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"area under the ROC curve\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds. But how do we summarize the curve with a single number? That’s where the area under the ROC curve, or AUC, comes in.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide49.json",
        "parsed": {
          "term": "predictive value",
          "category": "quality_metric"
        },
        "raw": "{\"slide_id\": \"Slide49\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"receiver operating characteristic\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"ROC curve\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"area under the ROC curve\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"AUC\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"sensitivity\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"specificity\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"predictive value\\\", \\\"category\\\": \\\"quality_metric\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds.\\\",\\n    \\\"That’s where the area under the ROC curve, or AUC, comes in.\\\",\\n    \\\"If the curve falls along the diagonal, the area is 0.5.\\\",\\n    \\\"A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1.\\\",\\n    \\\"The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases.\\\",\\n    \\\"It condenses all those trade-offs\", \"parsed\": {\"term\": \"predictive value\", \"category\": \"quality_metric\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide49.json",
        "parsed": {
          "triples": [
            {
              "s": "ROC curve",
              "p": "measures",
              "o": "diagnostic performance",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds."
            },
            {
              "s": "AUC",
              "p": "represents",
              "o": "area under the ROC curve",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "That’s where the area under the ROC curve, or AUC, comes in."
            },
            {
              "s": "AUC",
              "p": "measures",
              "o": "predictive value",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "If the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value - you’re essentially flipping a coin."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide49\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"ROC curve\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"diagnostic performance\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds.\\\"},\\n    {\\\"s\\\":\\\"AUC\\\", \\\"p\\\":\\\"represents\\\", \\\"o\\\":\\\"area under the ROC curve\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"That’s where the area under the ROC curve, or AUC, comes in.\\\"},\\n    {\\\"s\\\":\\\"AUC\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"predictive value\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"If the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value — you’re essentially flipping a coin.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"ROC curve\", \"p\": \"measures\", \"o\": \"diagnostic performance\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds.\"}, {\"s\": \"AUC\", \"p\": \"represents\", \"o\": \"area under the ROC curve\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"That’s where the area under the ROC curve, or AUC, comes in.\"}, {\"s\": \"AUC\", \"p\": \"measures\", \"o\": \"predictive value\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"If the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value - you’re essentially flipping a coin.\"}]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide49.json",
        "parsed": {
          "term": "trade-offs",
          "category": "signal_processing"
        },
        "raw": "{\"slide_id\": \"Slide49\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T04:46:42.881594+00:00\", \"text_length\": 1208, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s quantify what we’ve been discussing. The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds. But how do we summarize the curve with a single number?\\nThat’s where the area under the ROC curve, or AUC, comes in.\\n\\nIf the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value — you’re essentially flipping a coin. A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1. In practice, no system achieves that ideal, because there’s always some chance of error.\\nSo most real diagnostic tests fall somewhere in between. The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases.\\n\\nThis is why AUC has become such a standard benchmark. It condenses all those trade-offs between sensitivity and specificity into a single number. And just like in teaching or training, performance varies — some students, or some doctors, do exceptionally well; others struggle. The ROC and its AUC make that difference visible in a very clear, quantitative way.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"receiver operating characteristic\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"ROC curve\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"area under the ROC curve\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"AUC\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"diagnostic performance\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"trade-offs between sensitivity and specificity\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"quantitative\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"benchmark\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"performance\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"students\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"doctors\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"exceptional\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"struggle\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"visible\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"quantitative\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"trade-offs\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\":\", \"parsed\": {\"term\": \"trade-offs\", \"category\": \"signal_processing\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide49.json",
        "parsed": {
          "s": "Area under ROC Curve (AUC)",
          "p": "measures",
          "o": "Area under ROC Curve (AUC)",
          "modalities": [
            "text"
          ],
          "confidence": 1.0,
          "evidence": [
            "The area under the ROC curve, or AUC, comes in. If the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value - you're essentially flipping a coin. A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1. In practice, no system achieves that ideal, because there's always some chance of error. So most real diagnostic tests fall somewhere in between. The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases. This is why AUC has become such a standard benchmark. It condenses all those trade-offs between sensitivity and specificity into a single number. And just like in teaching or training, performance varies - some students, or some doctors, do exceptionally well; others struggle. The ROC and its AUC make that difference visible in a very clear, quantitative way."
          ]
        },
        "raw": "{\"slide_id\": \"Slide49\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T05:52:36.794128+00:00\", \"text_length\": 1208, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s quantify what we’ve been discussing. The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds. But how do we summarize the curve with a single number?\\nThat’s where the area under the ROC curve, or AUC, comes in.\\n\\nIf the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value — you’re essentially flipping a coin. A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1. In practice, no system achieves that ideal, because there’s always some chance of error.\\nSo most real diagnostic tests fall somewhere in between. The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases.\\n\\nThis is why AUC has become such a standard benchmark. It condenses all those trade-offs between sensitivity and specificity into a single number. And just like in teaching or training, performance varies — some students, or some doctors, do exceptionally well; others struggle. The ROC and its AUC make that difference visible in a very clear, quantitative way.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Area under ROC Curve (AUC)\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"Area under ROC Curve (AUC)\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"The area under the ROC curve, or AUC, comes in. If the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value — you’re essentially flipping a coin. A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1. In practice, no system achieves that ideal, because there’s always some chance of error. So most real diagnostic tests fall somewhere in between. The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases. This is why AUC has become such a standard benchmark. It condenses all those trade-offs between sensitivity and specificity into a single number. And just like in teaching or training, performance varies — some students, or some doctors, do exceptionally well; others struggle. The ROC and its AUC make that difference visible in a very clear, quantitative way.\\\"}\", \"parsed\": {\"s\": \"Area under ROC Curve (AUC)\", \"p\": \"measures\", \"o\": \"Area under ROC Curve (AUC)\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": [\"The area under the ROC curve, or AUC, comes in. If the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value - you're essentially flipping a coin. A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1. In practice, no system achieves that ideal, because there's always some chance of error. So most real diagnostic tests fall somewhere in between. The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases. This is why AUC has become such a standard benchmark. It condenses all those trade-offs between sensitivity and specificity into a single number. And just like in teaching or training, performance varies - some students, or some doctors, do exceptionally well; others struggle. The ROC and its AUC make that difference visible in a very clear, quantitative way.\"]}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide49.json",
        "parsed": {
          "term": "condenses all those trade-offs between sensitivity and specificity into a single number",
          "category": "quality_metric"
        },
        "raw": "{\"slide_id\": \"Slide49\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:08:05.775719+00:00\", \"text_length\": 1208, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s quantify what we’ve been discussing. The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds. But how do we summarize the curve with a single number?\\nThat’s where the area under the ROC curve, or AUC, comes in.\\n\\nIf the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value — you’re essentially flipping a coin. A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1. In practice, no system achieves that ideal, because there’s always some chance of error.\\nSo most real diagnostic tests fall somewhere in between. The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases.\\n\\nThis is why AUC has become such a standard benchmark. It condenses all those trade-offs between sensitivity and specificity into a single number. And just like in teaching or training, performance varies — some students, or some doctors, do exceptionally well; others struggle. The ROC and its AUC make that difference visible in a very clear, quantitative way.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"receiver operating characteristic, or ROC curve\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"area under the ROC curve, or AUC\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"no predictive value\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"perfect test\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"higher the area under the curve, the better the test\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"condenses all those trade-offs between sensitivity and specificity into a single number\\\",\\n      \\\"category\\\": \\\"quality_metric\\\"\\n    }\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds.\\\",\\n    \\\"That’s where the area under the ROC curve, or AUC, comes in.\\\",\\n    \\\"If the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value — you’re essentially flipping a coin.\\\",\\n    \\\"A perfect test, one that never misses\", \"parsed\": {\"term\": \"condenses all those trade-offs between sensitivity and specificity into a single number\", \"category\": \"quality_metric\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide49.json",
        "parsed": {
          "s": "Area under ROC Curve (AUC)",
          "p": "represents",
          "o": "trade-offs between sensitivity and specificity",
          "modalities": [
            "text",
            "image"
          ],
          "confidence": 0.9,
          "evidence": [
            "It condenses all those trade-offs between sensitivity and specificity into a single number."
          ]
        },
        "raw": "{\"slide_id\": \"Slide49\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:16:36.884207+00:00\", \"text_length\": 1208, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow let’s quantify what we’ve been discussing. The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds. But how do we summarize the curve with a single number?\\nThat’s where the area under the ROC curve, or AUC, comes in.\\n\\nIf the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value — you’re essentially flipping a coin. A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1. In practice, no system achieves that ideal, because there’s always some chance of error.\\nSo most real diagnostic tests fall somewhere in between. The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases.\\n\\nThis is why AUC has become such a standard benchmark. It condenses all those trade-offs between sensitivity and specificity into a single number. And just like in teaching or training, performance varies — some students, or some doctors, do exceptionally well; others struggle. The ROC and its AUC make that difference visible in a very clear, quantitative way.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"Area under ROC Curve (AUC)\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"diagnostic performance\\\",\\n      \\\"modalities\\\": [\\\"text\\\", \\\"image\\\"],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds. But how do we summarize the curve with a single number? That’s where the area under the ROC curve, or AUC, comes in.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"Area under ROC Curve (AUC)\\\",\\n      \\\"p\\\": \\\"represents\\\",\\n      \\\"o\\\": \\\"trade-offs between sensitivity and specificity\\\",\\n      \\\"modalities\\\": [\\\"text\\\", \\\"image\\\"],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"It condenses all those trade-offs between sensitivity and specificity into a single number.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"Area under ROC Curve (AUC)\\\",\\n      \\\"p\\\": \\\"produces\\\",\\n      \\\"o\\\": \\\"single number\\\",\\n      \\\"modalities\\\": [\\\"text\\\", \\\"image\\\"],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"That\", \"parsed\": {\"s\": \"Area under ROC Curve (AUC)\", \"p\": \"represents\", \"o\": \"trade-offs between sensitivity and specificity\", \"modalities\": [\"text\", \"image\"], \"confidence\": 0.9, \"evidence\": [\"It condenses all those trade-offs between sensitivity and specificity into a single number.\"]}}"
      }
    }
  }
}