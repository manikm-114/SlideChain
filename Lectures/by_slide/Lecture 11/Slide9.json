{
  "lecture": "Lecture 11",
  "slide_id": "Slide9",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide9.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide9.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide9.json",
        "parsed": {
          "concepts": [
            {
              "term": "mutual information",
              "category": "communication"
            },
            {
              "term": "KL divergence",
              "category": "mathematics"
            },
            {
              "term": "image registration",
              "category": "reconstruction"
            },
            {
              "term": "uncertainty",
              "category": "mathematics"
            },
            {
              "term": "correlation",
              "category": "signal_processing"
            }
          ],
          "evidence": [
            "Mutual information is a way to measure how much knowing one random variable tells us about another.",
            "Mathematically, mutual information can actually be expressed in terms of the KL divergence.",
            "In practice, this idea is very useful in areas like image registration, where we align two images.",
            "Instead of just matching pixel intensities, we can maximize the mutual information between the two images."
          ]
        },
        "raw": "{\"slide_id\": \"Slide9\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, just for your broader knowledge, let me connect this to another important concept in information theory: mutual information.\\n\\nMutual information is a way to measure how much knowing one random variable tells us about another. For example, suppose we have two variables, X and Y. If they are completely independent, then measuring X tells us nothing about Y. In that case, their mutual information is zero. On the other hand, if X and Y are perfectly dependent — meaning that once you know X, you completely know Y — then their mutual information is very high. Most real-world situations fall somewhere in between.\\n\\nMathematically, mutual information can actually be expressed in terms of the KL divergence. Specifically, it’s the KL divergence between the joint distribution of X and Y, and the product of their marginal distributions. Don’t worry about the details of the formula — the key idea is that it quantifies how much information one variable provides about the other.\\nYou can also think of it this way: when you measure one variable, how much does your uncertainty about the other variable decrease? That decrease in uncertainty is exactly what mutual information captures.\\n\\nIn practice, this idea is very useful in areas like image registration, where we align two images. Instead of just matching pixel intensities, we can maximize the mutual information between the two images. That way, even if the images look very different in terms of brightness or contrast, we can still measure how well they correspond.\\n\\nSo, mutual information is essentially a generalization of correlation, but in the language of information theory. It goes beyond simple linear relationships and captures any kind of statistical dependence.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"mutual information\\\", \\\"category\\\": \\\"communication\\\"},\\n    {\\\"term\\\": \\\"KL divergence\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"image registration\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"uncertainty\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"correlation\\\", \\\"category\\\": \\\"signal_processing\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"Mutual information is a way to measure how much knowing one random variable tells us about another.\\\", \\n               \\\"Mathematically, mutual information can actually be expressed in terms of the KL divergence.\\\", \\n               \\\"In practice, this idea is very useful in areas like image registration, where we align two images.\\\", \\n               \\\"Instead of just matching pixel intensities, we can maximize the mutual information between the two images.\\\"]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"mutual information\", \"category\": \"communication\"}, {\"term\": \"KL divergence\", \"category\": \"mathematics\"}, {\"term\": \"image registration\", \"category\": \"reconstruction\"}, {\"term\": \"uncertainty\", \"category\": \"mathematics\"}, {\"term\": \"correlation\", \"category\": \"signal_processing\"}], \"evidence\": [\"Mutual information is a way to measure how much knowing one random variable tells us about another.\", \"Mathematically, mutual information can actually be expressed in terms of the KL divergence.\", \"In practice, this idea is very useful in areas like image registration, where we align two images.\", \"Instead of just matching pixel intensities, we can maximize the mutual information between the two images.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide9.json",
        "parsed": {
          "triples": [
            {
              "s": "mutual information",
              "p": "measures",
              "o": "information theory",
              "modalities": [
                "text"
              ],
              "confidence": 0.9,
              "evidence": "Mutual information is a way to measure how much knowing one random variable tells us about another."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide9\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, just for your broader knowledge, let me connect this to another important concept in information theory: mutual information.\\n\\nMutual information is a way to measure how much knowing one random variable tells us about another. For example, suppose we have two variables, X and Y. If they are completely independent, then measuring X tells us nothing about Y. In that case, their mutual information is zero. On the other hand, if X and Y are perfectly dependent — meaning that once you know X, you completely know Y — then their mutual information is very high. Most real-world situations fall somewhere in between.\\n\\nMathematically, mutual information can actually be expressed in terms of the KL divergence. Specifically, it’s the KL divergence between the joint distribution of X and Y, and the product of their marginal distributions. Don’t worry about the details of the formula — the key idea is that it quantifies how much information one variable provides about the other.\\nYou can also think of it this way: when you measure one variable, how much does your uncertainty about the other variable decrease? That decrease in uncertainty is exactly what mutual information captures.\\n\\nIn practice, this idea is very useful in areas like image registration, where we align two images. Instead of just matching pixel intensities, we can maximize the mutual information between the two images. That way, even if the images look very different in terms of brightness or contrast, we can still measure how well they correspond.\\n\\nSo, mutual information is essentially a generalization of correlation, but in the language of information theory. It goes beyond simple linear relationships and captures any kind of statistical dependence.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"mutual information\\\", \\\"p\\\":\\\"measures\\\",\\n     \\\"o\\\":\\\"information theory\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.9, \\\"evidence\\\":\\\"Mutual information is a way to measure how much knowing one random variable tells us about another.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"mutual information\", \"p\": \"measures\", \"o\": \"information theory\", \"modalities\": [\"text\"], \"confidence\": 0.9, \"evidence\": \"Mutual information is a way to measure how much knowing one random variable tells us about another.\"}]}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide9.json",
        "parsed": {
          "term": "statistical dependence",
          "category": "mathematics"
        },
        "raw": "{\"slide_id\": \"Slide9\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"mutual information\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"KL divergence\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"joint distribution\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"marginal distributions\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"image registration\\\", \\\"category\\\": \\\"data_processing\\\"},\\n    {\\\"term\\\": \\\"correlation\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"statistical dependence\\\", \\\"category\\\": \\\"mathematics\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"mutual information is a way to measure how much knowing one random variable tells us about another\\\",\\n    \\\"Mathematically, mutual information can actually be expressed in terms of the KL divergence\\\",\\n    \\\"the KL divergence between the joint distribution of X and Y, and the product of their marginal distributions\\\",\\n    \\\"when you measure one variable, how much does your uncertainty about the other variable decrease\\\",\\n    \\\"In practice, this idea is very useful in areas like image registration\\\",\\n    \\\"So, mutual information is essentially a generalization of correlation\\\",\\n    \\\"It goes beyond simple linear relationships and captures any kind of statistical dependence\\\"\\n  ]\", \"parsed\": {\"term\": \"statistical dependence\", \"category\": \"mathematics\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide9.json",
        "parsed": {
          "triples": [
            {
              "s": "mutual information",
              "p": "represents",
              "o": "KL divergence",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "Mathematically, mutual information can actually be expressed in terms of the KL divergence."
            },
            {
              "s": "mutual information",
              "p": "measures",
              "o": "how much knowing one random variable tells us about another",
              "modalities": [
                "text"
              ],
              "confidence": 1.0,
              "evidence": "Mutual information is a way to measure how much knowing one random variable tells us about another."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide9\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"mutual information\\\", \\\"p\\\":\\\"represents\\\", \\\"o\\\":\\\"KL divergence\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Mathematically, mutual information can actually be expressed in terms of the KL divergence.\\\"},\\n    {\\\"s\\\":\\\"mutual information\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"how much knowing one random variable tells us about another\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Mutual information is a way to measure how much knowing one random variable tells us about another.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"mutual information\", \"p\": \"represents\", \"o\": \"KL divergence\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"Mathematically, mutual information can actually be expressed in terms of the KL divergence.\"}, {\"s\": \"mutual information\", \"p\": \"measures\", \"o\": \"how much knowing one random variable tells us about another\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": \"Mutual information is a way to measure how much knowing one random variable tells us about another.\"}]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide9.json",
        "parsed": {
          "concepts": [
            {
              "term": "Mutual information",
              "category": "mathematics"
            },
            {
              "term": "KL divergence",
              "category": "mathematics"
            },
            {
              "term": "Joint distribution",
              "category": "mathematics"
            },
            {
              "term": "Marginal distribution",
              "category": "mathematics"
            },
            {
              "term": "Image registration",
              "category": "algorithm"
            }
          ],
          "evidence": [
            "Mutual information is a way to measure how much knowing one random variable tells us about another.",
            "Mutual information can actually be expressed in terms of the KL divergence.",
            "Mutual information captures how much information one variable provides about the other.",
            "Mutual information is essentially a generalization of correlation, but in the language of information theory.",
            "Mutual information goes beyond simple linear relationships and captures any kind of statistical dependence."
          ]
        },
        "raw": "{\"slide_id\": \"Slide9\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:36:25.914318+00:00\", \"text_length\": 1734, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, just for your broader knowledge, let me connect this to another important concept in information theory: mutual information.\\n\\nMutual information is a way to measure how much knowing one random variable tells us about another. For example, suppose we have two variables, X and Y. If they are completely independent, then measuring X tells us nothing about Y. In that case, their mutual information is zero. On the other hand, if X and Y are perfectly dependent — meaning that once you know X, you completely know Y — then their mutual information is very high. Most real-world situations fall somewhere in between.\\n\\nMathematically, mutual information can actually be expressed in terms of the KL divergence. Specifically, it’s the KL divergence between the joint distribution of X and Y, and the product of their marginal distributions. Don’t worry about the details of the formula — the key idea is that it quantifies how much information one variable provides about the other.\\nYou can also think of it this way: when you measure one variable, how much does your uncertainty about the other variable decrease? That decrease in uncertainty is exactly what mutual information captures.\\n\\nIn practice, this idea is very useful in areas like image registration, where we align two images. Instead of just matching pixel intensities, we can maximize the mutual information between the two images. That way, even if the images look very different in terms of brightness or contrast, we can still measure how well they correspond.\\n\\nSo, mutual information is essentially a generalization of correlation, but in the language of information theory. It goes beyond simple linear relationships and captures any kind of statistical dependence.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"Mutual information\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"KL divergence\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Joint distribution\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Marginal distribution\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Conditional distribution\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Information gain\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Image registration\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Mutual information in image registration\\\", \\\"category\\\": \\\"algorithm\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Mutual information is a way to measure how much knowing one random variable tells us about another.\\\",\\n    \\\"Mutual information can actually be expressed in terms of the KL divergence.\\\",\\n    \\\"Mutual information captures how much information one variable provides about the other.\\\",\\n    \\\"Mutual information is essentially a generalization of correlation, but in the language of information theory.\\\",\\n    \\\"Mutual information goes beyond simple linear relationships and captures any kind of statistical dependence.\\\"\\n  ]\\n}\", \"parsed\": {\"concepts\": [{\"term\": \"Mutual information\", \"category\": \"mathematics\"}, {\"term\": \"KL divergence\", \"category\": \"mathematics\"}, {\"term\": \"Joint distribution\", \"category\": \"mathematics\"}, {\"term\": \"Marginal distribution\", \"category\": \"mathematics\"}, {\"term\": \"Image registration\", \"category\": \"algorithm\"}], \"evidence\": [\"Mutual information is a way to measure how much knowing one random variable tells us about another.\", \"Mutual information can actually be expressed in terms of the KL divergence.\", \"Mutual information captures how much information one variable provides about the other.\", \"Mutual information is essentially a generalization of correlation, but in the language of information theory.\", \"Mutual information goes beyond simple linear relationships and captures any kind of statistical dependence.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide9.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide9\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T05:23:08.471892+00:00\", \"text_length\": 1734, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, just for your broader knowledge, let me connect this to another important concept in information theory: mutual information.\\n\\nMutual information is a way to measure how much knowing one random variable tells us about another. For example, suppose we have two variables, X and Y. If they are completely independent, then measuring X tells us nothing about Y. In that case, their mutual information is zero. On the other hand, if X and Y are perfectly dependent — meaning that once you know X, you completely know Y — then their mutual information is very high. Most real-world situations fall somewhere in between.\\n\\nMathematically, mutual information can actually be expressed in terms of the KL divergence. Specifically, it’s the KL divergence between the joint distribution of X and Y, and the product of their marginal distributions. Don’t worry about the details of the formula — the key idea is that it quantifies how much information one variable provides about the other.\\nYou can also think of it this way: when you measure one variable, how much does your uncertainty about the other variable decrease? That decrease in uncertainty is exactly what mutual information captures.\\n\\nIn practice, this idea is very useful in areas like image registration, where we align two images. Instead of just matching pixel intensities, we can maximize the mutual information between the two images. That way, even if the images look very different in terms of brightness or contrast, we can still measure how well they correspond.\\n\\nSo, mutual information is essentially a generalization of correlation, but in the language of information theory. It goes beyond simple linear relationships and captures any kind of statistical dependence.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Mutual information\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"Mutual information as K-L Distance\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"Mutual information is a way to measure how much knowing one random variable tells us about another.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide9.json",
        "parsed": {
          "term": "correlation",
          "category": "ai_ml"
        },
        "raw": "{\"slide_id\": \"Slide9\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:01:23.046004+00:00\", \"text_length\": 1734, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, just for your broader knowledge, let me connect this to another important concept in information theory: mutual information.\\n\\nMutual information is a way to measure how much knowing one random variable tells us about another. For example, suppose we have two variables, X and Y. If they are completely independent, then measuring X tells us nothing about Y. In that case, their mutual information is zero. On the other hand, if X and Y are perfectly dependent — meaning that once you know X, you completely know Y — then their mutual information is very high. Most real-world situations fall somewhere in between.\\n\\nMathematically, mutual information can actually be expressed in terms of the KL divergence. Specifically, it’s the KL divergence between the joint distribution of X and Y, and the product of their marginal distributions. Don’t worry about the details of the formula — the key idea is that it quantifies how much information one variable provides about the other.\\nYou can also think of it this way: when you measure one variable, how much does your uncertainty about the other variable decrease? That decrease in uncertainty is exactly what mutual information captures.\\n\\nIn practice, this idea is very useful in areas like image registration, where we align two images. Instead of just matching pixel intensities, we can maximize the mutual information between the two images. That way, even if the images look very different in terms of brightness or contrast, we can still measure how well they correspond.\\n\\nSo, mutual information is essentially a generalization of correlation, but in the language of information theory. It goes beyond simple linear relationships and captures any kind of statistical dependence.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"mutual information\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"kl divergence\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"joint distribution\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"marginal distributions\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"conditional distribution\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"uncertainty\\\",\\n      \\\"category\\\": \\\"data_processing\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"image registration\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"statistical dependence\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"correlation\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    }\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Mutual information is a way to measure how much knowing one random variable tells us about another.\\\",\\n    \\\"Mutual information can actually be expressed in terms of the KL divergence.\\\",\\n    \\\"When you measure one variable, how much does your uncertainty about the other variable decrease?\\\",\", \"parsed\": {\"term\": \"correlation\", \"category\": \"ai_ml\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide9.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide9\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:11:27.096409+00:00\", \"text_length\": 1734, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, just for your broader knowledge, let me connect this to another important concept in information theory: mutual information.\\n\\nMutual information is a way to measure how much knowing one random variable tells us about another. For example, suppose we have two variables, X and Y. If they are completely independent, then measuring X tells us nothing about Y. In that case, their mutual information is zero. On the other hand, if X and Y are perfectly dependent — meaning that once you know X, you completely know Y — then their mutual information is very high. Most real-world situations fall somewhere in between.\\n\\nMathematically, mutual information can actually be expressed in terms of the KL divergence. Specifically, it’s the KL divergence between the joint distribution of X and Y, and the product of their marginal distributions. Don’t worry about the details of the formula — the key idea is that it quantifies how much information one variable provides about the other.\\nYou can also think of it this way: when you measure one variable, how much does your uncertainty about the other variable decrease? That decrease in uncertainty is exactly what mutual information captures.\\n\\nIn practice, this idea is very useful in areas like image registration, where we align two images. Instead of just matching pixel intensities, we can maximize the mutual information between the two images. That way, even if the images look very different in terms of brightness or contrast, we can still measure how well they correspond.\\n\\nSo, mutual information is essentially a generalization of correlation, but in the language of information theory. It goes beyond simple linear relationships and captures any kind of statistical dependence.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": []\\n}\", \"parsed\": {\"triples\": []}}"
      }
    }
  }
}