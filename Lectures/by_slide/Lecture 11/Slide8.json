{
  "lecture": "Lecture 11",
  "slide_id": "Slide8",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Images\\Slide8.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Texts\\Slide8.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide8.json",
        "parsed": {
          "concepts": [
            {
              "term": "KL divergence",
              "category": "mathematics"
            },
            {
              "term": "probability distributions",
              "category": "mathematics"
            },
            {
              "term": "Euclidean distance",
              "category": "mathematics"
            },
            {
              "term": "information theory",
              "category": "mathematics"
            },
            {
              "term": "KL divergence is not symmetric",
              "category": "mathematics"
            },
            {
              "term": "KL divergence measures directionality in information",
              "category": "mathematics"
            },
            {
              "term": "machine learning",
              "category": "ai_ml"
            },
            {
              "term": "statistical signal processing",
              "category": "signal_processing"
            }
          ],
          "evidence": [
            "This is where probability theory comes in.",
            "The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.",
            "The KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P.",
            "KL divergence measures directionality in information."
          ]
        },
        "raw": "{\"slide_id\": \"Slide8\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in.\\n\\nSuppose you don’t just have two images, but instead you have two probability distributions — for example, two different histograms of pixel values. The question becomes: how do we measure the difference between these two distributions?\\nOne option is to use Euclidean distance, just as before. But there is a more meaningful way in the context of information theory. This is called the Kullback–Leibler divergence, or KL distance for short.\\n\\nThe formula looks a bit unusual:\\u000bKL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\\nYou don’t need to worry too much about the details — this is beyond the scope of our lecture — but the idea is important. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\\n\\nOne interesting property is that the KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P. That may sound strange, but it has a good analogy. Think of climbing a mountain: going uphill is much harder than going back downhill, even though it’s the same physical path. In the same way, KL divergence measures directionality in information.\\n\\nSo, while we won’t use this directly in our course, it’s good to be aware that such information-based distances exist. They play a big role in areas like machine learning and statistical signal processing.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"KL divergence\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"probability distributions\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Euclidean distance\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"information theory\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"KL divergence is not symmetric\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"KL divergence measures directionality in information\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"machine learning\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"statistical signal processing\\\", \\\"category\\\": \\\"signal_processing\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"This is where probability theory comes in.\\\", \\n               \\\"The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\\\", \\n               \\\"The KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P.\\\", \\n               \\\"KL divergence measures directionality in information.\\\"]\\n}\\n```\", \"parsed\": {\"concepts\": [{\"term\": \"KL divergence\", \"category\": \"mathematics\"}, {\"term\": \"probability distributions\", \"category\": \"mathematics\"}, {\"term\": \"Euclidean distance\", \"category\": \"mathematics\"}, {\"term\": \"information theory\", \"category\": \"mathematics\"}, {\"term\": \"KL divergence is not symmetric\", \"category\": \"mathematics\"}, {\"term\": \"KL divergence measures directionality in information\", \"category\": \"mathematics\"}, {\"term\": \"machine learning\", \"category\": \"ai_ml\"}, {\"term\": \"statistical signal processing\", \"category\": \"signal_processing\"}], \"evidence\": [\"This is where probability theory comes in.\", \"The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\", \"The KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P.\", \"KL divergence measures directionality in information.\"]}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide8.json",
        "parsed": {
          "triples": [
            {
              "s": "KL divergence",
              "p": "measures",
              "o": "probability distributions",
              "modalities": [
                "text"
              ],
              "confidence": 0.0,
              "evidence": "KL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide8\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in.\\n\\nSuppose you don’t just have two images, but instead you have two probability distributions — for example, two different histograms of pixel values. The question becomes: how do we measure the difference between these two distributions?\\nOne option is to use Euclidean distance, just as before. But there is a more meaningful way in the context of information theory. This is called the Kullback–Leibler divergence, or KL distance for short.\\n\\nThe formula looks a bit unusual:\\u000bKL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\\nYou don’t need to worry too much about the details — this is beyond the scope of our lecture — but the idea is important. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\\n\\nOne interesting property is that the KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P. That may sound strange, but it has a good analogy. Think of climbing a mountain: going uphill is much harder than going back downhill, even though it’s the same physical path. In the same way, KL divergence measures directionality in information.\\n\\nSo, while we won’t use this directly in our course, it’s good to be aware that such information-based distances exist. They play a big role in areas like machine learning and statistical signal processing.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"KL divergence\\\", \\\"p\\\":\\\"measures\\\",\\n     \\\"o\\\":\\\"probability distributions\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"KL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"KL divergence\", \"p\": \"measures\", \"o\": \"probability distributions\", \"modalities\": [\"text\"], \"confidence\": 0.0, \"evidence\": \"KL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\"}]}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide8.json",
        "parsed": {
          "term": "statistical signal processing",
          "category": "signal_processing"
        },
        "raw": "{\"slide_id\": \"Slide8\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"information divergence\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Kullback-Leibler divergence\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"KL distance\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"probability distributions\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Euclidean distance\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"KL divergence\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"symmetric\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"directionality\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"machine learning\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"statistical signal processing\\\", \\\"category\\\": \\\"signal_processing\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"This is where probability theory comes in.\\\",\\n    \\\"The formula looks a bit unusual: KL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\\\",\\n    \\\"The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\\\",\\n    \\\"One interesting property is that the\", \"parsed\": {\"term\": \"statistical signal processing\", \"category\": \"signal_processing\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide8.json",
        "parsed": {
          "s": "KL divergence",
          "p": "is",
          "o": "always greater than or equal to zero",
          "modalities": [
            "text"
          ],
          "confidence": 1.0,
          "evidence": [
            "The KL divergence is always greater than or equal to zero"
          ]
        },
        "raw": "{\"slide_id\": \"Slide8\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"information divergence\\\",\\n      \\\"p\\\": \\\"represents\\\",\\n      \\\"o\\\": \\\"Kullback-Leibler divergence\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"This is where probability theory comes in.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"Kullback-Leibler divergence\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"difference between two probability distributions\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"how do we measure the difference between these two distributions?\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"KL divergence\\\",\\n      \\\"p\\\": \\\"is\\\",\\n      \\\"o\\\": \\\"always greater than or equal to zero\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"The KL divergence is always greater than or equal to zero\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"KL divergence\\\",\\n      \\\"p\\\": \\\"becomes\\\",\\n      \\\"o\\\": \\\"exactly zero\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\", \"parsed\": {\"s\": \"KL divergence\", \"p\": \"is\", \"o\": \"always greater than or equal to zero\", \"modalities\": [\"text\"], \"confidence\": 1.0, \"evidence\": [\"The KL divergence is always greater than or equal to zero\"]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide8.json",
        "parsed": {
          "term": "statistical signal processing",
          "category": "signal_processing"
        },
        "raw": "{\"slide_id\": \"Slide8\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:34:51.445847+00:00\", \"text_length\": 1577, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in.\\n\\nSuppose you don’t just have two images, but instead you have two probability distributions — for example, two different histograms of pixel values. The question becomes: how do we measure the difference between these two distributions?\\nOne option is to use Euclidean distance, just as before. But there is a more meaningful way in the context of information theory. This is called the Kullback–Leibler divergence, or KL distance for short.\\n\\nThe formula looks a bit unusual:\\u000bKL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\\nYou don’t need to worry too much about the details — this is beyond the scope of our lecture — but the idea is important. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\\n\\nOne interesting property is that the KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P. That may sound strange, but it has a good analogy. Think of climbing a mountain: going uphill is much harder than going back downhill, even though it’s the same physical path. In the same way, KL divergence measures directionality in information.\\n\\nSo, while we won’t use this directly in our course, it’s good to be aware that such information-based distances exist. They play a big role in areas like machine learning and statistical signal processing.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"information divergence\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Kullback-Leibler distance\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Euclidean distance\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"probability theory\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"histograms\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"pixel values\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"KL divergence\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"symmetry\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"information-based distances\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"machine learning\\\", \\\"category\\\": \\\"ai_ml\\\"},\\n    {\\\"term\\\": \\\"statistical signal processing\\\", \\\"category\\\": \\\"signal_processing\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Another type of distance, called information divergence.\\\",\\n    \\\"The question becomes: how do we measure the difference between these two distributions?\\\",\\n    \\\"One option is to use Euclidean distance, just as before.\\\",\\n    \\\"But there is a more meaningful way in the context of information theory. This is called the Kullback–\", \"parsed\": {\"term\": \"statistical signal processing\", \"category\": \"signal_processing\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide8.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide8\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T05:22:21.746338+00:00\", \"text_length\": 1577, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in.\\n\\nSuppose you don’t just have two images, but instead you have two probability distributions — for example, two different histograms of pixel values. The question becomes: how do we measure the difference between these two distributions?\\nOne option is to use Euclidean distance, just as before. But there is a more meaningful way in the context of information theory. This is called the Kullback–Leibler divergence, or KL distance for short.\\n\\nThe formula looks a bit unusual:\\u000bKL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\\nYou don’t need to worry too much about the details — this is beyond the scope of our lecture — but the idea is important. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\\n\\nOne interesting property is that the KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P. That may sound strange, but it has a good analogy. Think of climbing a mountain: going uphill is much harder than going back downhill, even though it’s the same physical path. In the same way, KL divergence measures directionality in information.\\n\\nSo, while we won’t use this directly in our course, it’s good to be aware that such information-based distances exist. They play a big role in areas like machine learning and statistical signal processing.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Kullback-Leibler Distance\\\", \\\"p\\\":\\\"measures\\\", \\\"o\\\":\\\"Information Divergence\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":1.0, \\\"evidence\\\":\\\"The formula looks a bit unusual: KL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide8.json",
        "parsed": {
          "term": "statistical signal processing",
          "category": "signal_processing"
        },
        "raw": "{\"slide_id\": \"Slide8\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T03:01:12.701637+00:00\", \"text_length\": 1577, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in.\\n\\nSuppose you don’t just have two images, but instead you have two probability distributions — for example, two different histograms of pixel values. The question becomes: how do we measure the difference between these two distributions?\\nOne option is to use Euclidean distance, just as before. But there is a more meaningful way in the context of information theory. This is called the Kullback–Leibler divergence, or KL distance for short.\\n\\nThe formula looks a bit unusual:\\u000bKL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\\nYou don’t need to worry too much about the details — this is beyond the scope of our lecture — but the idea is important. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\\n\\nOne interesting property is that the KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P. That may sound strange, but it has a good analogy. Think of climbing a mountain: going uphill is much harder than going back downhill, even though it’s the same physical path. In the same way, KL divergence measures directionality in information.\\n\\nSo, while we won’t use this directly in our course, it’s good to be aware that such information-based distances exist. They play a big role in areas like machine learning and statistical signal processing.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"information divergence\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"Kullback-Leibler Distance\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"D[p(x):q(x)] = sum p(x) log p(x)/q(x)\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"D[p(x):q(x)] >= 0 iff p(x) = q(x)\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"D[p:q] != D[q:p]\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"probability distributions\\\",\\n      \\\"category\\\": \\\"data_processing\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"histograms of pixel values\\\",\\n      \\\"category\\\": \\\"data_processing\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"machine learning\\\",\\n      \\\"category\\\": \\\"ai_ml\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"statistical signal processing\\\",\\n      \\\"category\\\": \\\"signal_processing\\\"\\n    }\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Now, let me briefly mention another type of distance,\", \"parsed\": {\"term\": \"statistical signal processing\", \"category\": \"signal_processing\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 11\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide8.json",
        "parsed": {
          "s": "Kullback-Leibler Distance",
          "p": "measures",
          "o": "information divergence",
          "modalities": [
            "text"
          ],
          "confidence": 0.95,
          "evidence": [
            "Now, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in."
          ]
        },
        "raw": "{\"slide_id\": \"Slide8\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T03:11:26.452017+00:00\", \"text_length\": 1577, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nNow, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in.\\n\\nSuppose you don’t just have two images, but instead you have two probability distributions — for example, two different histograms of pixel values. The question becomes: how do we measure the difference between these two distributions?\\nOne option is to use Euclidean distance, just as before. But there is a more meaningful way in the context of information theory. This is called the Kullback–Leibler divergence, or KL distance for short.\\n\\nThe formula looks a bit unusual:\\u000bKL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\\nYou don’t need to worry too much about the details — this is beyond the scope of our lecture — but the idea is important. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\\n\\nOne interesting property is that the KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P. That may sound strange, but it has a good analogy. Think of climbing a mountain: going uphill is much harder than going back downhill, even though it’s the same physical path. In the same way, KL divergence measures directionality in information.\\n\\nSo, while we won’t use this directly in our course, it’s good to be aware that such information-based distances exist. They play a big role in areas like machine learning and statistical signal processing.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"Kullback-Leibler Distance\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"difference between two probability distributions\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"The question becomes: how do we measure the difference between these two distributions?\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"Kullback-Leibler Distance\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"information divergence\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"Now, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"Kullback-Leibler Distance\\\",\\n      \\\"p\\\": \\\"measures\\\",\\n      \\\"o\\\": \\\"difference between two probability distributions\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"Suppose you don’t just have two images, but instead you have two probability distributions — for example, two different histograms\", \"parsed\": {\"s\": \"Kullback-Leibler Distance\", \"p\": \"measures\", \"o\": \"information divergence\", \"modalities\": [\"text\"], \"confidence\": 0.95, \"evidence\": [\"Now, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in.\"]}}"
      }
    }
  }
}