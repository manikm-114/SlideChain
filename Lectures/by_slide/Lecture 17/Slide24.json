{
  "lecture": "Lecture 17",
  "slide_id": "Slide24",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 17\\Images\\Slide24.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 17\\Texts\\Slide24.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 17\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide24.json",
        "parsed": {
          "term": "improved quality",
          "category": "reconstruction"
        },
        "raw": "{\"slide_id\": \"Slide24\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nThe IMIL approach is often used because it naturally incorporates statistical knowledge. To explain this more simply, let us recall the idea of CT reconstruction. Imagine you have multiple views of an object. One view looks like this, another view looks different. Now, you try to reconstruct the underlying image. Suppose the underlying image is a star. When you trace the star vertically, the profile does not look right. When you trace it horizontally, again, it does not match. Then, you try using a different model—say, an ellipse. If you trace the ellipse horizontally, you obtain a profile that matches the measured data more closely. This suggests that the elliptical model fits best. In this case, you would report the ellipse as the reconstructed image.\\n\\nThis is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image. This approach is fundamentally different from deterministic CT reconstruction methods. Instead of assuming the data is exact, it operates within a statistical framework.\\nIn this framework, measurements are modeled with a Poisson distribution. Because of the independence of measurements, the overall likelihood can be expressed as a product of individual probabilities. By taking the natural logarithm, this product is converted into a summation, making the formulation simpler and more practical to compute. The key point is that the statistical model accounts for the Poisson noise inherent in the data, which deterministic methods ignore.\\n\\nTo see the advantage, consider a phantom experiment. If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise—that every measurement is an exact line integral. But in reality, measurements always have statistical fluctuations. This mismatch leads to streaking artifacts and random noise in the reconstructed images. By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and improved quality, as demonstrated in the phantom example.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"maximum likelihood estimation\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"Poisson distribution\\\", \\\"category\\\": \\\"statistics\\\"},\\n    {\\\"term\\\": \\\"streaking artifacts\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"random noise\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"phantom experiment\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"filtered back projection\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"exact line integral\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"statistical fluctuations\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"statistical model\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"improved quality\\\", \\\"category\\\": \\\"reconstruction\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"This is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image.\\\", \\\"By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and\", \"parsed\": {\"term\": \"improved quality\", \"category\": \"reconstruction\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 17\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide24.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide24\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nThe IMIL approach is often used because it naturally incorporates statistical knowledge. To explain this more simply, let us recall the idea of CT reconstruction. Imagine you have multiple views of an object. One view looks like this, another view looks different. Now, you try to reconstruct the underlying image. Suppose the underlying image is a star. When you trace the star vertically, the profile does not look right. When you trace it horizontally, again, it does not match. Then, you try using a different model—say, an ellipse. If you trace the ellipse horizontally, you obtain a profile that matches the measured data more closely. This suggests that the elliptical model fits best. In this case, you would report the ellipse as the reconstructed image.\\n\\nThis is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image. This approach is fundamentally different from deterministic CT reconstruction methods. Instead of assuming the data is exact, it operates within a statistical framework.\\nIn this framework, measurements are modeled with a Poisson distribution. Because of the independence of measurements, the overall likelihood can be expressed as a product of individual probabilities. By taking the natural logarithm, this product is converted into a summation, making the formulation simpler and more practical to compute. The key point is that the statistical model accounts for the Poisson noise inherent in the data, which deterministic methods ignore.\\n\\nTo see the advantage, consider a phantom experiment. If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise—that every measurement is an exact line integral. But in reality, measurements always have statistical fluctuations. This mismatch leads to streaking artifacts and random noise in the reconstructed images. By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and improved quality, as demonstrated in the phantom example.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"Maximum likelihood estimation\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"CT reconstruction\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"This is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 17\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide24.json",
        "parsed": {
          "term": "reconstructed images",
          "category": "reconstruction"
        },
        "raw": "{\"slide_id\": \"Slide24\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"IMIL approach\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"CT reconstruction\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"maximum likelihood estimation\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"filtered back projection\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"Poisson distribution\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"natural logarithm\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"streaking artifacts\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"phantom experiment\\\", \\\"category\\\": \\\"reconstruction\\\"},\\n    {\\\"term\\\": \\\"radioactive tracers\\\", \\\"category\\\": \\\"physics\\\"},\\n    {\\\"term\\\": \\\"statistical fluctuations\\\", \\\"category\\\": \\\"data_processing\\\"},\\n    {\\\"term\\\": \\\"reconstructed images\\\", \\\"category\\\": \\\"reconstruction\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"To explain this more simply, let us recall the idea of CT reconstruction.\\\",\\n    \\\"maximum likelihood estimation does: you keep trying different underlying images...\\\",\\n    \\\"filtered back projection, you are assuming no noise...\\\",\\n    \\\"measurements are modeled with a Poisson distribution...\\\",\\n    \\\"taking the natural logarithm...\\\",\", \"parsed\": {\"term\": \"reconstructed images\", \"category\": \"reconstruction\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 17\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide24.json",
        "parsed": {
          "s": "filtered back projection",
          "p": "reconstructs_with",
          "o": "phantom experiment",
          "modalities": [
            "text",
            "image"
          ],
          "confidence": 1.0,
          "evidence": [
            "If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection..."
          ]
        },
        "raw": "{\"slide_id\": \"Slide24\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"IMIL approach\\\",\\n      \\\"p\\\": \\\"uses\\\",\\n      \\\"o\\\": \\\"statistical knowledge\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"The IMIL approach is often used because it naturally incorporates statistical knowledge.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"maximum likelihood estimation\\\",\\n      \\\"p\\\": \\\"reconstructs_with\\\",\\n      \\\"o\\\": \\\"Poisson distribution\\\",\\n      \\\"modalities\\\": [\\\"text\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"measurements are modeled with a Poisson distribution.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"filtered back projection\\\",\\n      \\\"p\\\": \\\"reconstructs_with\\\",\\n      \\\"o\\\": \\\"phantom experiment\\\",\\n      \\\"modalities\\\": [\\\"text\\\", \\\"image\\\"],\\n      \\\"confidence\\\": 1.0,\\n      \\\"evidence\\\": \\\"If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection...\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"maximum likelihood reconstruction\\\",\\n      \\\"p\\\": \\\"reconstructs_with\\\",\\n      \\\"o\\\": \\\"phantom experiment\\\",\\n      \\\"modalities\\\": [\\\"text\\\", \\\"image\\\"],\\n      \\\"\", \"parsed\": {\"s\": \"filtered back projection\", \"p\": \"reconstructs_with\", \"o\": \"phantom experiment\", \"modalities\": [\"text\", \"image\"], \"confidence\": 1.0, \"evidence\": [\"If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection...\"]}}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 17\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide24.json",
        "parsed": {
          "term": "reconstruction",
          "category": "reconstruction"
        },
        "raw": "{\"slide_id\": \"Slide24\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-09T11:09:25.322060+00:00\", \"text_length\": 2336, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nThe IMIL approach is often used because it naturally incorporates statistical knowledge. To explain this more simply, let us recall the idea of CT reconstruction. Imagine you have multiple views of an object. One view looks like this, another view looks different. Now, you try to reconstruct the underlying image. Suppose the underlying image is a star. When you trace the star vertically, the profile does not look right. When you trace it horizontally, again, it does not match. Then, you try using a different model—say, an ellipse. If you trace the ellipse horizontally, you obtain a profile that matches the measured data more closely. This suggests that the elliptical model fits best. In this case, you would report the ellipse as the reconstructed image.\\n\\nThis is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image. This approach is fundamentally different from deterministic CT reconstruction methods. Instead of assuming the data is exact, it operates within a statistical framework.\\nIn this framework, measurements are modeled with a Poisson distribution. Because of the independence of measurements, the overall likelihood can be expressed as a product of individual probabilities. By taking the natural logarithm, this product is converted into a summation, making the formulation simpler and more practical to compute. The key point is that the statistical model accounts for the Poisson noise inherent in the data, which deterministic methods ignore.\\n\\nTo see the advantage, consider a phantom experiment. If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise—that every measurement is an exact line integral. But in reality, measurements always have statistical fluctuations. This mismatch leads to streaking artifacts and random noise in the reconstructed images. By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and improved quality, as demonstrated in the phantom example.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"maximum likelihood estimation\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"Poisson distribution\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"likelihood\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"statistical model\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"phantom experiment\\\", \\\"category\\\": \\\"instrumentation\\\"},\\n    {\\\"term\\\": \\\"filtered back projection\\\", \\\"category\\\": \\\"algorithm\\\"},\\n    {\\\"term\\\": \\\"streaking artifacts\\\", \\\"category\\\": \\\"quality_metric\\\"},\\n    {\\\"term\\\": \\\"reconstruction\\\", \\\"category\\\": \\\"reconstruction\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"The IMIL approach is often used because it naturally incorporates statistical knowledge.\\\",\\n    \\\"This is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data.\\\",\\n    \\\"In this framework, measurements are modeled with a Poisson distribution.\\\",\\n    \\\"The key point is that the statistical model accounts for the Poisson noise inherent in the data, which deterministic methods ignore.\\\",\\n    \\\"By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model.\\\",\\n    \\\"As a result, the reconstructed images show fewer\", \"parsed\": {\"term\": \"reconstruction\", \"category\": \"reconstruction\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 17\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide24.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide24\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-09T12:18:27.355768+00:00\", \"text_length\": 2336, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nThe IMIL approach is often used because it naturally incorporates statistical knowledge. To explain this more simply, let us recall the idea of CT reconstruction. Imagine you have multiple views of an object. One view looks like this, another view looks different. Now, you try to reconstruct the underlying image. Suppose the underlying image is a star. When you trace the star vertically, the profile does not look right. When you trace it horizontally, again, it does not match. Then, you try using a different model—say, an ellipse. If you trace the ellipse horizontally, you obtain a profile that matches the measured data more closely. This suggests that the elliptical model fits best. In this case, you would report the ellipse as the reconstructed image.\\n\\nThis is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image. This approach is fundamentally different from deterministic CT reconstruction methods. Instead of assuming the data is exact, it operates within a statistical framework.\\nIn this framework, measurements are modeled with a Poisson distribution. Because of the independence of measurements, the overall likelihood can be expressed as a product of individual probabilities. By taking the natural logarithm, this product is converted into a summation, making the formulation simpler and more practical to compute. The key point is that the statistical model accounts for the Poisson noise inherent in the data, which deterministic methods ignore.\\n\\nTo see the advantage, consider a phantom experiment. If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise—that every measurement is an exact line integral. But in reality, measurements always have statistical fluctuations. This mismatch leads to streaking artifacts and random noise in the reconstructed images. By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and improved quality, as demonstrated in the phantom example.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"phantom reconstruction\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"filtered back projection\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"phantom reconstruction\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 17\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide24.json",
        "parsed": {
          "term": "filtered back projection",
          "category": "reconstruction"
        },
        "raw": "{\"slide_id\": \"Slide24\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-12T19:10:39.770540+00:00\", \"text_length\": 2336, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nThe IMIL approach is often used because it naturally incorporates statistical knowledge. To explain this more simply, let us recall the idea of CT reconstruction. Imagine you have multiple views of an object. One view looks like this, another view looks different. Now, you try to reconstruct the underlying image. Suppose the underlying image is a star. When you trace the star vertically, the profile does not look right. When you trace it horizontally, again, it does not match. Then, you try using a different model—say, an ellipse. If you trace the ellipse horizontally, you obtain a profile that matches the measured data more closely. This suggests that the elliptical model fits best. In this case, you would report the ellipse as the reconstructed image.\\n\\nThis is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image. This approach is fundamentally different from deterministic CT reconstruction methods. Instead of assuming the data is exact, it operates within a statistical framework.\\nIn this framework, measurements are modeled with a Poisson distribution. Because of the independence of measurements, the overall likelihood can be expressed as a product of individual probabilities. By taking the natural logarithm, this product is converted into a summation, making the formulation simpler and more practical to compute. The key point is that the statistical model accounts for the Poisson noise inherent in the data, which deterministic methods ignore.\\n\\nTo see the advantage, consider a phantom experiment. If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise—that every measurement is an exact line integral. But in reality, measurements always have statistical fluctuations. This mismatch leads to streaking artifacts and random noise in the reconstructed images. By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and improved quality, as demonstrated in the phantom example.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"the IMIL approach is often used because it naturally incorporates statistical knowledge\\\",\\n      \\\"category\\\": \\\"algorithm\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"CT reconstruction\\\",\\n      \\\"category\\\": \\\"reconstruction\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"maximum likelihood estimation\\\",\\n      \\\"category\\\": \\\"algorithm\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"deterministic CT reconstruction methods\\\",\\n      \\\"category\\\": \\\"algorithm\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"measurements are modeled with a Poisson distribution\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"the overall likelihood can be expressed as a product of individual probabilities\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"By taking the natural logarithm, this product is converted into a summation\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model\\\",\\n      \\\"category\\\": \\\"algorithm\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"filtered back projection\\\",\\n      \\\"category\\\": \\\"reconstruction\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"iterative reconstruction\\\",\\n      \\\"category\\\": \\\"\", \"parsed\": {\"term\": \"filtered back projection\", \"category\": \"reconstruction\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 17\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide24.json",
        "parsed": {
          "triples": [
            {
              "s": "phantom",
              "p": "reconstructs_with",
              "o": "filtered back projection",
              "modalities": [
                "image"
              ],
              "confidence": 0.9,
              "evidence": "If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise-that every measurement is an exact line integral."
            }
          ]
        },
        "raw": "{\"slide_id\": \"Slide24\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-12T19:18:01.014135+00:00\", \"text_length\": 2336, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nThe IMIL approach is often used because it naturally incorporates statistical knowledge. To explain this more simply, let us recall the idea of CT reconstruction. Imagine you have multiple views of an object. One view looks like this, another view looks different. Now, you try to reconstruct the underlying image. Suppose the underlying image is a star. When you trace the star vertically, the profile does not look right. When you trace it horizontally, again, it does not match. Then, you try using a different model—say, an ellipse. If you trace the ellipse horizontally, you obtain a profile that matches the measured data more closely. This suggests that the elliptical model fits best. In this case, you would report the ellipse as the reconstructed image.\\n\\nThis is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image. This approach is fundamentally different from deterministic CT reconstruction methods. Instead of assuming the data is exact, it operates within a statistical framework.\\nIn this framework, measurements are modeled with a Poisson distribution. Because of the independence of measurements, the overall likelihood can be expressed as a product of individual probabilities. By taking the natural logarithm, this product is converted into a summation, making the formulation simpler and more practical to compute. The key point is that the statistical model accounts for the Poisson noise inherent in the data, which deterministic methods ignore.\\n\\nTo see the advantage, consider a phantom experiment. If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise—that every measurement is an exact line integral. But in reality, measurements always have statistical fluctuations. This mismatch leads to streaking artifacts and random noise in the reconstructed images. By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and improved quality, as demonstrated in the phantom example.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"phantom\\\",\\n      \\\"p\\\": \\\"reconstructs_with\\\",\\n      \\\"o\\\": \\\"filtered back projection\\\",\\n      \\\"modalities\\\": [\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise—that every measurement is an exact line integral.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"phantom\\\",\\n      \\\"p\\\": \\\"reconstructs_with\\\",\\n      \\\"o\\\": \\\"iterative reconstruction\\\",\\n      \\\"modalities\\\": [\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.9,\\n      \\\"evidence\\\": \\\"By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and improved quality, as demonstrated in the phantom example.\\\"\\n    }\\n  ]\\n}\", \"parsed\": {\"triples\": [{\"s\": \"phantom\", \"p\": \"reconstructs_with\", \"o\": \"filtered back projection\", \"modalities\": [\"image\"], \"confidence\": 0.9, \"evidence\": \"If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise-that every measurement is an exact line integral.\"}]}}"
      }
    }
  }
}