{
  "lecture": "Lecture 5",
  "slide_id": "Slide11",
  "paths": {
    "image": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Images\\Slide11.JPG",
    "text": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Texts\\Slide11.txt"
  },
  "models": {
    "llava-hf__llava-onevision-qwen2-7b-ov-hf": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\concepts\\Slide11.json",
        "parsed": {
          "term": "machine learning",
          "category": "ai_ml"
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"concepts\", \"raw_output\": \"user \\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nPreviously, we saw how a vector in three-dimensional space can be represented using its projections onto the x, y, and z axes. Now, let’s extend this same idea to n-dimensional space — using orthogonal basis vectors to represent any arbitrary vector.\\n\\u000bLet’s quickly review the 3D case first.\\u000bWe have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1.\\u000bAny 3D vector — let’s say vector v — with components x, y, and z, can be written as a weighted sum of these basis vectors.\\u000bSo, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z.\\u000bEach of these terms represents the projection of vector v onto one of the coordinate directions. Because our basis vectors are orthogonal and of unit length, we don’t need to normalize — the inner product directly gives us the component in that direction.\\n\\nNow let’s generalize to n dimensions.\\u000bSuppose we have a vector — we’ll call it vector W — that lives in n-dimensional space. We can express this vector as a linear combination of n orthonormal basis vectors, denoted as e n, where n runs from 1 to capital N.\\u000bThe formula looks like this: vector W equals the sum from n equals 1 to capital N of W dot e n times e n.\\n\\nIn words, for each basis direction e n, we project W onto that direction by computing the inner product W dot e n. That gives us the scalar coefficient for that direction. Then we scale e n by that coefficient and sum everything together.\\u000b\\nThis is a very powerful idea: you can express any vector — no matter how high the dimension — as a sum of projections onto orthonormal basis vectors.\\u000bAnd what’s even more powerful is that those basis vectors don’t have to be just the standard coordinate axes.\\u000bThey could be sine and cosine functions, as we use in Fourier series. Or they could be eigenvectors in principal component analysis. Or wavelets, or anything else — as long as they are orthonormal.\\u000bSo what we’ve described on this slide is the general framework of orthogonal representation in n-dimensional space. It’s the core mathematical idea that underpins many tools in signal processing, data analysis, and machine learning.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Category guidance:\\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\\n  * physics → energy, radiation, waves, gradients, attenuation\\n  * instrumentation → scanners, detectors, coils, transducers, gantry\\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\\n  * communication → network or data transfer\\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\\n  * algorithm → analytical/iterative computational methods\\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\nassistant\\n```json\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"vector\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"basis vectors\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"orthonormal\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"projection\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"weighted sum\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"inner product\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"n-dimensional space\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"linear combination\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"orthonormal basis vectors\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Fourier series\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"principal component analysis\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"wavelets\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"signal processing\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"data analysis\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"machine learning\\\", \\\"category\\\": \\\"ai_ml\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"Let’s quickly review the 3D\", \"parsed\": {\"term\": \"machine learning\", \"category\": \"ai_ml\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Outputs\\llava-hf__llava-onevision-qwen2-7b-ov-hf\\triples\\Slide11.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", \"prompt\": \"triples\", \"raw_output\": \"user \\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nPreviously, we saw how a vector in three-dimensional space can be represented using its projections onto the x, y, and z axes. Now, let’s extend this same idea to n-dimensional space — using orthogonal basis vectors to represent any arbitrary vector.\\n\\u000bLet’s quickly review the 3D case first.\\u000bWe have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1.\\u000bAny 3D vector — let’s say vector v — with components x, y, and z, can be written as a weighted sum of these basis vectors.\\u000bSo, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z.\\u000bEach of these terms represents the projection of vector v onto one of the coordinate directions. Because our basis vectors are orthogonal and of unit length, we don’t need to normalize — the inner product directly gives us the component in that direction.\\n\\nNow let’s generalize to n dimensions.\\u000bSuppose we have a vector — we’ll call it vector W — that lives in n-dimensional space. We can express this vector as a linear combination of n orthonormal basis vectors, denoted as e n, where n runs from 1 to capital N.\\u000bThe formula looks like this: vector W equals the sum from n equals 1 to capital N of W dot e n times e n.\\n\\nIn words, for each basis direction e n, we project W onto that direction by computing the inner product W dot e n. That gives us the scalar coefficient for that direction. Then we scale e n by that coefficient and sum everything together.\\u000b\\nThis is a very powerful idea: you can express any vector — no matter how high the dimension — as a sum of projections onto orthonormal basis vectors.\\u000bAnd what’s even more powerful is that those basis vectors don’t have to be just the standard coordinate axes.\\u000bThey could be sine and cosine functions, as we use in Fourier series. Or they could be eigenvectors in principal component analysis. Or wavelets, or anything else — as long as they are orthonormal.\\u000bSo what we’ve described on this slide is the general framework of orthogonal representation in n-dimensional space. It’s the core mathematical idea that underpins many tools in signal processing, data analysis, and machine learning.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if the relation is clearly visible without the text.\\n- confidence in [0,1]. Do not invent entities or relations.\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"vector v\\\", \\\"p\\\":\\\"represents\\\", \\\"o\\\":\\\"3D vector with components x, y, and z\\\", \\\"modalities\\\":[\\\"text\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"Any 3D vector — let’s say vector v — with components x, y, and z, can be written as a weighted sum of these basis vectors.\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "OpenGVLab__InternVL3-14B": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Outputs\\OpenGVLab__InternVL3-14B\\concepts\\Slide11.json",
        "parsed": null,
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"concepts\", \"raw_output\": \"\", \"parsed\": null}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Outputs\\OpenGVLab__InternVL3-14B\\triples\\Slide11.json",
        "parsed": null,
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"OpenGVLab/InternVL3-14B\", \"prompt\": \"triples\", \"raw_output\": \"\", \"parsed\": null}"
      }
    },
    "Qwen__Qwen2-VL-7B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\concepts\\Slide11.json",
        "parsed": {
          "term": "orthogonal representation",
          "category": "mathematics"
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-08T21:44:26.011511+00:00\", \"text_length\": 2236, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nPreviously, we saw how a vector in three-dimensional space can be represented using its projections onto the x, y, and z axes. Now, let’s extend this same idea to n-dimensional space — using orthogonal basis vectors to represent any arbitrary vector.\\n\\u000bLet’s quickly review the 3D case first.\\u000bWe have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1.\\u000bAny 3D vector — let’s say vector v — with components x, y, and z, can be written as a weighted sum of these basis vectors.\\u000bSo, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z.\\u000bEach of these terms represents the projection of vector v onto one of the coordinate directions. Because our basis vectors are orthogonal and of unit length, we don’t need to normalize — the inner product directly gives us the component in that direction.\\n\\nNow let’s generalize to n dimensions.\\u000bSuppose we have a vector — we’ll call it vector W — that lives in n-dimensional space. We can express this vector as a linear combination of n orthonormal basis vectors, denoted as e n, where n runs from 1 to capital N.\\u000bThe formula looks like this: vector W equals the sum from n equals 1 to capital N of W dot e n times e n.\\n\\nIn words, for each basis direction e n, we project W onto that direction by computing the inner product W dot e n. That gives us the scalar coefficient for that direction. Then we scale e n by that coefficient and sum everything together.\\u000b\\nThis is a very powerful idea: you can express any vector — no matter how high the dimension — as a sum of projections onto orthonormal basis vectors.\\u000bAnd what’s even more powerful is that those basis vectors don’t have to be just the standard coordinate axes.\\u000bThey could be sine and cosine functions, as we use in Fourier series. Or they could be eigenvectors in principal component analysis. Or wavelets, or anything else — as long as they are orthonormal.\\u000bSo what we’ve described on this slide is the general framework of orthogonal representation in n-dimensional space. It’s the core mathematical idea that underpins many tools in signal processing, data analysis, and machine learning.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"ND Orthogonal Representation\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"vector\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"orthogonal basis vectors\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"projection\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"inner product\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"linear combination\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"orthonormal basis\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"Fourier series\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"principal component analysis\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"wavelets\\\", \\\"category\\\": \\\"mathematics\\\"},\\n    {\\\"term\\\": \\\"signal processing\\\", \\\"category\\\": \\\"signal_processing\\\"},\\n    {\\\"term\\\": \\\"data analysis\\\", \\\"category\\\": \\\"data_processing\\\"},\\n    {\\\"term\\\": \\\"machine learning\\\", \\\"category\\\": \\\"data_processing\\\"},\\n    {\\\"term\\\": \\\"orthogonal representation\\\", \\\"category\\\": \\\"mathematics\\\"}\\n  ],\\n  \\\"evidence\\\": [\\n    \\\"Let’s quickly review the 3D case first.\\\",\\n    \\\"Any 3D vector — let’s say\", \"parsed\": {\"term\": \"orthogonal representation\", \"category\": \"mathematics\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Outputs\\Qwen__Qwen2-VL-7B-Instruct\\triples\\Slide11.json",
        "parsed": {
          "triples": []
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"Qwen/Qwen2-VL-7B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-08T23:03:46.143663+00:00\", \"text_length\": 2236, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nPreviously, we saw how a vector in three-dimensional space can be represented using its projections onto the x, y, and z axes. Now, let’s extend this same idea to n-dimensional space — using orthogonal basis vectors to represent any arbitrary vector.\\n\\u000bLet’s quickly review the 3D case first.\\u000bWe have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1.\\u000bAny 3D vector — let’s say vector v — with components x, y, and z, can be written as a weighted sum of these basis vectors.\\u000bSo, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z.\\u000bEach of these terms represents the projection of vector v onto one of the coordinate directions. Because our basis vectors are orthogonal and of unit length, we don’t need to normalize — the inner product directly gives us the component in that direction.\\n\\nNow let’s generalize to n dimensions.\\u000bSuppose we have a vector — we’ll call it vector W — that lives in n-dimensional space. We can express this vector as a linear combination of n orthonormal basis vectors, denoted as e n, where n runs from 1 to capital N.\\u000bThe formula looks like this: vector W equals the sum from n equals 1 to capital N of W dot e n times e n.\\n\\nIn words, for each basis direction e n, we project W onto that direction by computing the inner product W dot e n. That gives us the scalar coefficient for that direction. Then we scale e n by that coefficient and sum everything together.\\u000b\\nThis is a very powerful idea: you can express any vector — no matter how high the dimension — as a sum of projections onto orthonormal basis vectors.\\u000bAnd what’s even more powerful is that those basis vectors don’t have to be just the standard coordinate axes.\\u000bThey could be sine and cosine functions, as we use in Fourier series. Or they could be eigenvectors in principal component analysis. Or wavelets, or anything else — as long as they are orthonormal.\\u000bSo what we’ve described on this slide is the general framework of orthogonal representation in n-dimensional space. It’s the core mathematical idea that underpins many tools in signal processing, data analysis, and machine learning.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"ND Orthogonal Representation\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"vector in n-dimensional space\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"ND Orthogonal Representation\\\"}\\n  ]\\n}\", \"parsed\": {\"triples\": []}}"
      }
    },
    "Qwen__Qwen3-VL-4B-Instruct": {
      "concepts": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\concepts\\Slide11.json",
        "parsed": {
          "term": "any vector - no matter how high the dimension - as a sum of projections onto orthonormal basis vectors",
          "category": "mathematics"
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"concepts\", \"timestamp_utc\": \"2025-11-07T22:47:54.735925+00:00\", \"text_length\": 2236, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are an expert educator in medical imaging. From the following slide text,\\nextract the key technical or scientific concepts that are essential for understanding the topic.\\n\\nUser:\\nSLIDE_TEXT:\\nPreviously, we saw how a vector in three-dimensional space can be represented using its projections onto the x, y, and z axes. Now, let’s extend this same idea to n-dimensional space — using orthogonal basis vectors to represent any arbitrary vector.\\n\\u000bLet’s quickly review the 3D case first.\\u000bWe have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1.\\u000bAny 3D vector — let’s say vector v — with components x, y, and z, can be written as a weighted sum of these basis vectors.\\u000bSo, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z.\\u000bEach of these terms represents the projection of vector v onto one of the coordinate directions. Because our basis vectors are orthogonal and of unit length, we don’t need to normalize — the inner product directly gives us the component in that direction.\\n\\nNow let’s generalize to n dimensions.\\u000bSuppose we have a vector — we’ll call it vector W — that lives in n-dimensional space. We can express this vector as a linear combination of n orthonormal basis vectors, denoted as e n, where n runs from 1 to capital N.\\u000bThe formula looks like this: vector W equals the sum from n equals 1 to capital N of W dot e n times e n.\\n\\nIn words, for each basis direction e n, we project W onto that direction by computing the inner product W dot e n. That gives us the scalar coefficient for that direction. Then we scale e n by that coefficient and sum everything together.\\u000b\\nThis is a very powerful idea: you can express any vector — no matter how high the dimension — as a sum of projections onto orthonormal basis vectors.\\u000bAnd what’s even more powerful is that those basis vectors don’t have to be just the standard coordinate axes.\\u000bThey could be sine and cosine functions, as we use in Fourier series. Or they could be eigenvectors in principal component analysis. Or wavelets, or anything else — as long as they are orthonormal.\\u000bSo what we’ve described on this slide is the general framework of orthogonal representation in n-dimensional space. It’s the core mathematical idea that underpins many tools in signal processing, data analysis, and machine learning.\\n\\nSTRICT INSTRUCTIONS:\\n- Return pure JSON only. No prose, no markdown, no code fences.\\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\\n- Assign exactly one category from:\\n  software | workflow | mathematics | signal_processing | frequency_domain |\\n  physics | instrumentation | data_processing | reconstruction |\\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\\n- Reject filler or administrative text.\\n- Output format:\\n{\\n  \\\"concepts\\\": [\\n    {\\\"term\\\": \\\"<exact term from slide>\\\", \\\"category\\\": \\\"<category>\\\"}\\n  ],\\n  \\\"evidence\\\": [\\\"<short supporting phrase from SLIDE_TEXT>\\\"]\\n}\\n\\nassistant\\n{\\n  \\\"concepts\\\": [\\n    {\\n      \\\"term\\\": \\\"vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"in the N-dimensional space we have vector W equals the sum from n equals 1 to capital N of W dot e n times e n\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"orthogonal basis vectors\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"projection of vector v onto one of the coordinate directions\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"inner product directly gives us the component in that direction\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"orthonormal basis vectors\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"any vector — no matter how high the dimension — as a sum of projections onto orthonormal basis vectors\\\",\\n      \\\"category\\\": \\\"mathematics\\\"\\n    },\\n    {\\n      \\\"term\\\": \\\"basis vectors don’t have to be just the standard\", \"parsed\": {\"term\": \"any vector - no matter how high the dimension - as a sum of projections onto orthonormal basis vectors\", \"category\": \"mathematics\"}}"
      },
      "triples": {
        "source": "G:\\My Drive\\1. Studies\\RPI\\Thesis\\1. Prof Ge Wang\\1. Avatar Project\\Future Directions\\Comparing Models\\Lectures\\Retrieved Data\\Further Work\\MILU23\\Lecture 5\\Outputs\\Qwen__Qwen3-VL-4B-Instruct\\triples\\Slide11.json",
        "parsed": {
          "s": "vector v",
          "p": "represents",
          "o": "v = (v · e_x) e_x + (v · e_y) e_y + (v · e_z) e_z",
          "modalities": [
            "text",
            "image"
          ],
          "confidence": 0.95,
          "evidence": [
            "We have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1. Any 3D vector - let's say vector v - with components x, y, and z, can be written as a weighted sum of these basis vectors. So, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z."
          ]
        },
        "raw": "{\"slide_id\": \"Slide11\", \"model\": \"Qwen/Qwen3-VL-4B-Instruct\", \"prompt\": \"triples\", \"timestamp_utc\": \"2025-11-07T22:55:24.936896+00:00\", \"text_length\": 2236, \"raw_output\": \"system\\nYou are a helpful AI for medical imaging.\\nuser\\nSystem:\\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\\n\\nUser:\\nSLIDE_TEXT:\\nPreviously, we saw how a vector in three-dimensional space can be represented using its projections onto the x, y, and z axes. Now, let’s extend this same idea to n-dimensional space — using orthogonal basis vectors to represent any arbitrary vector.\\n\\u000bLet’s quickly review the 3D case first.\\u000bWe have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1.\\u000bAny 3D vector — let’s say vector v — with components x, y, and z, can be written as a weighted sum of these basis vectors.\\u000bSo, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z.\\u000bEach of these terms represents the projection of vector v onto one of the coordinate directions. Because our basis vectors are orthogonal and of unit length, we don’t need to normalize — the inner product directly gives us the component in that direction.\\n\\nNow let’s generalize to n dimensions.\\u000bSuppose we have a vector — we’ll call it vector W — that lives in n-dimensional space. We can express this vector as a linear combination of n orthonormal basis vectors, denoted as e n, where n runs from 1 to capital N.\\u000bThe formula looks like this: vector W equals the sum from n equals 1 to capital N of W dot e n times e n.\\n\\nIn words, for each basis direction e n, we project W onto that direction by computing the inner product W dot e n. That gives us the scalar coefficient for that direction. Then we scale e n by that coefficient and sum everything together.\\u000b\\nThis is a very powerful idea: you can express any vector — no matter how high the dimension — as a sum of projections onto orthonormal basis vectors.\\u000bAnd what’s even more powerful is that those basis vectors don’t have to be just the standard coordinate axes.\\u000bThey could be sine and cosine functions, as we use in Fourier series. Or they could be eigenvectors in principal component analysis. Or wavelets, or anything else — as long as they are orthonormal.\\u000bSo what we’ve described on this slide is the general framework of orthogonal representation in n-dimensional space. It’s the core mathematical idea that underpins many tools in signal processing, data analysis, and machine learning.\\n\\nSTRICT INSTRUCTIONS:\\n- Return JSON only. No prose, no markdown, no code fences.\\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\\n- Add modalities [\\\"text\\\"] by default; add \\\"image\\\" only if clearly visible without the text.\\n- Confidence in [0,1].\\n\\nOUTPUT:\\n{\\n  \\\"triples\\\": [\\n    {\\\"s\\\":\\\"<verbatim subject>\\\", \\\"p\\\":\\\"uses|via|represents|depends_on|measures|produces|reconstructs_with\\\",\\n     \\\"o\\\":\\\"<verbatim object>\\\", \\\"modalities\\\":[\\\"text\\\",\\\"image\\\"], \\\"confidence\\\":0.0, \\\"evidence\\\":\\\"<short quote from SLIDE_TEXT>\\\"}\\n  ]\\n}\\n\\nassistant\\n{\\n  \\\"triples\\\": [\\n    {\\n      \\\"s\\\": \\\"vector v\\\",\\n      \\\"p\\\": \\\"represents\\\",\\n      \\\"o\\\": \\\"v = (v · e_x) e_x + (v · e_y) e_y + (v · e_z) e_z\\\",\\n      \\\"modalities\\\": [\\n        \\\"text\\\",\\n        \\\"image\\\"\\n      ],\\n      \\\"confidence\\\": 0.95,\\n      \\\"evidence\\\": \\\"We have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1. Any 3D vector — let’s say vector v — with components x, y, and z, can be written as a weighted sum of these basis vectors. So, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z.\\\"\\n    },\\n    {\\n      \\\"s\\\": \\\"vector W\\\",\\n      \\\"p\\\": \\\"represents\\\",\\n      \\\"o\\\": \\\"W = sum from n=1 to N of (W\", \"parsed\": {\"s\": \"vector v\", \"p\": \"represents\", \"o\": \"v = (v · e_x) e_x + (v · e_y) e_y + (v · e_z) e_z\", \"modalities\": [\"text\", \"image\"], \"confidence\": 0.95, \"evidence\": [\"We have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1. Any 3D vector - let's say vector v - with components x, y, and z, can be written as a weighted sum of these basis vectors. So, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z.\"]}}"
      }
    }
  }
}