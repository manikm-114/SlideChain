Supervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.

The figure shows how the inputs are transformed step by step through hidden units and weights, leading to correct outputs for all training examples. This principle extends far beyond toy problemsâ€”it is the basis of modern deep learning, where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance.
