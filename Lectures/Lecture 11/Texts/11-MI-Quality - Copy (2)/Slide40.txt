Now let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.

Here’s the key idea: sensitivity and specificity are not fixed properties of a test. They depend on the threshold we use to decide between “disease” and “no disease.”

For example, if I set the threshold very low, I’ll call almost everything abnormal. That means I’ll catch nearly every true case, so sensitivity will be very high. But I’ll also generate many false alarms, so specificity will drop.
On the other hand, if I set the threshold very high, I’ll call almost everything normal. That means I’ll have very few false alarms — so specificity will be excellent. But I’ll also miss many real cases, so sensitivity will be low.

The ROC curve captures this trade-off. On the x-axis, we plot 1 – specificity, which is the false positive rate. On the y-axis, we plot sensitivity, the true positive rate. By sweeping through all possible thresholds, we trace out the curve.
The ROC curve shows the overall performance of the test across all possible decision boundaries. And importantly, the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy. An AUC of 1.0 means perfect classification. An AUC of 0.5 — a diagonal line — means the test is no better than random guessing.

So the ROC curve is widely used in medical imaging research, because it provides a clear, quantitative way to compare diagnostic systems and observer performance.
