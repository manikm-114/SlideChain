In engineering, one of the most familiar terms you’ll encounter is the Signal-to-Noise Ratio, or SNR.
Whenever we take a measurement — whether it’s a photograph, an MRI scan, or an ultrasound image — we always have some amount of noise. Noise is unavoidable. It arises from the fundamental randomness of physical processes, and ultimately from quantum mechanics itself. Nothing is ever perfectly certain.

On top of this background noise, we have the signal — the part of the measurement that actually carries meaningful information.
SNR is a simple but powerful way of expressing how strong the signal is relative to the noise. Mathematically, it is defined as the ratio of signal power to noise power. Since power is proportional to the square of amplitude, you can also write SNR as the square of the signal amplitude divided by the noise amplitude.

Here’s the intuition:
If the SNR is around 1, the signal varies in the same range as the noise. That means the signal is barely visible.
If the SNR is 5 or 10 or higher, the signal stands out clearly above the noise, and it can be easily detected.

A related measure is the Contrast-to-Noise Ratio, or CNR. Instead of comparing one signal to background noise, we compare the difference between two signals — for example, a feature versus its background — divided by the noise level. This measure is very common in medical imaging because it directly reflects how well we can distinguish one structure from another.

So, SNR tells us whether a signal can rise above the noise at all, while CNR tells us whether two signals can be distinguished against that noisy background.
And next, we’ll move on to another fundamental aspect of image quality: resolution.
