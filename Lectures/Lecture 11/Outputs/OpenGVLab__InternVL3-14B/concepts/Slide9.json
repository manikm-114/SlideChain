{
  "slide_id": "Slide9",
  "model": "OpenGVLab/InternVL3-14B",
  "prompt": "concepts",
  "raw_output": "```json\n{\n  \"concepts\": [\n    {\"term\": \"mutual information\", \"category\": \"mathematics\"},\n    {\"term\": \"KL divergence\", \"category\": \"mathematics\"},\n    {\"term\": \"joint distribution\", \"category\": \"mathematics\"},\n    {\"term\": \"marginal distributions\", \"category\": \"mathematics\"},\n    {\"term\": \"image registration\", \"category\": \"data_processing\"},\n    {\"term\": \"correlation\", \"category\": \"mathematics\"},\n    {\"term\": \"statistical dependence\", \"category\": \"mathematics\"}\n  ],\n  \"evidence\": [\n    \"mutual information is a way to measure how much knowing one random variable tells us about another\",\n    \"Mathematically, mutual information can actually be expressed in terms of the KL divergence\",\n    \"the KL divergence between the joint distribution of X and Y, and the product of their marginal distributions\",\n    \"when you measure one variable, how much does your uncertainty about the other variable decrease\",\n    \"In practice, this idea is very useful in areas like image registration\",\n    \"So, mutual information is essentially a generalization of correlation\",\n    \"It goes beyond simple linear relationships and captures any kind of statistical dependence\"\n  ]",
  "parsed": {
    "term": "statistical dependence",
    "category": "mathematics"
  }
}