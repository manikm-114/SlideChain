{
  "slide_id": "Slide10",
  "model": "OpenGVLab/InternVL3-14B",
  "prompt": "concepts",
  "raw_output": "{\n  \"concepts\": [\n    {\"term\": \"entropy\", \"category\": \"mathematics\"},\n    {\"term\": \"mutual information\", \"category\": \"mathematics\"},\n    {\"term\": \"KL divergence\", \"category\": \"mathematics\"},\n    {\"term\": \"mean squared error\", \"category\": \"quality_metric\"}\n  ],\n  \"evidence\": [\n    \"entropy tells us how much information, or how much unpredictability, is contained in a random variable.\",\n    \"mutual information can be written as the difference between two entropies: the entropy of Y by itself, minus the entropy of Y given X.\",\n    \"these information-theoretic measures — KL divergence, mutual information, and entropy — are very powerful\",\n    \"simpler measures like mean squared error\"\n  ]\n}",
  "parsed": {
    "concepts": [
      {
        "term": "entropy",
        "category": "mathematics"
      },
      {
        "term": "mutual information",
        "category": "mathematics"
      },
      {
        "term": "KL divergence",
        "category": "mathematics"
      },
      {
        "term": "mean squared error",
        "category": "quality_metric"
      }
    ],
    "evidence": [
      "entropy tells us how much information, or how much unpredictability, is contained in a random variable.",
      "mutual information can be written as the difference between two entropies: the entropy of Y by itself, minus the entropy of Y given X.",
      "these information-theoretic measures - KL divergence, mutual information, and entropy - are very powerful",
      "simpler measures like mean squared error"
    ]
  }
}