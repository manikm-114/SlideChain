{
  "slide_id": "Slide8",
  "model": "OpenGVLab/InternVL3-14B",
  "prompt": "concepts",
  "raw_output": "{\n  \"concepts\": [\n    {\"term\": \"information divergence\", \"category\": \"mathematics\"},\n    {\"term\": \"Kullback-Leibler divergence\", \"category\": \"mathematics\"},\n    {\"term\": \"KL distance\", \"category\": \"mathematics\"},\n    {\"term\": \"probability distributions\", \"category\": \"mathematics\"},\n    {\"term\": \"Euclidean distance\", \"category\": \"mathematics\"},\n    {\"term\": \"KL divergence\", \"category\": \"mathematics\"},\n    {\"term\": \"symmetric\", \"category\": \"mathematics\"},\n    {\"term\": \"directionality\", \"category\": \"mathematics\"},\n    {\"term\": \"machine learning\", \"category\": \"ai_ml\"},\n    {\"term\": \"statistical signal processing\", \"category\": \"signal_processing\"}\n  ],\n  \"evidence\": [\n    \"This is where probability theory comes in.\",\n    \"The formula looks a bit unusual: KL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\",\n    \"The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\",\n    \"One interesting property is that the",
  "parsed": {
    "term": "statistical signal processing",
    "category": "signal_processing"
  }
}