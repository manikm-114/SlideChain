{
  "slide_id": "Slide40",
  "model": "Qwen/Qwen2-VL-7B-Instruct",
  "prompt": "concepts",
  "timestamp_utc": "2025-11-09T04:30:16.532019+00:00",
  "text_length": 1527,
  "raw_output": "system\nYou are a helpful AI for medical imaging.\nuser\nSystem:\nYou are an expert educator in medical imaging. From the following slide text,\nextract the key technical or scientific concepts that are essential for understanding the topic.\n\nUser:\nSLIDE_TEXT:\nNow let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.\n\nHere’s the key idea: sensitivity and specificity are not fixed properties of a test. They depend on the threshold we use to decide between “disease” and “no disease.”\n\nFor example, if I set the threshold very low, I’ll call almost everything abnormal. That means I’ll catch nearly every true case, so sensitivity will be very high. But I’ll also generate many false alarms, so specificity will drop.\nOn the other hand, if I set the threshold very high, I’ll call almost everything normal. That means I’ll have very few false alarms — so specificity will be excellent. But I’ll also miss many real cases, so sensitivity will be low.\n\nThe ROC curve captures this trade-off. On the x-axis, we plot 1 – specificity, which is the false positive rate. On the y-axis, we plot sensitivity, the true positive rate. By sweeping through all possible thresholds, we trace out the curve.\nThe ROC curve shows the overall performance of the test across all possible decision boundaries. And importantly, the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy. An AUC of 1.0 means perfect classification. An AUC of 0.5 — a diagonal line — means the test is no better than random guessing.\n\nSo the ROC curve is widely used in medical imaging research, because it provides a clear, quantitative way to compare diagnostic systems and observer performance.\n\nSTRICT INSTRUCTIONS:\n- Return pure JSON only. No prose, no markdown, no code fences.\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\n- Assign exactly one category from:\n  software | workflow | mathematics | signal_processing | frequency_domain |\n  physics | instrumentation | data_processing | reconstruction |\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\n- Reject filler or administrative text.\n- Output format:\n{\n  \"concepts\": [\n    {\"term\": \"<exact term from slide>\", \"category\": \"<category>\"}\n  ],\n  \"evidence\": [\"<short supporting phrase from SLIDE_TEXT>\"]\n}\n\nassistant\n{\n  \"concepts\": [\n    {\"term\": \"receiver operating characteristic curve\", \"category\": \"algorithm\"},\n    {\"term\": \"ROC curve\", \"category\": \"algorithm\"},\n    {\"term\": \"sensitivity\", \"category\": \"algorithm\"},\n    {\"term\": \"specificity\", \"category\": \"algorithm\"},\n    {\"term\": \"threshold\", \"category\": \"algorithm\"},\n    {\"term\": \"false positive rate\", \"category\": \"algorithm\"},\n    {\"term\": \"true positive rate\", \"category\": \"algorithm\"},\n    {\"term\": \"diagnostic performance evaluation\", \"category\": \"algorithm\"},\n    {\"term\": \"area under the ROC curve\", \"category\": \"algorithm\"},\n    {\"term\": \"AUC\", \"category\": \"algorithm\"},\n    {\"term\": \"perfect classification\", \"category\": \"algorithm\"},\n    {\"term\": \"random guessing\", \"category\": \"algorithm\"},\n    {\"term\": \"medical imaging research\", \"category\": \"algorithm\"}\n  ],\n  \"evidence\": [\n    \"The ROC curve captures this trade-off.\",\n    \"The ROC curve shows the overall performance of the test across all possible decision boundaries.\",\n    \"The area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy.\",",
  "parsed": {
    "term": "medical imaging research",
    "category": "algorithm"
  }
}