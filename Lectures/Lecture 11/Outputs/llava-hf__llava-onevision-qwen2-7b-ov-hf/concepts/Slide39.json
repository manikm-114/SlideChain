{
  "slide_id": "Slide39",
  "model": "llava-hf/llava-onevision-qwen2-7b-ov-hf",
  "prompt": "concepts",
  "raw_output": "user \nSystem:\nYou are an expert educator in medical imaging. From the following slide text,\nextract the key technical or scientific concepts that are essential for understanding the topic.\n\nUser:\nSLIDE_TEXT:\nHere’s a concrete example using tuberculosis screening with chest X-ray. In this study, nearly 2,000 people were tested, but only 30 actually had tuberculosis.\n\nLet’s break this down.\nAmong the 30 true TB patients, 22 were correctly reported as positive, while 8 were missed.\nAmong the 1,790 healthy individuals, 51 were incorrectly reported as positive, but the majority — 1,739 — were correctly classified as negative.\n\nNow, from this table, we can calculate the standard measures:\nSensitivity: 22 out of 30 true TB cases were detected → about 73%. This means the system caught almost three-quarters of real cases.\nSpecificity: 1,739 out of 1,790 healthy cases were correctly identified → about 97%. This shows the system rarely gave false alarms.\nPositive Predictive Value (PPV): 22 out of 73 reported positives were real TB cases → only 30%. This is much lower, meaning that when the system flagged a case as positive, it was wrong more often than it was right.\nNegative Predictive Value (NPV): 1,739 out of 1,747 reported negatives were truly healthy → nearly 99.5%. This means a negative result was highly reliable.\nDiagnostic Accuracy: The proportion of all correct results, positive and negative, was about 97%.\nPrevalence: Only 30 out of 1,820 people had TB → around 1.6%.\n\nNotice something important here: although accuracy and NPV are very high, the PPV is quite low. That’s because the disease is rare in this population — when prevalence is low, even a small number of false positives can outweigh the true positives.\n\nThis example shows why we need to interpret these metrics carefully, especially in screening programs. Sensitivity, specificity, PPV, NPV, and prevalence all interact, and each tells us something different about how the test performs in real-world conditions.\n\nSTRICT INSTRUCTIONS:\n- Return pure JSON only. No prose, no markdown, no code fences.\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\n- Assign exactly one category from:\n  software | workflow | mathematics | signal_processing | frequency_domain |\n  physics | instrumentation | data_processing | reconstruction |\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\n- Category guidance:\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\n  * physics → energy, radiation, waves, gradients, attenuation\n  * instrumentation → scanners, detectors, coils, transducers, gantry\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\n  * communication → network or data transfer\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\n  * algorithm → analytical/iterative computational methods\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\n- Output format:\n{\n  \"concepts\": [\n    {\"term\": \"<exact term from slide>\", \"category\": \"<category>\"}\n  ],\n  \"evidence\": [\"<short supporting phrase from SLIDE_TEXT>\"]\n}\nassistant\n```json\n{\n  \"concepts\": [\n    {\"term\": \"sensitivity\", \"category\": \"quality_metric\"},\n    {\"term\": \"specificity\", \"category\": \"quality_metric\"},\n    {\"term\": \"positive predictive value (PPV)\", \"category\": \"quality_metric\"},\n    {\"term\": \"negative predictive value (NPV)\", \"category\": \"quality_metric\"},\n    {\"term\": \"prevalence\", \"category\": \"quality_metric\"},\n    {\"term\": \"accuracy\", \"category\": \"quality_metric\"},\n    {\"term\": \"disease\", \"category\": \"anatomy\"},\n    {\"term\": \"screening program\", \"category\": \"workflow\"}\n  ],\n  \"evidence\": [\"Among the 30 true TB patients, 22 were correctly reported as positive, while 8 were missed.\", \"Among the 1,790 healthy individuals, 51 were incorrectly reported as positive, but the majority — 1,739 — were correctly classified as negative.\", \"This example shows why we need to interpret these metrics carefully, especially in screening programs. Sensitivity, specificity, PPV, NPV, and prevalence all interact, and each tells us something different about how the test performs in real-world",
  "parsed": {
    "term": "screening program",
    "category": "workflow"
  }
}