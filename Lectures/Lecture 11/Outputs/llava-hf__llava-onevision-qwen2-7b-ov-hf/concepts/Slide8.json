{
  "slide_id": "Slide8",
  "model": "llava-hf/llava-onevision-qwen2-7b-ov-hf",
  "prompt": "concepts",
  "raw_output": "user \nSystem:\nYou are an expert educator in medical imaging. From the following slide text,\nextract the key technical or scientific concepts that are essential for understanding the topic.\n\nUser:\nSLIDE_TEXT:\nNow, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in.\n\nSuppose you don’t just have two images, but instead you have two probability distributions — for example, two different histograms of pixel values. The question becomes: how do we measure the difference between these two distributions?\nOne option is to use Euclidean distance, just as before. But there is a more meaningful way in the context of information theory. This is called the Kullback–Leibler divergence, or KL distance for short.\n\nThe formula looks a bit unusual:\u000bKL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\nYou don’t need to worry too much about the details — this is beyond the scope of our lecture — but the idea is important. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\n\nOne interesting property is that the KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P. That may sound strange, but it has a good analogy. Think of climbing a mountain: going uphill is much harder than going back downhill, even though it’s the same physical path. In the same way, KL divergence measures directionality in information.\n\nSo, while we won’t use this directly in our course, it’s good to be aware that such information-based distances exist. They play a big role in areas like machine learning and statistical signal processing.\n\nSTRICT INSTRUCTIONS:\n- Return pure JSON only. No prose, no markdown, no code fences.\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\n- Assign exactly one category from:\n  software | workflow | mathematics | signal_processing | frequency_domain |\n  physics | instrumentation | data_processing | reconstruction |\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\n- Category guidance:\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\n  * physics → energy, radiation, waves, gradients, attenuation\n  * instrumentation → scanners, detectors, coils, transducers, gantry\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\n  * communication → network or data transfer\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\n  * algorithm → analytical/iterative computational methods\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\n- Output format:\n{\n  \"concepts\": [\n    {\"term\": \"<exact term from slide>\", \"category\": \"<category>\"}\n  ],\n  \"evidence\": [\"<short supporting phrase from SLIDE_TEXT>\"]\n}\nassistant\n```json\n{\n  \"concepts\": [\n    {\"term\": \"KL divergence\", \"category\": \"mathematics\"},\n    {\"term\": \"probability distributions\", \"category\": \"mathematics\"},\n    {\"term\": \"Euclidean distance\", \"category\": \"mathematics\"},\n    {\"term\": \"information theory\", \"category\": \"mathematics\"},\n    {\"term\": \"KL divergence is not symmetric\", \"category\": \"mathematics\"},\n    {\"term\": \"KL divergence measures directionality in information\", \"category\": \"mathematics\"},\n    {\"term\": \"machine learning\", \"category\": \"ai_ml\"},\n    {\"term\": \"statistical signal processing\", \"category\": \"signal_processing\"}\n  ],\n  \"evidence\": [\"This is where probability theory comes in.\", \n               \"The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\", \n               \"The KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P.\", \n               \"KL divergence measures directionality in information.\"]\n}\n```",
  "parsed": {
    "concepts": [
      {
        "term": "KL divergence",
        "category": "mathematics"
      },
      {
        "term": "probability distributions",
        "category": "mathematics"
      },
      {
        "term": "Euclidean distance",
        "category": "mathematics"
      },
      {
        "term": "information theory",
        "category": "mathematics"
      },
      {
        "term": "KL divergence is not symmetric",
        "category": "mathematics"
      },
      {
        "term": "KL divergence measures directionality in information",
        "category": "mathematics"
      },
      {
        "term": "machine learning",
        "category": "ai_ml"
      },
      {
        "term": "statistical signal processing",
        "category": "signal_processing"
      }
    ],
    "evidence": [
      "This is where probability theory comes in.",
      "The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.",
      "The KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P.",
      "KL divergence measures directionality in information."
    ]
  }
}