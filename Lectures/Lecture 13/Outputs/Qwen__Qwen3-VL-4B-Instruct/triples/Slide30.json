{
  "slide_id": "Slide30",
  "model": "Qwen/Qwen3-VL-4B-Instruct",
  "prompt": "triples",
  "timestamp_utc": "2025-11-08T11:47:21.606841+00:00",
  "text_length": 3282,
  "raw_output": "system\nYou are a helpful AI for medical imaging.\nuser\nSystem:\nYou are extracting factual relations from a medical imaging lecture. Use only information grounded in the input SLIDE_TEXT.\n\nUser:\nSLIDE_TEXT:\nSo here we have this picture. The backprojection is visualized. This is the projection profile after filtration, so it is a filtered projection profile. This filtered projection profile, Q theta i of t, is really smeared back over the field of view. Think of this projection profile as being smeared back from this particular direction. You have many filtered projection profiles; you sweep all of them and smear them back over the field of view. Add them together to get the result.\n\nFor a given x, y, what is the contribution from a given Q theta? That is the question I want to explain. Suppose x, y is here. For this x, y—the point in image space—how much contribution do we get from a particular filtered projection profile, here Q theta i? You do this inner product: you get t. Spoken clearly: t equals x cosine theta i plus y sine theta i. That t is the distance between this line and the central line, both perpendicular to this direction, making an angle theta i. For any point along this line, the inner product gives the same t. So, as long as the pixel is on this line at distance t, you retrieve this value from the filtered projection profile. For all points on this line, t is the same. You compute t, you get the value, and you put that value back for any x, y on this line. That value is the same here, here, and here—all along the line. If that value is one, then along that line every point gets one. This is what I call smearing back.\n\nThat was for this theta, theta i. For another angle, you smear back in a different way. The process is linear—the integral means summation. For each theta, you retrieve the value from Q theta, then add it back to the particular point x, y. For another angle you do the same. You can view it like this: from this filtered projection profile, you get the value here; along a different direction, another projection profile gives you another value through that same point x, y. So, heuristically, from the filtered projection domain, any profile is smeared back uniformly, its value covering all points along its line. All projection angles are added in the same way, and then you have the image value recovered. Think about that. I will show reconstructing examples in a few minutes, so you can get a better idea. OKay.\n\nNow, several slides with green buttons are about the reconstruction filter. Here, the reconstruction filter—a high-pass filter—is the absolute value of w. This filter does not have an inverse Fourier transform by itself, because the absolute value of w is not integrable. However, we can introduce a band-limited assumption: for a given projection profile, you have a maximum frequency, a maximum bandwidth W.\n\nSo the filtration really needs to be done with this truncated filter. In the original Fourier space, the ideal is the absolute value of omega extending to infinity. We assume a maximum bandwidth capital W; outside that, nothing is meaningful for the data. With this window, the spectrum becomes integrable. You truncate the otherwise divergent high-pass filter, then perform the inverse transform with respect to this truncated version, H of omega. You compute the inverse Fourier transform as the spatial-domain counterpart of the high-pass filter. You do the computation, and you get this part.\n\nSTRICT INSTRUCTIONS:\n- Return JSON only. No prose, no markdown, no code fences.\n- Output a triple only if both subject and object appear verbatim in SLIDE_TEXT (case-insensitive).\n- Predicates must be one of: uses | via | represents | depends_on | measures | produces | reconstructs_with.\n- Add modalities [\"text\"] by default; add \"image\" only if clearly visible without the text.\n- Confidence in [0,1].\n\nOUTPUT:\n{\n  \"triples\": [\n    {\"s\":\"<verbatim subject>\", \"p\":\"uses|via|represents|depends_on|measures|produces|reconstructs_with\",\n     \"o\":\"<verbatim object>\", \"modalities\":[\"text\",\"image\"], \"confidence\":0.0, \"evidence\":\"<short quote from SLIDE_TEXT>\"}\n  ]\n}\n\nassistant\n{\n  \"triples\": [\n    {\n      \"s\": \"H(w)\",\n      \"p\": \"represents\",\n      \"o\": \"the transfer function of a filter with which the projections must be processed\",\n      \"modalities\": [\n        \"text\"\n      ],\n      \"confidence\": 0.95,\n      \"evidence\": \"H(w), shown in Fig. 3.14, represents the transfer function of a filter with which the projections must be processed.\"\n    },\n    {\n      \"s\": \"H(w)\",\n      \"p\": \"uses\",\n      \"o\": \"the inverse Fourier transform\",\n      \"modalities\": [\n        \"text\"\n      ],\n      \"confidence\": 0.9,\n      \"evidence\": \"The impulse response, h(t), of this filter is given by the inverse Fourier transform of H(w) and is\"\n    },\n    {\n      \"s\": \"b_w(w)\",\n      \"p\": \"represents\",\n      \"o\": \"a window function\",\n      \"modalities\": [\n        \"text\"\n      ],\n      \"confidence\": 0.9,\n      \"evidence\": \"where, again, b_w(w) = { 1 |w| < W,",
  "parsed": {
    "s": "H(w)",
    "p": "uses",
    "o": "the inverse Fourier transform",
    "modalities": [
      "text"
    ],
    "confidence": 0.9,
    "evidence": [
      "The impulse response, h(t), of this filter is given by the inverse Fourier transform of H(w) and is"
    ]
  }
}