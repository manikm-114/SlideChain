Here, I’ll still use two horizontal rays and two vertical rays to show how we solve linear system equations using iterative algorithms through trial and error. 

Why explain an iterative algorithm? For a simple case like this, you can solve it directly using an analytic method. But when the number of unknowns is huge—like millions or billions of equations—the direct method will not work efficiently. You may not have enough computer memory, and other issues may arise. So you use an iterative algorithm to solve the situation. Also, the iterative algorithm I’m about to explain lets you impose prior knowledge, like non-negativity or smoothness. These topics are beyond the scope of this lecture.

Let me give you the basic idea—how you can do trial and error to solve a system of linear equations. As I told you, this is your underlying image. You have four measurements—two horizontal and two vertical—and you try to solve this system. The starting point: whenever you use an iterative algorithm, you need a starting point. Here, because I know nothing about the image, I choose a neutral guess and assume nothing in the field of view, so every pixel is zero. This is my starting point, or guess zero. It is a natural and unbiased starting point.

First, if this guess were correct, then the vertical integrals must be zero and zero. Based on the assumption, I get these two estimated values: zero, zero. We call this a predicted or synthetic projection. It’s not that you must accept zero, zero just because I say so—you can challenge it. You can say, “If everything is zero, the vertical integrals must be zero, zero.” But the physical measurement says six and four. It is not zero, zero. How do we explain the contradiction?
We compare the measurement with the prediction. We see errors six and four—here you measured six but predicted zero, so the error is six; there you measured four but predicted zero, so the error is four. This positive error indicates my initial guess underestimates the pixel values.

Along this ray, the real measurement is four, but I predicted zero. Clearly, there must be something in these two pixels. Their values should add to six along the first vertical ray. I do not know whether to give more to the first pixel or the second pixel, so to be fair, I evenly divide the error: six becomes three and three. I put the error back. After this redistribution, the two values add to six, so that vertical error is removed. Likewise, for the second vertical ray, four becomes two and two. After this, I am vertically consistent: along the first column, three plus three equals six, and along the second column, two plus two equals four, matching the measurements.

Next, let’s double-check the horizontal integrals. Now the row sums are five and five. The measured horizontal sums are seven and three. Comparing again, the errors are plus two and minus two. A positive error means I still underestimate that row; a negative error means the true sum is less than my current estimate.
I redistribute the errors: the plus two is decomposed into one and one and added back to the two pixels in the first row; the minus two is decomposed into minus one and minus one and subtracted from the two pixels in the second row. In this case, we are lucky—after this second iteration, we obtain the correct result. Once we reach this state, the vertical and horizontal data are both perfectly explained. We are done—this illustrates the idea.

In real situations, it is never this simple. You need many iterations and many unknowns go back and forth, and a well-designed iterative algorithm will ensure that after many iterations the solution converges. Sometimes the iterative process gives an oscillating solution, and you need regularization. But again, for this undergraduate-level course, notice the basic idea.