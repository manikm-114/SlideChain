Now let’s look at how we represent a neuron in a computer — what we call a perceptron.

A perceptron takes multiple inputs, each labeled here as a 1, a 2, a 3,…, a n. Each input has a weight, shown as w 1,w 2,w 3,…,w n​, which controls how important that input is.

The first thing we do is multiply each input by its weight. Then we sum them all together — this is an inner product operation. Often, we also add an extra term called a bias, b, which shifts the output. You can think of the bias as an offset — a way to control the baseline response of the neuron, even if all the inputs are zero.

Once we have this weighted sum, we pass it through a nonlinear function, which we usually call the activation function. Here it’s labeled 
sigma. This step is essential — without it, the perceptron would just be a simple linear device.
There are many choices for the activation function. A common one is the sigmoid, which produces an S-shaped curve. Another is the hyperbolic tangent, or tanh, which gives outputs between minus one and plus one. There are also piecewise-linear functions such as the ReLU — the rectified linear unit — where the output is zero for negative inputs and increases proportionally for positive inputs.

So in short: a perceptron takes weighted inputs, adds them up, applies a bias, and then uses a nonlinear transformation to produce its output. This artificial neuron is the basic building block for artificial neural networks — just as resistors, capacitors, and inductors are building blocks in electrical circuits.
