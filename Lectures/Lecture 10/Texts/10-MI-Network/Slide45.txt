Once we define a loss function — the measure of difference between the network’s output and the ground truth — our goal is to reduce that difference step by step. We do this by adjusting the network’s parameters in a way that moves us downhill on the loss surface.

You can imagine this process like standing on a hilly landscape and trying to reach the lowest valley. At each step, we look around, find the steepest downward direction, and take a small move in that direction. This is the essence of gradient descent.
Over time, with each update, the loss gets smaller and smaller until the network converges to a good solution — ideally, a point where further changes no longer improve the performance.
