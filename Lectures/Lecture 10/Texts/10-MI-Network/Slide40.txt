Now that we understand a single perceptron, let’s see what happens when we connect many of them. This gives us an artificial neural network.

At the far left, we have the input layer — each circle here receives one piece of information from our data. These could be pixel values from an image, measurements from a sensor, or any other kind of features we want the network to process.
Next, we have one or more hidden layers. Each neuron in a hidden layer takes the outputs from the previous layer, applies its weights, sums them up, passes the result through its activation function, and then sends the output to the next layer. These hidden layers are where the network learns increasingly complex patterns.

Finally, at the far right, we have the output layer. In a classification task, each neuron here might represent a possible label — for example, “dog,” “cat,” or “car.” The neuron with the highest output value would be the network’s prediction.

Here’s the key idea:
A single neuron can only separate very simple patterns.
By stacking many layers, the network can build up a hierarchy of features.
The first layer might detect basic edges or colors.
The next layer might detect shapes like wheels or ears.
Higher layers combine these into entire objects, like a car or a cat.

Training the network works like this: we start with random weights, so the predictions are essentially guesses. We compare the network’s output to the correct answer and compute the error. Then, using a process called backpropagation, we send this error backward through the network, adjusting the weights slightly to reduce the error next time.
With enough data, enough layers, and many training cycles, the network learns to recognize very complex patterns — the kind of capability that powers things like self-driving cars, medical image analysis, and voice recognition.
