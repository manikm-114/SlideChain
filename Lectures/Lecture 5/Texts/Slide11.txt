Previously, we saw how a vector in three-dimensional space can be represented using its projections onto the x, y, and z axes. Now, let’s extend this same idea to n-dimensional space — using orthogonal basis vectors to represent any arbitrary vector.
Let’s quickly review the 3D case first.We have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1.Any 3D vector — let’s say vector v — with components x, y, and z, can be written as a weighted sum of these basis vectors.So, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z.Each of these terms represents the projection of vector v onto one of the coordinate directions. Because our basis vectors are orthogonal and of unit length, we don’t need to normalize — the inner product directly gives us the component in that direction.

Now let’s generalize to n dimensions.Suppose we have a vector — we’ll call it vector W — that lives in n-dimensional space. We can express this vector as a linear combination of n orthonormal basis vectors, denoted as e n, where n runs from 1 to capital N.The formula looks like this: vector W equals the sum from n equals 1 to capital N of W dot e n times e n.

In words, for each basis direction e n, we project W onto that direction by computing the inner product W dot e n. That gives us the scalar coefficient for that direction. Then we scale e n by that coefficient and sum everything together.
This is a very powerful idea: you can express any vector — no matter how high the dimension — as a sum of projections onto orthonormal basis vectors.And what’s even more powerful is that those basis vectors don’t have to be just the standard coordinate axes.They could be sine and cosine functions, as we use in Fourier series. Or they could be eigenvectors in principal component analysis. Or wavelets, or anything else — as long as they are orthonormal.So what we’ve described on this slide is the general framework of orthogonal representation in n-dimensional space. It’s the core mathematical idea that underpins many tools in signal processing, data analysis, and machine learning.