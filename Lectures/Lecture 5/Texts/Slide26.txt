Now let’s take the next step and define the Fourier series more precisely.

To keep things simple — and without losing any generality — we’ll start with a function defined over the interval from 0 to 1. We call this the unit period. The key assumption here is that the function repeats every unit interval. So if we understand what the function does between 0 and 1, we automatically understand it from 1 to 2, 2 to 3, and even from negative 1 to 0.

Later, we’ll generalize to other periods — like from 0 to capital T, or from negative T over 2 to positive T over 2. But for now, let’s build our intuition using this simpler interval.

So, what kinds of functions are we considering here?
We’re working with real-valued functions that are square-integrable — that means when you square the function and integrate it over the interval from 0 to 1, the result is finite. If that integral blows up to infinity, the function is too wild, and we can’t represent it with a Fourier series. But if it’s finite, then the function behaves well enough for our purposes.

Mathematically, we say such functions belong to the space L square of zero to one — that’s the set of square-integrable functions over the interval from 0 to 1. This is a Hilbert space, meaning we can treat functions like vectors in an infinite-dimensional space.
And just like vectors have a basis, functions in this space also have an orthonormal basis.
What does that basis look like?

It turns out the basis consists of the constant function 1, plus an infinite family of sine and cosine functions:
Square root of 2 times cosine of 2 pi n t
Square root of 2 times sine of 2 pi n tfor n equals 1, 2, 3, and so on.
The factor of square root 2 ensures that each function has unit length — meaning, its inner product with itself equals 1. That’s what makes the basis orthonormal.

So here’s the big idea:
Any square-integrable function over the interval 0 to 1 can be expressed as a sum of these basis functions.And this sum is called the Fourier series.
Mathematically, we write:
f of t equals a naught over two,plus the summation over n of a n times cosine of 2 pi n t,plus the summation over n of b n times sine of 2 pi n t.

So, what do all these parts mean?
First, a naught is called the DC component. It represents the average value of the function over one period.
The a n coefficients are multiplied by cosine terms. These describe the even, symmetric parts of the function.
And the b n coefficients go with the sine terms. They capture the odd, asymmetric parts of the function.
In essence, what we’re doing is splitting the function into symmetrical and asymmetrical patterns, using sines and cosines as building blocks. This is what makes the Fourier series so elegant — it expresses any periodic function as a mix of smooth, familiar waveforms.

Now, each of these coefficients can be computed using inner products, which we'll define using integrals in the next slide.
But conceptually, this is just like decomposing a vector into components — except now we're working with continuous functions in an infinite-dimensional space, using smooth sine and cosine waves as our basis.
And that’s the beauty of the Fourier series:Rather than reconstructing a function using spikes or samples, we rebuild it by layering together simple waveforms.
This is the core principle of Fourier analysis:Break down the complex using the simple.Use a family of smooth waves to represent arbitrary structure.
Next, let’s see how to actually compute those coefficients.
