Let’s now talk about a foundational concept that underpins much of what we’re doing — the inner product, also known as the dot product.

Suppose you have two vectors, A and B. The inner product is calculated by pairing each element of vector A with the corresponding element of vector B, multiplying those pairs, and then summing the results.

In mathematical terms, if vector A has components A1 through An, and vector B has components B1 through Bn, then the inner product of A and B is the sum from i equals 1 to n of Ai times Bi.

Let’s look at an example. Take vector A as 1, 3, negative 5, and vector B as 4, negative 2, negative 1.

We compute the inner product as follows:One times four, plusthree times negative two, plusnegative five times negative one.
That gives us 4 minus 6 plus 5, which equals 3.

Now, why is this important? Because when we perform convolution or cross-correlation, we are essentially performing inner products — over and over again. Each time we slide one signal over another and compute the inner product, we generate one output value.
This is exactly how matched filtering works in radar systems. When an incoming signal — such as an echo from an aircraft — aligns with a known pattern, the inner product reaches its peak, making it possible to detect the target.

From the perspective of linear systems, cross-correlation involves flipping and shifting one signal, and then computing a whole series of inner products. The result becomes the output response of a shift-invariant system. So understanding inner products is not just helpful — it’s essential.

There’s also a beautiful geometric interpretation. The inner product of two vectors equals the product of their magnitudes multiplied by the cosine of the angle between them.

In other words, the dot product of A and B equals the norm of A times the norm of B times cosine theta — where theta is the angle between the two vectors.

So, if the vectors point in the same direction, the angle is zero, and cosine of zero is one — giving you the maximum value.
If the vectors are orthogonal — that is, they are at 90 degrees — then cosine theta is zero, and so is the inner product.

And here’s something really elegant: if you take the inner product of a vector with itself, you get the square of its length. That means the magnitude of a vector can be defined as the square root of its inner product with itself.

In summary, the inner product captures three things at once — length, angle, and alignment — all in a single operation. And that makes it the foundation for everything we’ll do with Fourier series.

Here’s another way to think about it — by drawing a biological analogy.

Think of the inner product like the base pairing in DNA. Just as DNA sequences are built from matched pairs of bases — adenine with thymine, guanine with cytosine — our inner product is built by matching one element from vector A with one from vector B. Each matched pair contributes to the total result.
And just like those base pairings carry the essential instructions for life, this mathematical pairing carries the structural foundation for data analysis. The overall sum of those matched pairs — the inner product — gives us meaningful information about the relationship between two datasets or signals.

In many ways, you can think of the inner product as the DNA of data science. It’s what lets us quantify similarity, compute distances, analyze angles, and perform transformations. We’ll keep coming back to this idea throughout the course, so keep it in mind as we go deeper into Fourier theory.