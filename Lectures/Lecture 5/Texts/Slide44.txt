Let’s wrap up this lecture by exploring a concrete and classic example: the square wave.
What you see here is a periodic function, one that flips back and forth between two constant values—a positive one and a negative one.

Let’s walk through the structure.
From time t equals zero up to one-half, the function holds steady at plus one. Then, from one-half to one, it suddenly drops to minus one. After that, the pattern repeats. And again. And again. This continues infinitely in both directions along the time axis.
So overall, this is a piecewise function. Within each period, it's constant in two segments, with an abrupt jump at the halfway point. And because it repeats every unit of time, we say it has a period of one.

Now, at first glance, you might think, “This function is too rough for sine and cosine to handle.” It has sharp edges, and it jumps suddenly. But that’s where the power of Fourier analysis shines.
Even though the square wave is not smooth, we can still represent it as a sum of smooth, continuous sinusoids. It’s exactly this kind of jumpy, discontinuous behavior that makes the square wave such a great test case for the Fourier series.

You’ll soon see that we can approximate the square wave by adding up enough sine components. And the more terms we include, the closer the sum will come to capturing that sharp transition from plus one to minus one.
This example will also reveal an interesting phenomenon known as the Gibbs effect, which we’ll talk about next.
But for now, just keep in mind: even for a simple-looking function like this square wave, Fourier series gives us a powerful and systematic way to build it up—using only smooth building blocks.

So now let’s take the next step and compute the Fourier coefficients for our square wave.

As we saw before, the square wave alternates between plus one and minus one over each period. From zero to one-half, it’s plus one. From one-half to one, it drops to minus one. And this pattern repeats every unit interval.

Now, we begin with the zeroth Fourier coefficient. This represents the average value of the function over one period. But notice: for every positive bump, there’s a negative one of equal size. So the total area above the axis cancels out the area below. That means the average value is zero.
So, the zeroth term disappears.

Next, we compute the other Fourier coefficients, often denoted by f-hat of n. These are obtained by taking the inner product of the function with the complex exponential e to the negative 2 pi i n t.
Since our square wave has two constant segments, we split the integral into two parts: one from 0 to one-half, where the function equals plus one, and one from one-half to 1, where the function equals minus one.
We integrate e to the negative 2 pi i n t over each of these intervals, then subtract them.

After doing the math, we arrive at this compact expression:
F hat n equals to 1 over pi i n times the quantity 1 minus e to the negative pi i n.

Now this is an elegant formula. It tells us exactly how strong each frequency component is. For each integer value of n—not equal to zero—we get a corresponding complex exponential that contributes to building up the square wave.

And so, we write the full Fourier series as an infinite sum of these terms. That is, summing over all non-zero values of n:
The sum of 1 over pi i n times 1 minus e to the negative pi i n times e to the 2 pi i n t
Each of these exponential functions corresponds to a specific frequency, and each coefficient tells us how much of that frequency is present in the signal.
On the next slide, we’ll further simplify this result and begin to see the pattern more clearly—especially for odd harmonics.
