{
  "slide_id": "Slide24",
  "model": "llava-hf/llava-onevision-qwen2-7b-ov-hf",
  "prompt": "concepts",
  "raw_output": "user \nSystem:\nYou are an expert educator in medical imaging. From the following slide text,\nextract the key technical or scientific concepts that are essential for understanding the topic.\n\nUser:\nSLIDE_TEXT:\nThe IMIL approach is often used because it naturally incorporates statistical knowledge. To explain this more simply, let us recall the idea of CT reconstruction. Imagine you have multiple views of an object. One view looks like this, another view looks different. Now, you try to reconstruct the underlying image. Suppose the underlying image is a star. When you trace the star vertically, the profile does not look right. When you trace it horizontally, again, it does not match. Then, you try using a different model—say, an ellipse. If you trace the ellipse horizontally, you obtain a profile that matches the measured data more closely. This suggests that the elliptical model fits best. In this case, you would report the ellipse as the reconstructed image.\n\nThis is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image. This approach is fundamentally different from deterministic CT reconstruction methods. Instead of assuming the data is exact, it operates within a statistical framework.\nIn this framework, measurements are modeled with a Poisson distribution. Because of the independence of measurements, the overall likelihood can be expressed as a product of individual probabilities. By taking the natural logarithm, this product is converted into a summation, making the formulation simpler and more practical to compute. The key point is that the statistical model accounts for the Poisson noise inherent in the data, which deterministic methods ignore.\n\nTo see the advantage, consider a phantom experiment. If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise—that every measurement is an exact line integral. But in reality, measurements always have statistical fluctuations. This mismatch leads to streaking artifacts and random noise in the reconstructed images. By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and improved quality, as demonstrated in the phantom example.\n\nSTRICT INSTRUCTIONS:\n- Return pure JSON only. No prose, no markdown, no code fences.\n- Each concept must appear verbatim (case-insensitive) in SLIDE_TEXT.\n- Assign exactly one category from:\n  software | workflow | mathematics | signal_processing | frequency_domain |\n  physics | instrumentation | data_processing | reconstruction |\n  quality_metric | communication | modality | anatomy | algorithm | ai_ml\n- Category guidance:\n  * software → programming tools/environments (MATLAB, Octave, MathWorks)\n  * workflow → procedural/course steps (BUT exclude admin like office hours)\n  * mathematics / signal_processing / frequency_domain → formulas, transforms, filters, spectra\n  * physics → energy, radiation, waves, gradients, attenuation\n  * instrumentation → scanners, detectors, coils, transducers, gantry\n  * data_processing / reconstruction → corrections, backprojection, iterative, FBP, FFT usage\n  * quality_metric → SNR, resolution, artifacts, MTF, DQE\n  * communication → network or data transfer\n  * modality → CT, MRI, PET, SPECT, Ultrasound (US), Optical/Photoacoustic, etc.\n  * anatomy → biological structures (organs, tissues, bones, vessels) ONLY\n  * algorithm → analytical/iterative computational methods\n  * ai_ml → learning methods (deep learning, CNN, transformer, self-supervised)\n- Reject filler or administrative text (office hours, emails, rooms, homework logistics).\n- Output format:\n{\n  \"concepts\": [\n    {\"term\": \"<exact term from slide>\", \"category\": \"<category>\"}\n  ],\n  \"evidence\": [\"<short supporting phrase from SLIDE_TEXT>\"]\n}\nassistant\n```json\n{\n  \"concepts\": [\n    {\"term\": \"maximum likelihood estimation\", \"category\": \"reconstruction\"},\n    {\"term\": \"Poisson distribution\", \"category\": \"statistics\"},\n    {\"term\": \"streaking artifacts\", \"category\": \"reconstruction\"},\n    {\"term\": \"random noise\", \"category\": \"reconstruction\"},\n    {\"term\": \"phantom experiment\", \"category\": \"reconstruction\"},\n    {\"term\": \"filtered back projection\", \"category\": \"reconstruction\"},\n    {\"term\": \"exact line integral\", \"category\": \"reconstruction\"},\n    {\"term\": \"statistical fluctuations\", \"category\": \"reconstruction\"},\n    {\"term\": \"statistical model\", \"category\": \"reconstruction\"},\n    {\"term\": \"improved quality\", \"category\": \"reconstruction\"}\n  ],\n  \"evidence\": [\"This is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image.\", \"By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and",
  "parsed": {
    "term": "improved quality",
    "category": "reconstruction"
  }
}