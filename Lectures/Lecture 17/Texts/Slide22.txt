Once you convert PET data into line integrals, you can remove the attenuation factor, and that will make PET imaging more accurate. Quantitative information can be extracted this way.

Like I said here, the PET result without attenuation correction shows a very dark side. But when you apply the correct attenuation coefficient, you see a much more uniform appearance. The result is more accurate for concentrations—for example, in the liver—making the measurements statistically meaningful. That’s one reason I covered the deterministic reconstruction process first. Now I’m giving you a high-level idea about statistical reconstruction. So, if you have lambda—capital lambda—as the image to be estimated, here’s how the statistical approach works. We are given noisy measurements, and we want to estimate the image from those measurements. The measured quantity, called q, is what actually comes from the system, whether you use mechanical collimators or coincidence detection. The measurement is very noisy because the gamma-ray tracer concentration isn’t perfect, and the decay rate is reasonable, but the noise is not normal anymore. It’s not the normal noise you see in CT reconstruction. The underlying source distribution can be modeled statistically; the image represents the source distribution, broken down into small pixels, where each pixel acts like a small light bulb.

So far, we’ve talked about image reconstruction for both SPECT and PET. This assumes your data model is good enough—meaning you have a large number of gamma photons captured. In that situation, the measurements reflect the actual statistical mean, so you get a formula—the data model.
The data model is tricky for single photon emission and different for paired photon emission because the weighting factor changes. For single photon emission, the weighting factor fundamentally depends on the location of the active source element. The weighting factor always depends on location.
I explained that this location dependence cannot be easily removed. When you try to multiply paired measurements together, cross terms appear and you can't easily get rid of the spatial dependence.

But for PET photon emission, you’re actually able to account for the single source distribution, with a probability factor in the formula. If you have multiple radioactive source points along the same line, you just form the line integral by adding them up. The attenuation factor for any radioactive element along that line is the same.
So you can factor it out. As long as you know mu—the attenuation coefficient—you know the attenuation factor, and you get a neat line integral. Then you use filtered back projection, just like in CT. This is a very nice feature for image reconstruction. If you haven’t got the point yet, review the slides—I’ve been busy these past few days and haven’t uploaded this part yet, but I’ll refine it and upload it soon. It’s important to review the slides and follow the argument—this way, you’ll truly understand the data model for SPECT and PET. That’s key. Once you know the model, for PET in particular, you can use filtered back projection.

That’s why nuclear tomography comes after CT, because you need to have some background knowledge from CT. For SPECT imaging, you have a location-dependent attenuation factor. But if you know mu, and if you measure it—like in an individual transmission scan—you can still convert it into the attenuation rate for your data.
You can turn each measurement into something that forms a line integral—a sum. Then you’ll have a system of linear equations, which is just like CT—a system of linear equations to solve. You can use a real high-performance computer for this. Linear algebra gives you the tools to measure and solve these equations, and we know how to do that both numerically and analytically.

But what happens if you don’t have enough flux? That’s a big problem. I mentioned it the other day: X-ray flux is high, but for nuclear imaging, it’s much lower. That’s because you’re introducing radioisotopes. The number of emitted gamma-ray photons isn’t ideal, because you don’t want to introduce too much radioactive material into the human body. You need to use the minimum amount for safety. On the other hand, radioisotopes are incredibly sensitive. Any signal you detect comes from the radioisotopes, so you get very high sensitivity.